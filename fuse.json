{"index":{"keys":[{"path":["content"],"id":"content","weight":1,"src":"content","getFn":null},{"path":["frontmatter","title"],"id":"frontmatter.title","weight":1,"src":"frontmatter.title","getFn":null},{"path":["frontmatter","tags"],"id":"frontmatter.tags","weight":1,"src":"frontmatter.tags","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"프로젝트 진행 중 새로운 상태 관리 라이브러리를 도입해야 하는 상황이다.\n\n보통 서드파티 라이브러리 중 하나를 고를텐데. 이 때 보통 하는 것이 아래 사항들을 검토하는 것이다.\n\n * 필요성: 직접 구현보다 라이브러리를 쓰는 게 명확히 이점이 있는가?\n * 안정성: 최근까지 활발히 유지보수되고 있는가?\n * 커뮤니티: 이슈 대응 속도와 커뮤니티 지원이 활발한가?\n * API 안정성: 업데이트 시 Breaking Change 위험이 낮은가?\n * 유지보수 용이성: 라이브러리 교체/제거가 쉽게 가능하도록 구조화했는가?\n * 그 외 호환성, 번들 크기, 성능 영향 등.\n\n검토 이유, 항목 별 우선순위는 상황마다 다르지만 근본적인 이유는 개발 조직이 이 라이브러리를 도입하여 얻는 실직적인 이득이 있는가? 이겠다.\n\n그런데 유독 이런 조건을 빡빡하게 따지지 않는 경우가 있다. 바로 nextjs, angular와 같은 프론트엔드 프레임웍들이다.\n\n\n보통 NEXTJS가 선택되는 이유\n\n * react 기반이다\n * ssr 을 쉽게 구현할 수 있다\n   * seo 최적화, 첫 페이지 로딩 속도 개선\n * 간단한 백엔드 구현 가능 -> 서버리스 및 엣지\n * 뛰어난 DX 및 생태계, 호스팅\n * 유지보수 및 채용\n * 최근 트렌드임\n\n보통 위의 조건으로 nextjs 가 선택된다. 개발 조직 입장에서는 꽤 합리적인 조건이라고 생각할 수 있다.\n\n\n누락된 검토 사항\n\n어떤 프론트엔드 프레임웍을 선택하는가에 따라 구현가능한 기술, 구현이 어려운 기능, 지불해야하는 비용 등이 천차만별로 달라진다.\n\n따라서 선택 조건에는 위 개발조직 입장에서의 항목 뿐만 아니라 이런 부분도 반영되어야 한다.\n\n자본금을 관리하는 재무 부서, 제품 퀄리티를 최종적인 입장에서 챙겨야 하는 마케팅, 기획 부서의 요구사항도 반영할 수 있는가가 중요하다는\n것이다.\n\n사실 다른 부서들은 프론트엔드 개발팀에 이런 부분을 알고 요구할 수 없다. 거의 대부분은 그냥 믿고 맞기는 것이다.\n\n고객은 본인이 뭘 원하는 지 본인도 모른다라는 말이 있지 않은가? 비슷한 맥락이라 볼 수 있다.\n\n\n불필요한 서버비 지출\n\nnextjs 는 nodejs를 실행할 수 있는 웹 서버가 필요하다. aws 기준으로 ec2 or lambda, alb 여기에 오토 스케일까지\n고려할 경우 그 비용은 엄청 늘어난다.\n\n반면 CSR 정적 호스팅은 어떤가. S3, CloudFront 사용료만 내면 되고 인스턴스같은 건 없다.\n\n그럼 여기서 따져봐야 할 것은. 그 압도적인 비용 차이를 납득시킬만 한 가치를 제공할 수 있는가 이다.\n\n최소 10배 이상의 서버비를 납부하며 얻은 서버 측 렌더링 혹은 그 부수적인 가능성을 통해 회사가 목표로 하는 임팩트를 만들거나, 그에 가까워질\n수 있나?\n\n잠깐 스켈레톤 보였다 내용이 천천히 나오는 것을 로딩 없이 보여주도록 변경한 것이. 저 위의 비용과 맞먹는 임팩트를 주는 건가?\n\n추가로 로딩이 없어 지는것이 맞는가?\n\n\n요구사항 구현의 어려움\n\n페이지 이동 취소 기능 [/_astro/page_leave.DlXWnbfc_BLe8r.webp]페이지 이동 취소 기능\n\nnextjs 는 웹 앱 기획에서 거의 필수적으로 요구하는 페이지 이동 취소 기능을 공식적으로 제공하지 않고 있다.\n\ntrick 을 사용하면 가능한데. 이 경우 주소창의 경로와 보이는 페이지가 달라지는 문제가 발생할 수 있다.\n\n보통 고맙게도 노운이슈 처리하여 개별 안내하거나 하는 등으로 넘어가는데, 한편으로는 안타깝기 그지 없다.\n\n이 기능은 고객이 정보를 한창 입력하다 실수로 페이지를 벗어나버려 입력된 정보를 날리는 문제를 해결하기 위해 추가되는데. 상황에 따라 큰 문제가\n될 수 있다.\n\n입력값을 세션에 담아 주면 되지 않느냐란 의문이 있을 수 있는데, 그렇게 해서 해결이 되는 곳이 있고 아닌 곳이 있다.\n\n\n그래서 어떻게 하라고\n\n당연히 위의 내용은 구성원 혹은 구성 조직의 방향성에 따라 전혀 문제가 되지 않을 수 있다.\n\n이런 부분들도 함께 고려하여 조직이나 회사에 기여하는것이 진정한 값어치를 하는 것이다.\n\nnextjs 를 쓰지 말라는 이야기도 더더욱 아니다. 이왕 쓸것이라면 사용 방법과 주의사항, 문제가 될 수 있는 부분에 대한 대비를 충분히 해야\n한 다는 것이다.\n\n라우팅 경로 별 렌더링 전략을 달리 할 수 있는 것은 정말 멋진 기능이긴 하다. 멋진 기능이긴 하다.","n":0.045},"1":{"v":"Next.js 를 사용해야 할 이유","n":0.447},"2":[{"v":"frontend","i":4,"n":1},{"v":"astro","i":3,"n":1},{"v":"remix","i":2,"n":1},{"v":"angular","i":1,"n":1},{"v":"next.js","i":0,"n":1}]}},{"i":1,"$":{"0":{"v":"MCP는 Anthropic이 24년 11월 공개한 오픈소스 프로토콜이다. AI모델이 외부 도구를 사용하는 개념은 이전에도 있었지만, 개방적인\n설계로 커뮤니티 참여를 유도하여 현 시점 꽤 핫한 기술이 되었다.\n\n당장 실무에 도움이 되지 않더라도, 트렌드는 파악해야겠다 싶어 Open WebUI, MCPO, MCP Server를 활용해 외부 html문서를\n요약하는 시나리오를 테스트 해 보았다.\n\n현재까지 Youtube등의 활용 사례들은 대부분 claude desktop이나 cursor, vscode를 사용하고 있는데, 로컬 모델을\n사용하기 위해 Open WebUI를 사용했다.\n\n\nOLLAMA\n\nollama [https://ollama.com] 설치하고 gemma3:27b, mistral-small3.1 모델을 받는다.\n\nollama run gemma3:27b\nollama run mistral-small3.1\n\n\nOPEN WEBUI\n\nInstallation with Default Configuration\n[https://github.com/open-webui/open-webui?tab=readme-ov-file#installation-with-default-configuration]\n참고. Nvidia GPU Support로 설치.\n\nWSL2 에서 쓰면 성능 감소폭이 크다는 글도 있고 큰 차이가 없다는 글도 있긴 한데 찍먹이므로 스킵.\n\ndocker run -d -p 3000:8080 --gpus all -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:cuda\n\n\nMCPO\n\nMCPO는 Open WebUI에게 사용 가능한 tool목록과 인자, 데이터를 OpenAPI 포멧으로 주고받고. MCP 서버와는 호환되는 데이터\n포멧으로 데이터를 주고받는 중계 서버 역할을 한다.\n\n자세한 내용은 MCPO: Supercharge Open-WebUI /Ollama with MCP Tools\n[https://mychen76.medium.com/mcpo-supercharge-open-webui-with-mcp-tools-4ee55024c371]\n참고.\n\nInstalling UV [https://docs.astral.sh/uv/getting-started/installation/] python\n사용해야 하므로 UV설치. python만 쓰면 되므로 필요에 따라 다른 것 써도 무방할 듯.\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\nbackend 라는 폴더 하나 만들고 안에서 프로젝트 설정 및 mcpo, mcp server들 설치.\n\ncd ~/Projects/\nmkdir backend\ncd backend\n\nuv venv\nuv pip install mcpo mcp-server-fetch mcp-server-time\n\nrun.sh 파일 만들고 아래 내용 기입.\n\n#!/usr/bin/env bash\n\nuvx mcpo --config ./config.json --host localhost --port 8080\n\nconfig.json 파일 만들고 아래 내용 기입.\n\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-fetch\"]\n    },\n    \"time\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-time\", \"--local-timezone=Asia/Seoul\"]\n    }\n  }\n}\n\n그 다음 MCPO 서버 실행\n\n./run.sh\n\nStarting  MCP OpenAPI Proxy with config file: ./config.json\nINFO:     Started server process [30187]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n\n\nOPEN WEBUI 세팅\n\n 1. 좌측 하단 사용자 이름 눌러 설정 클릭.\n\n 2. 모달 좌측 중간의 도구 클릭.\n\n 3. Manage Tool Servers 우측 + 버튼 클릭.\n\n 4. 위 config.json 에 2개 등록했으므로 아래 두개 항목 추가. (새로고침 아이콘 눌러서 연결 성공 토스트 노출되면 OK)\n    \n    1. url: \"http://localhost:8080/fetch/openapi.json\"\n    2. url: \"http://localhost:8080/time/openapi.json\"\n\n 5. 이제 프롬프트 입력창 좌측 하단에 렌치 아이콘과 함께 tool을 사용할 수 있게 됨\n\n위에서 추가한 두개의 tool에 대한 아이콘이 노출됨 [/_astro/open-webui-tools.hMaYaYzT_Zl9PDs.webp]위에서\n추가한 두개의 tool에 대한 아이콘이 노출됨 config.json에 추가했던 tool목록이 노출됨. 참고로\nmcp-server-fetch-python 인 건 mcp-server-fetch 대신 다른 tool추가해서 그럼\n[/_astro/open-webui-tools2.CPKYvj34_ZTqrwv.webp]config.json에 추가했던 tool목록이 노출됨.\n참고로 mcp-server-fetch-python 인 건 mcp-server-fetch 대신 다른 tool추가해서 그럼\n\n\nMCP FETCH 사용 소감\n\n> 블로그 발행 후 다시 테스트 해 보니 gemma3, mistral-small3.1 둘 다 tool을 잘 활용하길래 내용을 수정했다.\n\n * 테스트에 사용한 GPU는 RTX 4090 Founders Edition (24GB)\n * 로컬 모델은 gemma3, mistral-small3.1 두개 돌렸고 추가로 개발자 설정에서 MCP를 사용할 수 있는 claude\n   desktop을 사용해 보았다\n * fetch는 mcp-server-fetch\n   [https://github.com/modelcontextprotocol/servers/tree/main/src/fetch],\n   mcp-server-fetch-python [https://github.com/tatn/mcp-server-fetch-python] 둘 다\n   테스트해 보았다\n\n--------------------------------------------------------------------------------\n\n * 일단 ChatGPT는 붙여넣은 링크를 인식하지 못 했다. 다만 해당 주소를 웹 검색하여 내용을 찾아내고. 본문에 대한 질문에 답변해주었다.\n   이런 걸 보면 tool들을 상호 보완하여 사용해야 할 듯.\n * claude.ai 는 아예 붙여넣은 링크는 인식 자체를 하지 못 했다. 웹 검색을 시도하지도 않았다.\n * 로컬 모델 및 claude desktop의 경우 MCP를 잘 활용하여 요약을 해 주었다. 다만 크롤링이 안 되면 요약 불가했음.\n * 당연한 말이지만 MCP서버의 품질에 따른 경험 차이가 클 것으로 보인다. Youtube에서 MCP를 소개할 때 Figma 를 예시로 드는\n   이유가 있는 듯. MCP는 결국 서드파티이기 때문에 경우에 따라서 희망고문만 당하다 끝날지도 모르겠다.\n\nmcp-server-fetch-python이 제공하는 tool_get_markdown_post를 사용하여 블로그 본문을 정확히 요약해주었다.\n[/_astro/using-tool.Cp7qob7Q_16ttnf.webp]mcp-server-fetch-python이 제공하는\ntool_get_markdown_post를 사용하여 블로그 본문을 정확히 요약해주었다. 네이버 블로그 글은 MCP 서버가 크롤링을 못 해서\n내용을 알 수 없다고 안내해 주었다. [/_astro/using-tool2.DIqzRlCG_2qvo1t.webp]네이버 블로그 글은 MCP\n서버가 크롤링을 못 해서 내용을 알 수 없다고 안내해 주었다. claude desktop도 mcp-server-fetch를 사용해 본문 내용을\n요약해 주었다. [/_astro/using-tool3.CR-RxKZy_wnisn.webp]claude desktop도\nmcp-server-fetch를 사용해 본문 내용을 요약해 주었다. MCP 서버가 크롤링을 못 하니 claude desktop도 요약이\n불가능했다. [/_astro/using-tool4.Ds_MtmEr_Z29dhJ4.webp]MCP 서버가 크롤링을 못 하니 claude\ndesktop도 요약이 불가능했다.","n":0.042},"1":{"v":"Open WebUI - MCP fetch 체험기","n":0.408},"2":[{"v":"AI Model","i":2,"n":0.707},{"v":"MCP","i":1,"n":1},{"v":"Open WebUI","i":0,"n":0.707}]}},{"i":2,"$":{"0":{"v":"얼마 전부터 AI를 활용한 개발을 Vibe Coding이라 부르기 시작했다. 단어가 적절한 듯 하면서도, 무언가 불필요하게 포장된 느낌이라 입에\n딱 붙지는 않는다. 업무상으로는 아예 다르게 표현하고 싶을 정도…\n\n2024년 중순부터 내가 속한 조직에서도 copilot을 시작으로 cursor 등의 AI를 보안 검토 후 사용할 수 있게 되었다. 하지만 개인,\n조직별로 실제 실무에서 적극적으로 활용하는 사례는 찾아보기 어려운 듯 하다.\n\n개발자 커뮤니티, YouTube등 AI를 활용한 개발 대해 검색해 보면. 효율에 대해서는 좀 의견이 갈린다. 실제로 과거에 함께 일했던 분들의\n이야기를 들어보았을 때는 부정적인 쪽으로 기울어있는 느낌이다.\n\n실효성이 떨어진다는 의견의 이유를 대략 정리하자면 “답변의 품질이 떨어진다.”, “업무 프로세스 상 번거롭다.” 였다.\n\n\n생각 외로 높은 진입장벽\n\nAI로 효율적으로 사용하는 것이 생각보다 만만하지 않다. 그나마 IDE에 붙어 다양한 정보를 주고받게 되어 단순 채팅만 가능했던 이전보다 쓰기는\n훨씬 편해졌지만, 그것만으로 실무에서 큰 도움이나 성과를 만들기엔 무리가 있다.\n\n효과를 보려면 의외로 개발자의 역량이 중요하다. 일을 시키는것도 아는 사람이 시킬수 있고. AI모델의 답변을 다듬는데에도 역량이 필요하다.\n\n또한 업무 프로세스도 이에 맞춰 변경해야 한다. 단순히 지금 필요한 알고리즘을 짜는 것이 아니라, 업무 시작 전. AI로 효율을 볼 수 있는\n작업과 직접 하는게 더 효율적인 작업을 현실감있게 구분한다. AI에 대한 지식이 필요한 부분이다.\n\n업무 환경도 중요하다. 협업 부서와의 R&R정리라던가, AI사용에 대해 편견이 없는 분위기인지 등등 생각보다 이 부분도 쉽지 않다.\n\n그 동안 대부분의 개발자가 행해 왔던 업무 패턴도 걸림돌이 될 수 있다. 이를테면 자체 구축한 디자인 시스템을 AI가 알 수 있나? 당연 방법은\n있지만 AI활용에 대한 지식이 필요하다.\n\n\n앞으로의 VIBE CODING\n\n앞서 언급한 내용들로 인해. 현실적으로 소, 중규모의 시스템이 갖춰지지 않은 곳에서 먼저 적극적으로 활용할 듯 하다. 다만 대기업들이\ncopilot이나 cursor 등 보안에 민감한데도 도입하는 것을 보면 중요성을 알게 되는 순간 금방 따라잡긴 할 것이다.\n\n과거 2010년 중반만 해도 Virtual Scroll, Web Based Editable Grid 들은 어느정도 FE에 투자할 수 있는 의지나\n규모가 있는 회사들의 전유물이었다. 그런데 지금은 AI모델 하나에 스크린샷만 보여주면 눈 깜짝할새에 구현해주는것은 물론이며 SSR도 완벽하다.\n\n그리고 AI는 정말 빠르게 새로운 기술이 나오고 있다. 지금도 그렇지만 23년만 해도 AI에 관련된 논문이 쉴 새 없이 쏟아져 나왔는데. 이제는\n개발자가 실제로 적용할만한 기능으로 쏟아져 나오고 있다.\n\n당연히 FE개발 역량은 똑같이 쌓아야 하며, AI에 대한 트렌드도 적극적으로 따라가야 한다. 대기업이 허튼데 돈 쓰는것 본 적이 없다.","n":0.056},"1":{"v":"Vibe Coding","n":0.707},"2":[{"v":"Vibe Coding","i":3,"n":0.707},{"v":"AI","i":2,"n":1},{"v":"Development","i":1,"n":1},{"v":"Frontend","i":0,"n":1}]}},{"i":3,"$":{"0":{"v":"큰 규모의 개발 조직에서는 보통 예산이나 리소스를 여유롭게 사용한다. 이런 조직은 대개 비용보다 기술적인 가치에 중점을 두고 업무를 진행하고\n평가도 그 기준에 따른다.\n\n그렇다 보니 소속 개발자들도 자연스럽게 기술 중심으로 업무를 진행하게 된다. 예를 들면 최신 기술 스택들을 빠르게 습득하여 서비스에 적용하고.\n그 과정에서 경우에 따라 예산이나 리소스를 초과하여 사용하기도 한다.\n\n회사 입장에서는 요구한 바를 충실히 이행하였고 초과분에 대한 소명도 방향성에 어긋나지 않으므로 딱히 문제 될 것이 없다. 하지만, 이런 방식은\n나한테도 회사에게도 장기적으로는 전혀 도움이 되지 않을 가능성이 높다.\n\n이런 부류의 오류들 중. 가장 흔한 형태의 오류를 꼽자면 Server Side Rendering의 적용이 있다.\n\n서비스에 왜 SSR을 적용해야 하는지. Next.js를 사용해야 하는지 “기술적”, “환경적” 두 가지 측면의 고려가 필요하다. 기술적 측면은\n익숙한 개발자들이 많지만, 환경적 측면을 고려하지 못 하면 주니어 개발자에 머무를 수 밖에 없다.\n\n\n고려해야 할 사항들\n\n이제 “SSR적용” 이라는 주제로 고려할 수 있는 내용에 대해 이야기 해 보자. 먼저 보면서 생각할만한 내용이 필요하므로 각 방식의 장단점을\n정리한다.\n\n“SSR의 장단점”을 구글링하면 정말 많은 글을 볼 수 있다. 일부 글에서는 SSR을 php같은 전통적인 방식으로 설명하는 경우도 있고.\n부정확하거나 모호하게 정리한 경우가 대부분이긴 하나. 대략 정리해보면 아래와 같다.\n\n단점은 장점들의 반대이므로 따로 적지 않았다. 또 SSG나 iSSG(Next.js에서는 ISR이라 함)은 사실 SSR을 사용할 때 경우에 따라\n적용할 수 있는 기법이므로 따로 구별하지 않는다.\n\n\nCSR\n\n * 인프라 구성이 비교적 단순하고 저렴하다.\n * INP가 짧다.\n\n\nSSR\n\n * SEO에서 조금 더 유리하다.\n * FCP가 짧다.\n\n\n기술적 측면\n\n보통 주니어 시절에 집착하는 기술 중심의 고민들이다.\n\n * CSR은 INP가 짧은가? 반대로 SSR은 INP가 길 수밖에 없나?\n * SSR은 FCP가 짧은가? 반대로 CSR은 FCP가 길 수밖에 없나?\n   * SSR의 TTFB는 CSR에 비교해 길 수밖에 없나? 반대로 CSR은 TTFB가 짧나?\n * CSR은 Waterfall Request에 취약한가? 해결책이 정말 SSR, Suspense 뿐인가?\n * SEO의 적용이 서비스에 어떤 이득을 주는가?\n * CSR은 UI/UX측면에서 제한되는가? 반대로 SSR은 제한이 없는가?\n * CWV는 어떠한가? 모바일의 경우 클라이언트 측에 전송되는 전체 리소스 크기는 어떠한가?\n * 빌드 속도는 어떠한가?\n * 트래픽이 늘어나도 안정적인가?\n\n\n환경적 측면\n\n조직과 회사 입장에서의 고민이 중심이다.\n\n * CSR의 기술 및 인프라 구성이 단순한가? 단순함의 기준이 무엇인가?\n   * 조직 내 모든 구성원이 이해할 수 있을 정도로 단순한가? (쉽나)\n * 반대로 SSR의 기술 및 인프라 구성은 어려운가? 어려움의 기준은 무엇인가?\n   * 도입 후 유지보수는 어떠한가? 새로운 비즈니스의 빠른 실현에 발목을 잡지는 않는가?\n   * 클라우드 쓰면 된다고? 비용은 공짜인가?\n * CSR의 인프라 구성이 정말로 저렴한가? 반대로 SSR은 비용이 높을 수 밖에 없나?\n * SSR도입 대비 얻을 수 있는 이득을 돈으로 환산하면 얼마나 될까? 혹시 마이너스가 되지는 않는가?\n * CSR(혹은 SSR)도입이 개발자 커리어와 회사 모두에게 이득이 되는가?\n * CSR(혹은 SSR)도입이 협업에 도움이 되는가?\n\n\n정말 중요한 것\n\n연차가 쌓여 기술에 대한 결정권을 갖게 되면. 예시로 다뤘던 위의 내용 외에 디자인 시스템의 자체 개발 여부 등 다양한 선택들에서도 깊은 고민이\n필요하다.\n\n기술 트렌드를 쫓는것도 중요하다. 새 기술이 만들어지고 유행을 타는 것은 대부분 개발자들의 고민을 해결해 주거나 해결을 위한 아이디어를 떠오르게\n해 줄수 있기 때문이다.\n\n하지만. 그런 기술이나 방법론을 무작정 따르기 보다는 해당 기술의 탄생 배경에 대한 이해. 그리고 현재 서비스에 제공할 수 있는 기회 혹은\n기회비용을 정확히 파악하고 필요에 따라 적용해야 한다.\n\n충분한 이해 후에는 단순 도입 뿐만 아니라 아이디어만 차용하는 등 여러 선택지도 생기며. 그에 따라 모두에게 이득을 주는 진정한 기여에 더\n가까워질 수 있다.\n\n또. 개발중인 소스코드 포함하여 회사 뿐만 아니라 개발자 본인도 영원하지 않다는 것을 기억해야 한다. 언제까지 지금 둘러진 울타리 안에서 풍족한\n리소스를 소비해가며 서비스를 개발하고 운영할 수 있을까?","n":0.045},"1":{"v":"연차가 쌓일수록 중요해지는 것","n":0.5},"2":[{"v":"development cost management","i":0,"n":0.577}]}},{"i":4,"$":{"0":{"v":"최근 담당한 서비스의 CWV개선작업 중 이상한 현상을 발견했다. 페이지에 <img>태그를 사용하면 이미지가 강제로 preload처리되는\n현상이었다. CWV개선을 위해서는 리소스의 우선순위 최적화\n[https://patterns-dev-kr.github.io/performance-patterns/loading-sequence/]가\n필수인데. 위의 현상때문에 주요 리소스의 로드를 방해하도록 html이 렌더되었다.\n\nexport default function Page() {\n  return (\n    <>\n      <img src=\"a.png\" />\n    </>\n  )\n}\n\n// 렌더링 결과:\n// \"<html><head>\n//   <link rel=\"preload\" href=\"a.png\" /> ???\n// </head> ...생략... </html>\"\n\n위 현상 때문에 LCP를 많이 깎아먹고 있었고 이리저리 소스코드를 보다 보니 이 문제가 next.js가 아니라 react-dom의 의도된\n동작이라는 것을 알았다.\n\n[Fizz] Preload “suspensey” images [https://github.com/facebook/react/pull/27191]\n\n해당 PR은 이미지들의 loading, fetchPriority 속성에 따라 몇 가지 조건에 해당할 경우 이미지들을 preload처리하는\n내용이다.\n\n특이한 건 이후에 <picture>안의 <img>들은 preload하지 않는 PR도 발견했다.\n\n[Fizz][Float] <img> inside <picture> should not preload during SSR\n[https://github.com/facebook/react/pull/27346]\n\n위의 내용을 정리하면. react-dom/server 의 스트리밍 버전인 ReactDomFizzServer의\nrenderToPipableStream함수는. 이미지가 <picture>바깥에 있거나, fetchPriority=\"low\" 속성을 주지 않을\n경우. 강제로 preload처리한다.\n\n리소스 우선순위를 조절해야 하는 경우. <img>태그를 직접 사용하려면 위의 조건을 만족하도록 적절히 수정하거나, next.js가 제공하는\n<Image>태그를 사용해야 한다.\n\n--------------------------------------------------------------------------------\n\n위 현상의 경우 별다른 가이드가 없기도 하고. 사실 <img>태그들을 왜 preload처리하는지 이해가 안 된다. 심지어 이미지 태그가 20개면\n20개 다 preload하는데 도대체 왜 그런지…\n\nnext.js는 개발자들이 이미지를 사용할 때 대부분 <Image> 등 제공된 API들만 사용하길 바란 듯 하다. 그랬다면 위 문제는 없었을\n것이긴 하다…\n\n사실 next.js덕분에 개발자 입장에서는 다양한 렌더링 전략을 요구사항에 맞춰 쉽게 구현할 수 있어 좋긴 하다, 그런데 일전의\nSuspense사태도 그렇고. 올바른 방향으로 가고 있는지는 잘 모르겠다.","n":0.071},"1":{"v":"Next.js에서 img태그 사용 시 주의사항","n":0.447},"2":[{"v":"performance","i":1,"n":1},{"v":"next.js","i":0,"n":1}]}},{"i":5,"$":{"0":{"v":"페이지에 고해상도 이미지를 출력할 때 img요소는 본문에 삽입되었지만, 이미지가 바로 보이지 않고 잠시 후에 출력되는 현상을 본 적 있을\n것이다.\n\n이는 이미지 다운로드가 완료되었지만, 아직 디코딩이 되기 전이라 페이지에 나타나지 않는 현상이다.\n\n요소 추가와 동시에 이미지가 보여지길 원한다면 아래 코드를 사용해야 한다.\n\nconst img = new Image()\n\nimg.src = '...'\n\nimg.decode().then(() => {\n  document.body.appendChild(img)\n})\n\n\n이미지 디코딩\n\njpg, png, webp 등 대부분의 이미지 파일들은 기본적으로 인코딩이 되어 있다. 쉽게 말하자면 압축이 되어 있는 것이다. 브라우저는\n서버로부터 응답받은 이미지들을 화면에 그리기 위해 비트맵으로 디코딩. 즉 압축을 해제한다.\n\n디코딩 결과는 브라우저가 알아서 캐시 했다가 화면에 그려야 할 때 알아서 메모리에 올리고. 화면에서 안 보이면 다시 해제하는 식으로 동작한다.\n\n\n디코딩 시간에 영향을 주는 요소들\n\n먼저 CPU와 RAM에 영향을 많이 받으므로. 모바일에서 성능 감소가 두드러질 수 있다.\n\n제일 큰 영향을 주는 요소는 이미지의 크기이다. 크기가 큰 이미지일수록 디코딩 시간이 많이 소요되므로. 서버에서 적절한 크기의 이미지를 서빙하는\n것이 매우 중요하다.\n\n이미지 포멧 역시 중요하다. jpg나 png들은 오랫동안 사용되었기 때문이 이미 다양한 플랫폼에서 사용 가능한 효율적인 디코더(압축 해제\n프로그램으로 이해하면 쉽다)들이 존재하지만, 비교적 최신 포멧인 jpeg-xr이나 webp의 경우 아직 그렇지 않으므로 디코딩 시간이 증가할 수\n있다.\n\n호텔, 호스텔 등 다양한 숙박시설의 요금을 비교하는 서비스인 trivago 에서는 고객으로부터 jpeg-xr 포멧에 관련된 성능 이슈\n[https://calendar.perfplanet.com/2018/dont-use-jpeg-xr-on-the-web/]를 접수했었다고 한다.\n\n브라우저의 휴리스틱 알고리즘을 통해 일부는 GPU를 사용하여 디코딩되기도 하고. 이미지를 실제로 그려내기 위해서도 GPU를 사용하고 있기도\n하다.\n\n> 장축 3000정도 되는 크기의 이미지를 png, webp포멧으로 디코딩 시간을 측정해보았더니. png가 8ms, webp가 12ms 정도로\n> 증가하였다 (M2 macbook pro 에서 측정)\n> \n> 각 포멧별로 하드웨어 자체에서 디코더를 제공하거나 그렇지 않아 소프트웨어 디코더를 적용해야 하는 등. 브라우저마다 차이가 있으므로 결정 전에\n> 한번 찾아 보길 바란다.\n\n\n이미지 디코딩 시간 측정하기\n\n 1. 크롬 개발자 도구의 Performance탭에서 새로고침 측정으로 확인 가능.\n 2. webpagetest의 측정 결과를 chrome://tracing페이지에서 로드하여 확인 가능.\n\n\nDECODING 속성을 활용한 비동기 디코딩\n\n이미지 디코딩은 메인 쓰레드 혹은 래스터라이징 쓰레드를 사용한다. 해당 쓰레드에 여러 작업들이 할당될 것인데. 당연하게도 이미지 디코딩이 오래\n걸리면 다른 작업들은 지연된다.\n\n다른 이미지의 디코딩 뿐만 아니라, 화면에 텍스트를 출력하는 작업들도 지연될 수 있고. 60fps로 밑돌게 되면 사용자는 끊기는 느낌을 받게\n된다. 이 때 이미지 디코딩을 비동기 처리한다면 도움이 될 수 있다.\n\ndecoding 지정 전 [/_astro/before.DIyUZACy_Z1rtrfK.webp]decoding 지정 전\n\n위 이미지는 고해상도 이미지의 decoding을 적용하기 전의 측정 결과인데. 이미지 디코딩 과정으로 래스터라이저 쓰레드 1번의 다른 작업들이\n밀린것을 확인할 수 있다.\n\n여기서 img태그에 decoding=async를 할당할 경우 아래와 같은 측정 결과를 확인할 수 있다\n\ndecoding=async 지정 후 [/_astro/after.CZunzkoJ_ZezIXQ.webp]decoding=async 지정 후\n\n개선 후의 측정 결과에서 디코딩 과정이 별도의 프로세스로 분리되어 다른 태스크들이 방해되지 않는 것을 볼 수 있다.\n\n\n.DECODE() 메서드를 활용한 PREDECODING\n\n서론에서 언급한 문제는 주로 이미지 갤러리등 이미지가 주요 컨텐츠이거나, 스크롤 등에 따른 점진적인 이미지 렌더링 과정에서 두드러지며 UX에\n악영향을 끼칠 수 있다.\n\n이 때 앞서 언급한 코드와 같이 .decode()를 호출해 디코딩이 완료된 후 본문에 삽입할 경우 이런 문제들을 해결할 수 있다.\n\nnext.js의 이미지 컴포넌트의 decode적용\n[https://github.com/vercel/next.js/blob/v14.2.15/packages/next/src/client/image-component.tsx#L75]를\n참고해서 필요에 따라 서비스에 구현하도록 하자.\n\n--------------------------------------------------------------------------------\n\n참고\n\n 1. Don’t use JPEG-XR on the Web\n    [https://calendar.perfplanet.com/2018/dont-use-jpeg-xr-on-the-web/] jpeg-xr\n    은 소프트웨어 디코더를 사용하다 보니 렌더링 성능에 악영향이 있다고 함.\n 2. Best practices from open source: Use img.decode() in image-heavy\n    applications\n    [https://www.linkedin.com/pulse/best-practices-from-open-source-use-imgdecode-ramu-narasinga-qukie/]","n":0.047},"1":{"v":"Image Decoding","n":0.707},"2":[{"v":"performance","i":2,"n":1},{"v":"ui optimzation","i":1,"n":0.707},{"v":"image","i":0,"n":1}]}},{"i":6,"$":{"0":{"v":"서론\n\nXMLHttpRequest기반 RIA의 개발부터. Selective Hydration을 구현하고 있는 웹 기술의 변천사를 하나하나 돌이켜 보다\n보면. 어떻게 이런 생각까지 할 수 있을까라는 생각으로 자연스럽게 흘러 가게 된다.\n\n웹 기술의 변화를 이끄는 조직은 수도 없겠지만 가장 눈에 띄는 곳은 단연 facebook, vercel, shopify일 것이다. 이 곳의\n협업을 통해 개선되는 react, nextjs 의 신기능 시연은 여가 시간에 한번이라도 더 컴퓨터 앞에 앉게 만드는 자극제가 되고 있다.\n\n소개된 기술 중 Streaming Server Rendering with Suspense\n[https://youtu.be/8dUpL8SCO1w?t=4174] 와 같은 기능들은 이 글을 쓰고 있는 2023년에도 아직 실험적 기능이라는\n딱지를 달고 있고 완전하게 동작하지는 않고 있지만. (nextjs 13.2 에는 라우팅 단위로 비슷하게 구현될 수 있도록 업데이트 되었다)\n\n사실 Hydration 과정을 스트리밍하는 개념 자체는 react의 발표 전에도 존재했으며 오늘 글에서 소개할 Astro는 그런 개념 중\nIsland Architecture\n[https://patterns-dev-kr.github.io/rendering-patterns/the-island-architecture/]를\n프레임워크 레벨에서 구현하고 있다.\n\n용어가 생소할 수 있는데 내용을 보다 보면 익숙한 느낌이 들 것이다. 기본적인 개념 설명과 Gatsby로 개발했던 블로그를 Astro로 옮기며\n생긴 변화들에 대해서 정리한다.\n\n\nISLAND ARCHITECTURE\n\npreactjs의 개발자 Jason Miller가 쓴 Island Architecture에 대한 소개\n[https://jasonformat.com/islands-architecture/]를 보면 Etsy의 프론트엔드 엔지니어 Katie\nSylor-Miller [https://twitter.com/ksylor] 와의 미팅 중에 관련된 개념에 대한 이야기를 처음 나눴다고 한다.\n\n출처: Islands Architecture: Jason Miller\n[/_astro/island-architecture.DV2hn7vO_Z1PLSdv.webp]출처: Islands Architecture:\nJason Miller\n\nIsland Architecture의 요점은 페이지 내에 동적인 영역에 대한 html은 서버가 렌더링하여 내려주고. 인터렉션을 위한 JS를\n후속하여 내려받도록 해 점진적으로 페이지의 기능을 이용할 수 있도록 하는 것이다.\n\nSSR이 SEO를 고려해 자주 구현되곤 하지만, SSR은 인터렉션에 관련된 JS가 다운로드되고 실행되기 전 까지 페이지가 동작하지 않는 부정적인\n경험을 사용자에게 줄 수 있다. 일반적으로 개발되는 JS를 처리하기 위한 리소스 소모가 생각보다 많다고 한다.\n\n또한. 서버에서 렌더링해 내려주는 html에는 뉴스의 경우 본문이, 상품 페이지의 경우 상품 설명 등. 사용자 입장에서 필수적인 내용이 꼭\n포함되어 있어야 한다는 내용을 포함하고 있다.\n\nreact의 스트리밍 서버 렌더링 소개 영상이 2021년 발표되었으니. 이 개념은 알려진 것 보다 프론트엔드 엔지니어들 사이에서 훨씬 전에\n논의되고 구현되어 왔던 것이다.\n\n따라서 Island Architecture는 어느 프레임웍 혹은 라이브러리를 지칭하는 용어가 아니라 디자인 패턴일 뿐이다. 해당 디자인 패턴을\n구현하는 프레임웍이나 라이브러리는 많다. 그런데 Astro가 State Of JS 2022 기준으로는 가장 높은 Ratio를 기록하고 있다.\n\nState Of JS 2022 Rendering Frameworks\n[/_astro/ratios_over_time.B6TXcGVP_1SY1lA.webp]State Of JS 2022 Rendering\nFrameworks\n\n\nASTRO의 COMPONENT ISLAND\n\nAstro는 기본적으로 모든 웹을 zero client-side JS으로 생성한다. Astro는 React, Preact, Svelte,\nVue, SolidJS, AlpineJS, Lit 등으로 만든 UI컴포넌트들도 HTML을 만들긴 하지만 마찬가지로 JS를 걷어낸 상태로\n렌더링한다.\n\n렌더링 된 HTML에는 placeholder혹은 slot역할을 하는 <astro-island /> 요소가 자리잡게 된다. 아래는 현재 블로그에서\npreact로 구현된 Table Of Content (목차) 컴포넌트의 SSR결과 스크린샷이다. 보면 알겠지만 주요 컨텐츠를 포함하고 있다.\n\npreact로 개발된 TOC컴포넌트의 SSR 결과 [/_astro/astro-island.BD2bmuZr_1sLmtU.webp]preact로\n개발된 TOC컴포넌트의 SSR 결과\n\nAstro는 각 island에 대해 어느 순간에 어떻게 로드될 것인지에 대한 지시자를 사용할 수 있다. 이 블로그의 경우 뷰포트의 크기가\n1388px 이상일 때에만 우측에 TOC를 노출하고 있는데 따라서 아래와 같이 지시자를 추가하였다.\n\n<Layout>\n  <div class=\"wrapper\">\n    <div class=\"content\">{/* 본문 */}</div>\n    <TOC headings={headings} client:visible />\n  </div>\n</Layout>\n\n@media (max-width: 1388px) {\n  .toc {\n    display: none;\n  }\n}\n\n미디어쿼리를 통해 뷰포트가 작으면 <TOC />가 노출되지 않고. 조건에 해당될 경우 client:visible 지시자를 통해 화면에 노출될\n때에만 컴포넌트에 필요한 JS를 동적으로 로드하여 Hydration하게 된다.\n\n동적으로 로드된 JS들. react 런타임마저도 동적으로 로드되고 있다.\n[/_astro/network.CLUbBT6s_1h184y.webp]동적으로 로드된 JS들. react 런타임마저도 동적으로 로드되고 있다.\n\n이런 클라이언트 지시자\n[https://docs.astro.build/en/reference/directives-reference/#client-directives]들은\n위에서 소개한 뷰포트 노출 여부 뿐만 아니라 브라우저 활성화 상태, 미디어 쿼리 등 다양하게 지원하고 있으므로 적절하게 잘 사용하는 것 만으로도\n브라우저의 FCP를 위해 열심히 일하고 있는 메인 스레드를 방해하지 않을 수 있다.\n\n필요한 곳에 직접 구현해 적용해야 했던 과거와 달리 엄청 간편해진 셈이다.\n\n\nZERO CLIENT-SIDE JS\n\n어떤 기술을 사용하던 결국 모던 웹 앱 개발에서 TTI(인터렉티브가 가능해지기까지의 시간)를 개선하는 것은 JS를 얼만큼 줄이느냐의 싸움으로\n귀결된다. Netflix가 서비스를 Vanilla JS로 전환하고 TTI가 50% 감소했다\n[https://medium.com/dev-channel/a-netflix-web-performance-case-study-c0bcde26a9d9]는\n사례를 보면 JS가 차지하는 비중이 작지 않다는 것을 알 수 있다.\n\n아무런 내용 없이 create-react-app과 react-router-dom만을 사용한 앱의 번들 크기는 gzipped기준 약 70kB\n정도가 된다. history를 응용해 인앱 라우팅을 구현하는 코드만으로도 번들 크기가 꽤 늘어나는 것을 볼 수 있다.\n\nastro로 옮긴 후 측정한 결과. 이전에도 좋았지만 훨씬 좋아졌다.\n[/_astro/blog-lighthouse.N4Mr4sXd_ZpoXWb.webp]astro로 옮긴 후 측정한 결과. 이전에도 좋았지만 훨씬\n좋아졌다.\n\n이는 Astro로 개발한 이 블로그의 홈에서 이미지와 GA를 제외한 초기 사이즈 (약 30KB)대비 57.14%나 안 좋은 수치이다. (이\n블로그 뿐만 아니라 Astro공식 홈페이지의 Showcase에 있는 사이트들은 모두 매우 작은 크기를 자랑하고 있다)\n\nAstro가 용량이 작을 수 있는 건. 초기 설계 단에서 신경을 쓴것도 영향이 있겠지만 단지.. 이런 코드들이 없기 때문이다. Astro\nRouting API문서\n[https://docs.astro.build/en/core-concepts/routing/#navigating-between-pages]를\n확인해 보면 알겠지만 기본적으로 SPA를 위한 js가 포함되어 있지 않다. 페이지 이동은 <a href=\"..\" />를 사용한 전통적인\n리다이렉트에 의존하고 있다.\n\n또 Astro에서는 위에서 소개한 서드파티 컴포넌트 라이브러리 없이 Astro로 컴포넌트를 만들 경우에 대해서는. DOM에 이벤트를 붙이기 위해\ndocument.querySelector와 addEventListener를 사용해야 한다.\n\n그럼 이벤트 제거는 어떻게 하나? 안해도 된다. 왜냐면 <a href=\"..\" />로 페이지 자체가 이동되어 버리기 때문이다. (물론 이런\n경우가 필요하면 위에서 언급한 react, preact, solidjs 등을 사용하면 된다)\n\n페이지 로드 후 실행되는 코드들에 대해 트랜스파일은 해 주지만 별도 추가 코드가 포함되어 있지는 않다. 말 그대로 Zero Client-Side\nJS에 가까운 셈이다.\n\n\nASTRO를 도입해야 하나 말아야 하나?\n\ngatsby 공식 스타터 중 gatsby-starter-blog\n[https://www.gatsbyjs.com/starters/gatsbyjs/gatsby-starter-blog] 데모는 내용이 별로 많지\n않음에도 웹 폰트 제외 초기 JS의 크기만 100kB이다.\n\n이 블로그도 이전에 gatsby로 개발했었는데. 블로그 본질적인 기능만을 남긴다고 했을 때 불필요한 JS가 너무 많았다. 스타터 템플릿이 그런데\n예전 gatsby버전은 더 했을 것이다.\n\n글을 보여주는 기능 외 다른 리소스들은 아무리 사용자 경험을 개선한다는 목적이라고 해도 꼼꼼히 따져본다면 결국 불필요한 리소스가 되고 만다.\n이는 앱을 기획할 때 부터 앱의 성격에 따라 전략을 달리하는것이 올바르다 할 수 있다.\n\n현재까지 Astro를 포함 Island Architecture를 구현한 프로젝트들은 모두 적은 TTI를 강점으로 내새우고 있고 이는 정적 컨텐츠\n위주의 사이트에서 극대화된다. JS를 최소화하기 용이하도록 만들어졌기 때문이다.\n\n반면에 대시보드 앱과 같이 인터렉션이 필요한 UI가 많은 상황에서는 페이지 로드와 동시에 모든 JS를 받게 될 확률이 높을 것이다. 이렇게 되면\nAstro의 모든 장점을 극대화하기가 어려워진다. 이럴 땐 핸들러 단위로 JS를 쪼개주는 qwik\n[https://qwik.builder.io/]을 고려해보는 것이 좋겠다.\n\n\n결론\n\n아래는 위의 내용을 바탕으로 정리해본 생각들과 장점 및 단점이다\n\n장점\n\n * 정적 컨텐츠 위주의 사이트에서 JS를 최소하하여 TTI를 크게 개선할 수 있다 (페이지 로드 속도가 드라마틱하게 빨라진다)\n * react, preact, solidjs, lit 등 기존에 만들어진 컴포넌트를 그대로 쓸 수 있다\n * Island 별로 지시자를 통해 Hydration을 적절히 컨트롤할 수 있다\n\n단점\n\n * 동적 컨텐츠 위주의 사이트에서는 초기에 모든 JS를 로드해야 할 가능성이 높아 장점이 퇴색된다\n * 커뮤니티가 아직 많이 활성화되지 않았다\n\n티저 사이트, 블로그, 포트폴리오 사이트에는 무조건 쓰는 것이 좋겠다.\n\n> 참고\n> \n>  * Islands Architecture [https://jasonformat.com/islands-architecture/]\n>  * The Island Architecture\n>    [https://patterns-dev-kr.github.io/rendering-patterns/the-island-architecture/]","n":0.033},"1":{"v":"웹 성능 관점에서 본 Astro","n":0.447},"2":[{"v":"island architecture","i":2,"n":0.707},{"v":"performance","i":1,"n":1},{"v":"astro","i":0,"n":1}]}},{"i":7,"$":{"0":{"v":"서론\n\n현재 Web Push는 무료[1]이며 아래 조건들만 만족하면 이메일보다는 훨씬 나은것은 물론이고. SMS, 카카오톡 알림급의 전달력을 가진\n메시지를 무제한으로 보낼 수 있다.\n\n * 브라우저가 Web Push를 지원할 것\n * 사용자가 웹 알림 권한을 허용할 것\n * 웹 워커를 등록한 브라우저 프로세스가 실행되어 있을 것\n\nWeb Push를 구현하려면 원래 firebase 패키지를 설치하고. 사용자가 웹 알림 권한을 허용할 때 Firebase Cloud\nMessaging (이하 FCP) 플랫폼에 엔드포인트를 등록하는 코드를 직접 작성해야 한다.\n\n하지만 Angular는 Google에서 개발하는 것이라 그런지 @angular/service-worker 패키지에 이미 해당 기능이 구현되어\n있어 쉽게 Web Push를 구현할 수 있다.\n\nAngular의 @angular/service-worker패키지는 해당 기능을 포함해 App Shell, Runtime Caching,\nSmart Updates기능을 쉽게 구현할 수 있도록 기능을 제공하고 있다. 17년 12월 발표한 Service Worker 소개자료\n[https://javascript-conference.com/wp-content/uploads/2017/12/Automatic_Progressive_Web_Apps_using_Angular_Service_Worker_Maxim_Salnikov.pdf]에\n따르면 이를 Automatic Progressive Web Apps 로 소개하고 있다.\n\n\nWEB PUSH의 시스템 구조\n\nWeb Push 데이터 흐름 [/_astro/webpush.D_BqVeDG_ZQADYV.webp]Web Push 데이터 흐름\n 1. 사용자가 웹 서비스에 접속하면 웹 워커를 설치하게 된다\n 2. 알림 버튼을 클릭하면 SwPush.requestSubscription메서드를 호출하여 FCP에 등록하고\n    PushSubscription객체를 응답받는다\n 3. PushSubscription객체를 서버에 전송하여 어딘가에 저장해둔다\n 4. 서버에서 메시지를 PushSubscription을 참고하여 전송한다\n 5. FCP에서 PushSubscription 객체에 해당하는 웹 워커에게 메시지를 전달한다. 해당 웹 워커는 이를 읽어 알림을 출력한다\n\nSwPush [https://angular.io/api/service-worker/SwPush]는 Angular에서 제공하는 푸시 알림 등록\n구현체이다. 이 서비스를 통해 알림 구독 자체를 나타내는 PushSubscription\n[https://developer.mozilla.org/en-US/docs/Web/API/PushSubscription]객체를 만들 수 있다.\n이 객체는 json으로 변환 가능하므로 서버에 보내 저장했다가 원할 때 메시지를 보내도록 하는것이다.\n\n\nWEB PUSH 구현하기\n\n\nWEB-PUSH 설치 밎 키 생성하기\n\nnpm install --save web-push\n\nnpx web-push generate-vapid-keys\n\n=======================================\n\nPublic Key:\n{VAPID 공개키}\n\nPrivate Key:\n{VAPID 비밀키}\n\n=======================================\n\n\n위 명령 수행시 나오는 공개키와 비밀키는 해당 웹 서버기준으로 발급되는 것이므로 잘 기억해둔다. 서비스 도중에 이것이 바뀌면 기존 구독들에게\n메시지를 보낼 수 없게 된다.\n\n공개키는 environment.ts에 추가하고. .dotenv파일에는 둘 다 저장하여 백엔드에서 읽어다 쓸 수 있도록 하면 된다.\n\n// This file can be replaced during build by using the `fileReplacements` array.\n// `ng build` replaces `environment.ts` with `environment.prod.ts`.\n// The list of file replacements can be found in `angular.json`.\n\nexport const environment = {\n  production: false,\n  VAPIDPublicKey: '{VAPID 공개키}',\n}\n\n/*\n * For easier debugging in development mode, you can import the following file\n * to ignore zone related error stack frames such as `zone.run`, `zoneDelegate.invokeTask`.\n *\n * This import should be commented out in production mode because it will have a negative impact\n * on performance if an error is thrown.\n */\n// import 'zone.js/plugins/zone-error';  // Included with Angular CLI.\n\n# .dotenv\nVAPID_PUBLIC_KEY={VAPID 공개키}\nVAPID_PRIVATE_KEY={VAPID 비밀키}\n\n\nSERVICE WORKER 패키지 추가\n\nng add @angular/pwa --project <project-name>\n\n위 명령어를 사용하여 앱에 기본 서비스워커를 추가한다. 프로젝트에 서비스워커 추가하기\n[https://angular.io/guide/service-worker-getting-started#adding-a-service-worker-to-your-project]에\n해당 명령의 변경사항이 나와 있는데 직접 수정해도 된다.\n\n이제 앱을 빌드하면 결과물에 ngsw-worker.js가 추가되고. 이 파일이 AppModule의 ServiceWorkerModule\nimport에 추가되어 있다. 이 파일이 위 소개자료에 있는 기능들의 Angular 구현체이다.\n\n@NgModule({\n  declarations: [AppComponent, NotiComponent],\n  imports: [ServiceWorkerModule.register('ngsw-worker.js')],\n  providers: [WindowService],\n  bootstrap: [AppComponent],\n})\nexport class AppModule {}\n\n\n알림 버튼 만들기\n\n먼저 브라우저가 Web Push를 지원하는지 확인해야 한다. ua-parser-js와 compare-versions패키지를 활용하여 구분하고\n가능한 경우 알림 버튼을 노출한다.\n\n> ‘22 7/27 기준 web-push는 safari를 지원하지 않는다 web-push 브라우저 지원 범위\n> [https://github.com/web-push-libs/web-push#browser-support] 참고\n\nimport { compare } from 'compare-versions'\nimport UAParser from 'ua-parser-js'\n\n@Injectable()\nexport class WindowService {\n  isSupportNotification = false\n\n  constructor() {\n    const {\n      browser: { name, version },\n    } = new UAParser().getResult()\n\n    // 공식 문서 기준으로 필터링\n    this.isSupportNotification =\n      'Notification' in window &&\n      !!version &&\n      (name === 'Chrome' ||\n        compare(version, '52', '>=') ||\n        name === 'Edge' ||\n        compare(version, '17', '>=') ||\n        name === 'Firefox' ||\n        compare(version, '46', '>='))\n  }\n}\n\n다음 컴포넌트에서 서비스를 DI받아 버튼 노출을 제어한다. 클릭 시 위에 설명한대로 SwPush.requestSubscription메서드를\n호출하여 PushSubscription객체를 받고. 서버에 전송하여 저장하도록 한다.\n\nimport { SwPush } from '@angular/service-worker'\nimport { environment } from '../environments/environment'\n\n@Component({\n  template: `\n    <button\n      type=\"button\"\n      (click)=\"onClick()\"\n      *ngIf=\"win.isSupportNotification; else notSupport\"\n    >\n      알림 구독하기\n    </button>\n    <ng-template #notSupport> 알림을 지원하지 않는 브라우저입니다 </ng-template>\n  `,\n})\nexport class NotificationComponent {\n  constructor(\n    private swPush: SwPush,\n    private http: HttpClient,\n    protected win: WindowService\n  ) {}\n\n  onClick() {\n    this.swPush\n      .requestSubscription({ serverPublicKey: environment.VAPIDPublicKey })\n      .then((pushSubscription) => {\n        this.http.post('/api/subscribe', pushSubscription).subscribe()\n      })\n  }\n}\n\n\n백엔드 WEBPUSH 설정\n\n위에서 생성한 키를 읽어다 서버 실행과 동시에 web-push모듈을 초기화한다. 예제에서는 express 웹 서버를 사용했다.\n\n> web-push 모듈 사용시 메서드를 찾지 못한다는 류의 에러가 발생할 경우 예제 코드처럼 defaultExport를 사용하도록 한다.\n\nimport webpush from 'web-push'\n\n// .dotenv 읽기\nrequire('dotenv').config()\n\n// web-push 모듈 초기화\nwebpush.setVapidDetails(\n  'mailto: {관리용 이메일 주소 기입}',\n  process.env.VAPID_PUBLIC_KEY,\n  process.env.VAPID_PRIVATE_KEY\n)\n\n그 다음 클라이언트에서 전송한 PushSubscription 객체를 받아 저장하는 라우터를 작성한다.\n\nimport { Router, json, urlencoded } from 'express'\n\nconst router = Router()\n\nrouter.use(json({ limit: '5mb' }))\nrouter.use(urlencoded({ limit: '5mb', extended: true }))\n\nlet pushSubscription = null\n\nrouter.get('/api/subscribe', (req, res) => {\n  pushSubscription = req.body\n  res.send('ok').end()\n})\n\n알림을 보내야 할 땐 아래와 같이 저장했던 pushSubscription을 sendNotification\n[https://github.com/web-push-libs/web-push#sendnotificationpushsubscription-payload-options]의\n인자로 보내면 된다. 두 번째 인자에 메시지를 문자열이나 버퍼로 만들어 넘기면 되는데.\nServiceWorkerRegistration.showNotification\n[https://developer.mozilla.org/en-US/docs/Web/API/ServiceWorkerRegistration/showNotification]메서드로\nOS알림을 출력할 것이므로 json으로 만들어 넣는게 일반적이다.\n\nimport webpush from 'web-push'\n\nwebpush\n  .sendNotification(\n    pushSubscription,\n    JSON.stringify({ title: 'hello', body: 'world' })\n  )\n  .then((res) => {\n    console.log(res)\n  })\n  .catch((err) => {\n    console.error(err)\n  })\n\n\n웹 워커에서 OS알림 띄우기\n\n이제 마지막으로 웹 워커에서 FCP에서 전송한 알림을 OS에 띄워주면 된다. @angular/pwa설치 후 번들에 만들어지는\nngsw-worker.js에는 이 코드가 없으므로. 이 부분을 직접 구현하여 사용해야 한다.\n\n주석에 명시된대로 해당 js파일은 window가 아니라 WorkerGlobalScope\n[https://developer.mozilla.org/en-US/docs/Web/API/WorkerGlobalScope]에서 실행되기 때문에\n코드 작성시 주의해야 한다.\n\n/**\n * WorkerGlobalScope에서 동작하는 코드임. 컨텍스트에 대한 정보는 아래 문서 참고할 것\n * https://developer.mozilla.org/en-US/docs/Web/API/WorkerGlobalScope\n *\n * ngsw-worker.js 는 angular.json의 빌더 설정에 \"serviceWorker\" \"ngswConfigPath\"\n * 값이 설정되어 있으면 빌드 완료 후 자동으로 생성되는 Angular의 서비스 워커 구현 스크립트임\n *\n * 웹 푸시만 필요하다면 없어도 되지만 Automatic Service Worker의 기능 활용을 위해 포함한다\n */\nimportScripts('ngsw-worker.js')\n\n/**\n * Angular의 SwPush 서비스를 이용해 생성한 구독은 아래 인스턴스의 형대로 저장됨\n * https://developer.mozilla.org/en-US/docs/Web/API/PushSubscription\n *\n *\n * 서버에서 각 이벤트 발생 시 `web-push`라이브러리를 사용해 위에서 만든 구독 정보로\n * 데이터를 보내는데. 해당 구독 정보와 매칭되는 웹 워커에서 아래 이벤트가 발생함\n * https://developer.mozilla.org/en-US/docs/Web/API/PushEvent\n */\nself.addEventListener('push', (e) => {\n  const { title, body, ...data } = e.data.json()\n\n  if (!title || !body) {\n    return\n  }\n\n  self.registration.showNotification(title, { body, data })\n})\n\n/**\n * OS에 뜬 알림을 클릭했을 때 하는 동작. 예제는 url이 있다면 열어주는 코드이다\n */\nself.addEventListener('notificationclick', (e) => {\n  const url = e.notification?.data?.url\n\n  if (!url) {\n    return\n  }\n\n  self.clients.openWindow(url)\n})\n\n위 파일을 my-worker.js 로 만들어 src폴더 밑에 두고. angular.json과 app.module.ts를 수정한다\n\n{\n  \"targets\": {\n    \"build\": {\n      \"options\": {\n        \"assets\": [\n          \"src/my-worker.js\" // 빌드에 포함될 수 있도록 추가\n        ]\n      }\n    }\n  }\n}\n\n@NgModule({\n  declarations: [AppComponent, NotiComponent],\n  imports: [\n    ServiceWorkerModule.register('my-worker.js'), // 수정\n  ],\n  providers: [WindowService],\n  bootstrap: [AppComponent],\n})\nexport class AppModule {}\n\nWeb Push 구현 결과 [/_astro/webpush-demo.WegUnp9Z_ZQfst9.webp]Web Push 구현 결과\n\n위처럼 http 에 localhost 에서도 잘 동작하는것을 확인할 수 있다. cu 로고는 OS알림을 띄울 때 추가로 넣을 수 있으니\nshowNotification\n[https://developer.mozilla.org/en-US/docs/Web/API/ServiceWorkerRegistration/showNotification]\nAPI를 참고하면 된다.\n\n글을 작성하며 테스트해본 코드를 Angular 14 웹 푸시 테스트 리포지토리\n[https://github.com/johnny-mh/ng14-web-push-example]에 푸시해 두었다. 서버 실행 방법을\nREADME.md 에 적어두었으니 클론받아서 직접 테스트해볼 수 있다. 본문과 다르게 백엔드를 SSR로 구현해놓았으므로 참고 바란다.\n\n\n주석\n\n> 1. https://firebase.google.com/docs/cloud-messaging\n> [https://firebase.google.com/docs/cloud-messaging]","n":0.032},"1":{"v":"Angular앱에 Web Push 구현하기","n":0.5},"2":[{"v":"push notification","i":2,"n":0.707},{"v":"web push","i":1,"n":0.707},{"v":"angular","i":0,"n":1}]}},{"i":8,"$":{"0":{"v":"GATSBY에 검색을 추가하는 방법\n\n블로그에 글이 늘어나면 카테고리와 태그만으로는 원하는 내용을 찾기 어려워진다. 지금 블로그가 그 정도까지 내용이 많지는 않지만 학습을 위해\n운영하는것도 있어서 검색 기능을 구현해 보았다.\n\n구현 완료된 검색 화면 (메뉴바의 검색 버튼을 눌러 사용해볼 수 있다)\n[/_astro/gatsby-search.DTDkpCdy_Z1uCTJJ.webp]구현 완료된 검색 화면 (메뉴바의 검색 버튼을 눌러 사용해볼 수\n있다)\n\n공식 사이트의 gatsby 에 검색 기능 추가\n[https://www.gatsbyjs.com/docs/how-to/adding-common-features/adding-search/] 문서를\n보면 검색에 필요한 기본 요소들에 대한 설명과 함께 gatsby에 검색을 구현하는 2가지 방법을 소개하고 있다.\n\n첫 번째 방법은 클라이언트 측 검색이다. 빌드 혹은 런타임에 데이터를 인덱싱하고 이를 이용해 로컬에서 검색을 수행하는 방법이다. 위의 공식\n문서에서는 js-search, gatsby-plugin-elasticlunr-search, gatsby-plugin-local-search 를\n활용하라고 안내하고 있다.\n\n두 번째 방법은 API기반 검색엔진을 활용하는 방법으로 Algolia와 같은 외부 서비스를 활용한다. 빌드 시점에 검색 대상 데이터들을 인덱싱해\n외부 서비스에 업로드해 두고 런타임에는 API로 검색한다.\n\nAPI기반 검색엔진사용 시 블로그의 빌드 배포 프로세스에 인덱스를 전송해야 하는 번거로움과. 사용 시 비용이 발생하거나. 무료인 경우 횟수에\n제약이 있어 사용하고 싶지 않았다. 결국 클라이언트 측 검색 방법으로 검색을 구현했다. 이 글에서는 구현 과정에 대해서 다룬다.\n\n\n검색 라이브러리와 한글 이슈\n\n사실 검색은 단순히 글 목록을 순회하며 텍스트가 포함되어있는지를 검사해도 된다. 하지만 실제 사용시에는 포함 뿐만 아니라 상황에 따라 아래의\n기능들을 필요로 하는데 이 경우 추가 구현이 필수이므로. 대개 별도의 텍스트 검색 라이브러리를 사용하게 된다.\n\n * 배열의 모든 요소를 순회하지 않고도 빠르게 검색할 수 있는 기능 (인덱스 생성 및 사용)\n * 제목보다 본문에 조금 더 가중치를 두고 검색 (필드 가중치 적용)\n * 검색어 하이라이팅을 위한 매칭 텍스트 위치 배열\n * and, or 논리 연산 검색 등\n\ngatsby 공식 사이트의 gatsby 에 검색 기능 추가\n[https://www.gatsbyjs.com/docs/how-to/adding-common-features/adding-search/]\n문서에는 js-search, flexsearch, lunr 세가지 라이브러리를 추천하고 있는데. 설명 자체가 장황하거나 flexsearch,\nlunr는 둘 다 한글 검색 관련 문제[1]가 있어 사용이 어려웠다.\n\n한참을 디버깅하다 결국 실무에서 특별한 문제없이 잘 사용하고 있는 fuse.js를 활용해보기로 했다.\n\n\nFUSE.JS의 사용\n\nfuse.js [https://fusejs.io/]는 텍스트 검색 라이브러리로 위에 적힌 모든 기능을 제공하며. 브라우저 / NodeJS환경\n둘다에서 사용할 수 있으므로 gatsby의 빌드 과정에서는 인덱스를 생성해 두고. 런타임에 사용하여 검색하도록 구성하면 된다.\n\nfuse.js에 대한 자세한 내용은 공식 사이트에서 확인할 수 있으니 생략하고. 본문에서는 이 글의 핵심인 인덱스의 생성 및 사용 방법에\n대해서만 다룬다.\n\n\n인덱스 생성 및 사용\n\nimport Fuse from 'fuse.js'\n\nconst books = [\n  {\n    title: \"Old Man' War\",\n    author: { firstName: 'John', lastName: 'Scalzi' },\n  },\n  {\n    title: 'The Lock Artist',\n    author: { firstName: 'Steve', lastName: 'Hamilton' },\n  },\n  // {...}, {...}, ...\n]\n\n// index 생성\nconst index = Fuse.createIndex(['title', 'author.firstName'], books)\n\n// 예시를 위해 로컬스토리지에 저장한다\nlocalStorage.setItem('index', JSON.stringify(index.toJSON()))\n\ncreateIndex메서드 호출로 만들어진 index객체는 fusejs가 books를 검색하는데 있어 순회를 줄일 수 있는 정보를 담고 있는\n인스턴스이다. 이 인스턴스는 toJSON()메서드를 통해 json문자열로 변환도 가능하여. 스토리지에 담거나 API로 보내거나 하는 등 유용하게\n쓸 수 있다\n\nimport Fuse from 'fuse.js'\n\nconst index = Fuse.parseIndex(JSON.parse(localStorage.getItem('index')))\n\n// 원본 데이터와 인덱스를 가지고 최적화 된 검색을 수행할 수 있는 인스턴스 생성\nconst fuse = new Fuse(books, undefined, index)\n\nconst result = fuse.search('검색어')\n\n이렇게 만들어진 인덱스 인스턴스와 원본 데이터를 가지고 텍스트 검색을 수행할 수 있다.\n\n\nGATSBY에서 FUSE.JS 활용하기\n\n앞서 언급했던 대로 빌드 과정에서는 목록을 인덱싱하여 어딘가 저장해두어야 하고 이렇게 생성된 데이터는 블로그의 런타임에 fuse.js 인스턴스를\n만들어 사용하도록 구성해야 한다.\n\n이때 빌드 과정은 플러그인을 활용하면 되고. 런타임 검색은 훅을 활용하면 된다.\n\n\nGATSBY-PLUGIN-FUSEJS\n\nnpm install gatsby-plugin-fusejs\n\n설치 후 gatsby-config.js에 인덱스가 만들어지기 원하는 데이터의 쿼리, 데이터 중에서도 검색이 되었으면 하는 프로퍼티,\ngraphql결과물을 단순 객체 배열로 변환하기 위한 함수를 옵션으로 전달한다\n\nmodule.tsports = {\n  plugins: [\n    {\n      resolve: `gatsby-plugin-fusejs`,\n      options: {\n        // 인덱스를 만들고자 하는 데이터의 쿼리\n        query: `\n          {\n            allMarkdownRemark {\n              nodes {\n                id\n                rawMarkdownBody\n                frontmatter {\n                  title\n                }\n              }\n            }\n          }\n        `,\n\n        // 인덱스를 만들고자 하는 데이터의 프로퍼티\n        keys: ['title', 'body'],\n\n        // graphql의 결과물을 단순 객체 배열로 변환하는 함수\n        normalizer: ({ data }) =>\n          data.allMarkdownRemark.nodes.map((node) => ({\n            id: node.id,\n            title: node.frontmatter.title,\n            body: node.rawMarkdownBody,\n          })),\n      },\n    },\n  ],\n}\n\n옵션으로 만들어진 인덱스를 아래 스크린샷과 같이 활용할 수 있게 되었다\n[/_astro/gatsby-plugin-fusejs.CLCx_raF_1QrTIO.webp]옵션으로 만들어진 인덱스를 아래 스크린샷과 같이\n활용할 수 있게 되었다\n\n\nREACT-USE-FUSEJS\n\nnpm install react-use-fusejs\n\n다음은 만들어진 인덱스를 활용하기 위해 fuse.js 훅을 설치한다. 예제에서는 Search라는 컴포넌트를 만들고. useStaticQuery로\n컴포넌트 단위 쿼리로 인덱스와 원본 데이터를 불러온 후 검색어 입력시마다 결과를 화면에 노출하고 있다.\n\nimport { graphql, useStaticQuery } from 'gatsby'\nimport * as React from 'react'\nimport { useGatsbyPluginFusejs } from 'react-use-fusejs'\n\nexport function Search() {\n  const data = useStaticQuery(graphql`\n    {\n      fusejs {\n        index\n        data\n      }\n    }\n  `)\n\n  const [query, setQuery] = React.useState('')\n\n  // fusejs 객체를 가공 없이 그대로 넘긴다\n  const result = useGatsbyPluginFusejs(query, data.fusejs)\n\n  return (\n    <div>\n      <input\n        type=\"text\"\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n      />\n      <ul>\n        {result.map(({ item }) => (\n          <li key={item.id}>{item.title}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n\nexport default Search\n\n위 예제의 경우 검색어 입력시 검색결과가 즉시 화면에 노출되는데. 이 것을 조절하려면 query의 변화에 throttle이나 debounce를\n활용하면 해결할 수 있을 것이다.\n\n\n인덱스를 LAZY LOADING하기\n\nSearch컴포넌트는 렌더링 즉시 인덱스를 파싱하여 인스턴스를 만들기 때문에 검색을 하지 않더라도 자원을 소모하게 된다. 문서량이 적다면\n괜찮겠지만 많아지는 경우 신경이 쓰일 수 있는데. 그 경우를 위해 Lazy Loading을 적용할 수 있다.\n\n아래 코드는 실제 검색 키워드가 입력될 때 즉시 인덱스를 다운로드 받고 파싱하여 검색을 수행하는 예제이다.\n\nimport { graphql, useStaticQuery } from 'gatsby'\nimport * as React from 'react'\nimport { useGatsbyPluginFusejs } from 'react-use-fusejs'\n\nexport function Search() {\n  const data = useStaticQuery(graphql`\n    {\n      fusejs {\n        publicUrl\n      }\n    }\n  `)\n\n  const [query, setQuery] = React.useState('')\n  const [fusejs, setFusejs] = React.useState(null)\n  const result = useGatsbyPluginFusejs(query, fusejs)\n\n  const fetching = React.useRef(false)\n\n  React.useEffect(() => {\n    if (!fetching.current && !fusejs && query) {\n      fetching.current = true\n\n      fetch(data.fusejs.publicUrl)\n        .then((res) => res.json())\n        .then((json) => setFusejs(json))\n    }\n  }, [fusejs, query])\n\n  return (\n    <div>\n      <input\n        type=\"text\"\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n      />\n      <ul>\n        {result.map(({ item }) => (\n          <li key={item.id}>{item.title}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n\nexport default Search\n\n\n인덱스 재사용하기\n\n위의 예제에서 다운로드 받아 만든 fuse.js 데이터는 컴포넌트가 제거되면 함께 사라진다. 따라서 데이터를 컨텍스트에 담아 재사용하도록 한다.\n\n// src/context/app.jsx\nimport { createContext, useState } from 'react'\n\nexport const AppContext = createContext({\n  fusejs: null,\n  setFusejs: () => {},\n})\n\nexport const AppProvider = ({ children }) => {\n  const [fusejs, setFusejs] = useState(null)\n\n  return (\n    <AppContext.Provider value={{ fusejs, setFusejs }}>\n      {children}\n    </AppContext.Provider>\n  )\n}\n\n// gatsby-browser.js\nimport { AppProvider } from './src/context/app'\n\nexport const wrapRootElement = ({ element }) => {\n  return <AppProvider>{element}</AppProvider>\n}\n\n// src/components/Search.jsx\nimport { AppContext } from '../context/app'\nimport { graphql, useStaticQuery } from 'gatsby'\nimport * as React from 'react'\nimport { useGatsbyPluginFusejs } from 'react-use-fusejs'\n\nexport function Search() {\n  const data = useStaticQuery(graphql`\n    {\n      fusejs {\n        publicUrl\n      }\n    }\n  `)\n\n  const [query, setQuery] = React.useState('')\n  const [fusejs, setFusejs] = React.useContext(AppContext)\n  const result = useGatsbyPluginFusejs(query, fusejs)\n\n  const fetching = React.useRef(false)\n\n  React.useEffect(() => {\n    if (!fetching.current && !fusejs && query) {\n      fetching.current = true\n\n      fetch(data.fusejs.publicUrl)\n        .then((res) => res.json())\n        .then((json) => setFusejs(json))\n    }\n  }, [fusejs, query])\n\n  return (\n    <div>\n      <input\n        type=\"text\"\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n      />\n      <ul>\n        {result.map(({ item }) => (\n          <li key={item.id}>{item.title}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n\nexport default Search\n\n\n> 주석\n> \n> 1. lunr의 한글 검색 이슈는 Jekyll에 lunr.js 붙이기 (+ 한국어 검색 문제 해결)\n> [https://cjeon.com/2016/05/29/Jekyll-lunr.html] 에서 볼 수 있고. flexsearch의 한글 검색\n> 이슈는 gatsby를 이용한 Github blog 개발후기\n> [https://jaeseokim.dev/React/gatsby-blog-%EA%B0%9C%EB%B0%9C-%ED%9B%84%EA%B8%B0/#%EB%91%90-%EB%B2%88%EC%A7%B8-%EA%B3%A0%EB%AF%BC---%EA%B2%80%EC%83%89-%EA%B8%B0%EB%8A%A5]에서\n> 볼 수 있으며. 공식 문서\n> [https://github.com/nextapps-de/flexsearch#cjk-word-break-chinese-japanese-korean]를\n> 따라해도 동작하지 않는다.","n":0.031},"1":{"v":"gatsby 블로그에 검색을 붙여보자","n":0.5},"2":[{"v":"fusejs","i":2,"n":1},{"v":"search","i":1,"n":1},{"v":"gatsby","i":0,"n":1}]}},{"i":9,"$":{"0":{"v":"BLOOM FILTER\n\nBloom Filter는 어떤 Set안에 특정 값이 존재하는지 여부를 빠르게 계산할 수 있는 알고리즘이다. 속도도 빠르면서 저장 공간을 적게\n차지하기 때문에 대규모 서비스에서 신규회원 여부, 크롬 브라우저의 멀웨어 사이트 여부, hbase, redis 등 여러 군데에 활용되고 있다.\n\n재미있는 점은 이 Bloom Filter 알고리즘은 확률적 자료 구조라는 점이다. 계산의 정확도가 100%가 아니다. False Positive\nProbability(이하 FPP) 즉 거짓을 반환할 확률이 있다. 정확도를 희생하고 메모리 사용량을 얻은 셈이다.\n\n이 FPP는 0%가 될 순 없지만 일반적으로 알고리즘의 각 변수를 제어하여 0% 에 수렴하는 최적의 값을 찾아서 도입한다. 이에 관련한 자세한\n내용은 뒤에서 다루고. 먼저 원리에 대해 간단히 설명한다.\n\n\n동작 원리\n\n이 알고리즘은 요소들이 담길 공간, 필터, 해시 함수가 필요하다. 공간은 앞서 언급한 대로 회원 DB등 특정 값이 존재하는지 검사할 집합이다.\n필터는 0, 1을 표현할 수 있는 Bit의 배열이다. 해시 함수는 특정 값을 넣었을 때 항상 똑같은 길이의 값을 반환하는 함수를 말한다.\n\n설명을 위해 공간은 문자열 배열, 필터는 6칸을 사용하고. 해시는 길이 6의 값을 출력하는 hashA, hashB 를 사용한다고 하자. 그리고\n코드는 이해를 돕기 위해 비트를 나열한 수도 코드를 사용한다.\n\n\n배열에 값 추가하기\n\n먼저 배열에 요소를 추가할 땐 항상 해시함수 2개를 돌린 값을 OR 비트 연산하고 이를 필터와 OR연산하여 필터를 업데이트한다.\n\narr = []\nfilter = 000000\nnewValue = 'owl'\n\nh1 = hashA(newValue) // 010000\nh2 = hashB(newValue) // 001000\n\n(filter |= h1) |= h2 // 011000\narr.push(newValue)\n\n요소 owl을 추가하여 필터 값은 011000이 되었다. 그리고 이어 값을 하나 더 추가한다.\n\nnewValue = 'hawk'\n\nh1 = hashA(newValue) // 100000\nh2 = hashB(newValue) // 000010\n\n(filter |= h1) |= h2 // 111010\narr.push(newValue)\n\n이제 필터의 값은 111010이 되었다.\n\n\n🍳 값 존재 여부 검사하기\n\n이제 처음 추가한 owl이 존재하는지 여부를 검사해 보자.\n\ncheckValue = 'owl'\n\nh1 = hashA(checkValue) // 010000\nh2 = hashB(checkValue) // 001000\n\nh = h1 | h2 // 011000\n\n!!(filter & h) // 011000 => true\n\n해시 함수를 돌린 값 011000 과 현재 필터 값 111010을 AND연산하면 값은 011000으로 Boolean변환 시 true가 된다. 이\n값은 배열에 존재할 확률이 높다. 다음은 존재하지 않는 값에 대한 검사이다.\n\ncheckValue = 'duck'\n\nh1 = hashA(checkValue) // 000100\nh2 = hashB(checkValue) // 000001\n\nh = h1 | h2 // 000101\n\n!!(filter & h) // 000000 => false\n\n해시 함수를 돌린 값 000101과 현재 필터 값 111010을 AND연산하면 값은 000000이므로 Boolean변환 시 false가 된다.\n이 값은 배열에 존재하지 않는다. 👀 여기까지 따라왔다면 중요한 문제인 다음 예제를 보자.\n\ncheckValue = 'crow'\n\nh1 = hashA(checkValue) // 001000\nh2 = hashB(checkValue) // 000010\n\nh = h1 | h2 // 001010\n\n!!(filter & h) // 001010 => true\n\n값 **‘crow’**는 배열에 없지만 해시 값은 001010이므로 연산 결과에서 true가 나왔다. 이것이 바로 Bloom Filter의\nFPP특성. 없는 값을 있는 것으로 계산할 수 있는 특성이다. 이런 특성에도 불구하고 이 알고리즘 사용하는 이유는 메모리 사용량이 적다는 점\n때문이다.\n\n일반적으로 배열에 특정 값이 존재하는지 빠르게 찾기 위해서 값을 키로 갖는 객체를 만들어 사용한다. 해당 객체 안에 키 값이 있다면 배열 내에\n값이 존재하는 것이다. 이 경우 객체외 키의 갯수만큼의 메모리 공간을 더 사용해야 한다. 하지만 예제의 필터에서는 6칸의 배열만 필요했다.\n실무에서도 256길이의 필터면 대 부분 해결되는 수준이다.\n\n> 혹시 위에 언급하는 예제를 데모로 보고 싶다면 Bloom Filters by Example\n> [https://llimllib.github.io/bloomfilter-tutorial/]을 참고하기 바란다.\n\n실제로 서비스에 적용할 땐. 이 FPP를 0%에 근접하도록 각 변수의 값을 계산해 적용한다. 위의 예제에서 바꾼다면 필터의 길이를 256과 같이\n매우 길게 하고. 해시 함수도 20개 이상으로 많이 사용하면 정확도가 올라갈 것이다. 계산식도 나와 있는데 Bloom Filter 사이즈 계산기\n[https://hur.st/bloomfilter]에서 확인해 볼 수 있다.\n\n요소에 들어가는 아이템의 개수 (n), 원하는 FPP값(p), 필터의 비트 개수(m), **해시 함수의 개수(k)**를 부분적으로 입력하면\n입력하지 않은 변수의 최적의 값을 뽑아 준다.\n\nBloom Filter는 Backend뿐만 아니라 Frontend에서도 얼마든지 사용할 수 있다. 해시 함수의 정확도나 비용을 고려했을 때는\n대량의 데이터를 다루긴 어려울 수 있지만. 웹어셈블리를 활용한다면 이 제약도 없다.\n\n\nANGULAR의 BLOOM FILTER\n\n> 로직 설명은 Angular DI: Getting to know the Ivy NodeInjector\n> [https://medium.com/angular-in-depth/angular-di-getting-to-know-the-ivy-nodeinjector-33b815642a8e]\n> 에 상세히 나와 있다. 그런데 글이 조금 어려워서 알기 쉽게 풀어 설명해 본다.\n\n> ⚠ 예제는 Angular v8.3.28 기준이다.\n\nAngular는 v8버전부터 Ivy Renderer가 추가되면서 대대적인 성능 향상 작업이 이루어졌다. 그 수 많은 개선사항 중 Bloom\nFilter는 NodeInjector에서 특정 Provider의 존재 여부를 검사하는 로직에 적용되어 있다.\n\nIvy가 템플릿 렌더링에 사용하는 IncrementalDOM [https://github.com/google/incremental-dom]은\n전통적으로 사용하던 VirtualDOM 에 비해 메모리 사용량이 적다는 특징이 있는데. Bloom Filter 알고리즘도 메모리 사용량이\n적으므로 시너지가 있을 것으로 보인다.\n\nAngular는 아래 코드처럼 특정 Component의 생성자에 특정 타입의 파라미터를 추가하면. 런타임에 알아서 타입에 맞는 의존 인스턴스를\n찾아 없으면 만들어서 주고 있으면 주는 DI 시스템이 있다.\n\n// PARENT\n@Component({\n  template: `<app-hello></app-hello>`,\n})\nclass AppComponent {}\n\n// CHILD\n@Component({\n  selector: 'app-hello',\n})\nclass HelloComponent {\n  // AppComponent 를 생성자에서 주입받고 있다\n  constructor(private app: AppComponent) {}\n}\n\n위 코드에서 HelloComponent는 자신을 그린 상위 컴포넌트 AppComponent를 DI받고 있다(생성자에 같은 타입의 파라미터를\n기입함). 따라서 HelloComponent는 인스턴스 생성 시점에 첫 번째 인자로 AppComponent의 인스턴스를 받을 수 있다.\n\n이 때 AppComponent타입의 인스턴스를 찾기 위해 Angular내부적으로 먼저 NodeInjector를 거슬러 올라가며 찾고 찾지 못하면\n이어 ModuleInjector를 찾아 올라가는 동작을 하게 된다.\n\n이 중 NodeInjector는 Component들의 계층 구조에서 bootstrap된 최 상위 컴포넌트 까지 마치 DOM트리에서 버블링이\n일어나는 것 처럼 각 Component마다의 providers, viewProviders를 살펴본다.\n\nAngular의 Injector Chain [/_astro/injector-chain.JJ_q07XQ_1sRPUB.webp]Angular의\nInjector Chain\n\n이 때 Component의 메타데이터에 설정한 배열을 직접 살피는 게 아니라. 각 노드에 있는 Injector 인스턴스에게 특정 타입으로 생성된\n인스턴스가 있는지 묻게 된다. 바로 이 과정\n[https://github.com/angular/angular/blob/8.2.14/packages/core/src/render3/di.ts#L332]에서\nBloom Filter를 사용\n[https://github.com/angular/angular/blob/8.2.14/packages/core/src/render3/di.ts#L588]한다.\n\n아래 코드는 해시와 필터를 AND연산하여 존재 유무를 판별하는 로직을 가져와서 조금 해석해 보았다.\n\nexport function bloomHasToken(\n  // 컴포넌트 ID의 해시 값 (입력값)\n  // 위의 데모 코드에서 AppComponent는 처음으로 처리되기에 값이 0이다.\n  // 따지고 보면 Provider의 종류가 공식에서 (n)이 된다.\n  bloomHash: number,\n  // JS의 비트연산은 32비트라 256짜리 필터를 담을 수 없어 필터를 32비트씩 8개로 쪼갠 것으로 보인다\n  injectorIndex: number,\n  // 순수함수라 배열을 세 번째 인자로 넘기고 있다\n  injectorView: LView | TData\n) {\n  // Create a mask that targets the specific bit associated with the directive we're looking for.\n  // JS bit operations are 32 bits, so this will be a number between 2^0 and 2^31, corresponding\n  // to bit positions 0 - 31 in a 32 bit integer.\n  const mask = 1 << bloomHash\n  const b7 = bloomHash & 0x80\n  const b6 = bloomHash & 0x40\n  const b5 = bloomHash & 0x20\n\n  // Our bloom filter size is 256 bits, which is eight 32-bit bloom filter buckets:\n  // bf0 = [0 - 31], bf1 = [32 - 63], bf2 = [64 - 95], bf3 = [96 - 127], etc.\n  // Get the bloom filter value from the appropriate bucket based on the directive's bloomBit.\n  let value: number\n\n  // 8개 필터를 모두 사용하는 것이 아니라. 위에서 만든 플래그 기준으로\n  // 어떤 것을 써야 하는지 분기하고 있다. 즉 실제로 사용하는 필터 자체는 32비트이다.\n  if (b7) {\n    value = b6\n      ? b5\n        ? injectorView[injectorIndex + 7]\n        : injectorView[injectorIndex + 6]\n      : b5\n        ? injectorView[injectorIndex + 5]\n        : injectorView[injectorIndex + 4]\n  } else {\n    value = b6\n      ? b5\n        ? injectorView[injectorIndex + 3]\n        : injectorView[injectorIndex + 2]\n      : b5\n        ? injectorView[injectorIndex + 1]\n        : injectorView[injectorIndex]\n  }\n\n  // 앞서 공부했던 대로 AND연산하여 존재 여부를 확인하고 있다.\n  // If the bloom filter value has the bit corresponding to the directive's bloomBit flipped on,\n  // this injector is a potential match.\n  return !!(value & mask)\n}","n":0.031},"1":{"v":"Bloom Filter알고리즘과 Angular DI 성능 개선","n":0.408},"2":[{"v":"angular","i":2,"n":1},{"v":"algorithm","i":1,"n":1},{"v":"bloomfilter","i":0,"n":1}]}},{"i":10,"$":{"0":{"v":"STANDARD-VERSION 소개\n\nlerna에는 커밋 메시지를 읽어 자동으로 새로운 버전과 CHANGELOG를 작성하고 git 태그를 작성해 주는 기능(이하 버전 자동화)도\n포함되어 있는데. 이 기능을 활용하면 운영에 큰 도움이 된다.\n\n이 기능을 사용하려면 —conventional-commits 인자를 전달해야 한다. 이름에서 알 수 있듯 이 기능을 사용하려면 커밋 메시지들을\nConventional Commit [https://www.conventionalcommits.org/ko/v1.0.0/]컨벤션에 맞춰 작성해야\n한다.\n\nlerna말고도 같은 기능을 제공하는 도구가 몇개 있는데. 그 중 standard-version\n[https://github.com/conventional-changelog/standard-version]은 단일 리포지토리에 버전 자동화를\n제공하는 도구이다.\n\n이 도구는 태생 자체가 버전 자동화라서 lerna보다 직관적이고. 다루기 쉽고. 조금 더 다양한 기능들을 제공한다. 만들어 내는 태그의 커밋\n메시지 포멧을 변경하거나 changelog를 뽑아 API를 호출하는 등 여러가지로 응용할 수 있다.\n\n이 글에서는 최근에 이 standard-version을 이용해 사내 서비스 배포와 롤백 시스템을 구축했던 경험을 정리한다.\n\n\n서비스를 위한 버전 자동화\n\n지난 글 모노레포 도입 검토\n[/post/%EC%95%B1%EA%B3%BC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EA%B4%80%EB%A6%AC%EC%97%90-Monorepo]\n단계에서는 lerna를 완전히 적용하며 자동 버저닝을 서비스에서도 사용할 수 있을 것이라 예상했다.\n\n하지만 서비스들의 커밋이 섞여 각 담당자들이 라이브러리 수정 코드만을 보고 사이드 이펙트를 예측할 수 있을 지 확실하지 않다는 문제로 진행하지\n않기로 했고. 서비스는 다른 방법을 찾아야만 했다.\n\n서비스 사이드이펙트 문제 [/_astro/side-effect.wJ0eN_Vv_2oVHSw.webp]서비스 사이드이펙트 문제\n\n그 때 Conventional Commit 사이트에 소개된 툴 중에 standard-version을 도입하여 현재 문제 없이 운영하고 있다.\n\n부끄러운 이야기지만 도입 전 약 5년 동안이나 프론트 코드의 롤백은 수동으로 할 수밖에 없었다. 대부분의 배포 후 사이드이펙트는 서버API의\n문제였기 때문에 크게 필요성을 느끼지 못했던 부분도 있다.\n\n하지만 FE개발파트가 커지고 사내 모든 서비스를 담당하게 되면서 시스템의 필요성이 생겨 지금에라도 도입하게 되었다.\n\n\n새로 도입한 배포 프로세스\n\n> 먼저 그림으로 보고 각 단계에서 하는 일들을 아래에서 설명한다\n\n배포 프로세스 [/_astro/deploy-strategy.Dtp7EMUc_ZmgYzT.webp]배포 프로세스\n\n\n1. 기능 개발\n\n커밋 메시지는 Conventional Commit [https://www.conventionalcommits.org/ko/v1.0.0/]을\n준수해야 한다. 작성이 어렵다면 도움을 주는 commitizen [https://github.com/commitizen/cz-cli]을 사용하면\n좋다. 만약 cli에 익숙하지 않다면 어댑터 [https://github.com/commitizen/cz-cli#adapters]를 사용하여\n원하는 도구에서도 commitizen을 이용할 수 있다.\n\n# 커밋 메시지 작성을 대화형으로 도와주는 도구 설치\nnpm install -g commitizen\n\n# 프로젝트를 commitizen friendly 하게 만들어야 한다\ncommitizen init cz-conventional-changelog --save-dev --save-exact\n\n# 설치하고 나면 아래 명령으로 커밋 메시지를 대화형 도구로 작성할 수 있다\ngit cz\nSelect the type of change that you're commiting: (use arrow keys)\n> feat:     A new feature\n  fix:      A bug fix\n  docs:     Documentation only changes\n  styles:   Changes that do not effect the meaning of the code\n            (white-space, formatting, missing semi-colons, etc)\n  refactor: A code change that neither fixes a bug or add a feature\n  perf:     A code change that improves performance\n\n\n커밋 전에 커밋 메시지들이 컨벤션에 맞게 작성되었는지 검사해 주면 좋다. commitlint\n[https://github.com/conventional-changelog/commitlint]와 husky\n[https://github.com/typicode/husky]를 이용한다.\n\n# 커밋 시 커밋 메시지들을 검사하기 위한 도구 설치\nnpm install -D commitlint husky\n\n// package.json 에 커밋할 때 메시지들을 검사하도록 수정\n{\n  \"husky\": {\n    \"hooks\": {\n      \"commit-msg\": \"commitlint -E HUSKY_GIT_PARAMS\"\n    }\n  }\n}\n\n\n2. CI PR CHECK\n\nPull Request를 만들어 코드리뷰를 한다면 PR Check 기능을 이용해 커밋 메시지를 검사해 준다. 실수로 잘못된 커밋 메시지의 커밋이\n푸시될 수 있기 때문이다. PR Check을 하지 않는다면 이 단계는 건너뛰고 다음 빌드 과정에서 하면 된다.\n\n대부분의 유명 오픈소스 프로젝트는 모두 PR Check에서 이 검사를 한다\n[/_astro/pr-build.CiUSv7a0_Z8lDR8.webp]대부분의 유명 오픈소스 프로젝트는 모두 PR Check에서 이 검사를 한다\n\n# 커밋 메시지들이 컨벤션에 맞게 작성되었는지 검사한다\ngit log -1 --pretty=format:\"%s\" | npx commitlint\n\n\n3. CI 빌드 설정\n\n> 전 단계를 건너뛰었다면 이 단계에서 커밋 메시지를 검사하도록 해 준다.\n\n배포 빌드에서는 특정 커밋 메시지엔 빌드가 돌지 않도록 설정해야 한다. 위의 그림에서 B-1커밋은 개발 장비에서 배포를 위해\nstandard-version명령을 실행하여 package.json의 버전 범핑과 CHANGELOG.md의 수정사항 커밋이 추가된 경우인데 이때\n빌드는 무의미하다.\n\nJENKINS기준으로는 Git Plugin > Polling ignores commits with certain messages 항목을 추가하고\n값은 (?s).*chore\\(release\\).*로 설정하면 된다. 비슷한 기능을 Git Actions, Travis나 Circie 에서도\n지원하므로 똑같이 설정하면 된다.\n\n\n4. CDN 업로드\n\nCI가 만든 css, js, jpg, html등의 리소스 파일을 CDN에 업로드하도록 한다. 이 때 서비스가 여러개라면 겹치지 않도록 나누어야\n한다. 또 테스트 서버가 여럿이라면 또 페이즈별로 겹치지 않도록 폴더를 나누어 주어야 할 것이다. 아래는 실제로 사용하고 있는 주소의 형태를\n변형한 예제이다.\n\n> 서비스 A 페이즈: beta\n> \n>  1. https://mycdn.net/mycompany/service-a/beta/2b30274/\n>     \n>     위 경로에 빌드 결과물들을 업로드한다.\n> \n>  2. https://mycdn.net/mycompany/service-a/beta/latest\n>     \n>     이 파일에는 1번의 url (최신 버전이 무엇인지)를 기록해 둔다\n> \n> 서비스: B, 페이즈: real\n> \n>  1. https://mycdn.net/mycompany/service-b/real/2b30274/\n>  2. https://mycdn.net/mycompany/service-b/real/latest\n\n> 이 단계에서는 빌드 결과물을 어디에 그리고 얼마동안 보관해야 하는지에 대한 고민이 있었다. 나의 경우 사내 인프라로 CDN이 제공되고 있었고\n> 용량이 무제한이라 CDN에 저장하기로 했고 오래된 빌드를 따로 삭제하지는 않았다. 다만 CDN은 위처럼 빌드 결과물의 경로를 신경써야 한다.\n> 하지만 NPM을 사용한다면 압축 파일을 풀면 바로 결과물을 확인할 수 있기 때문에 이후 배포 과정의 스크립트를 더 단순하게 작성할 수 있다.\n\n\n5. 배포 및 버저닝\n\n\n웹 서버에 리소스 배포하기\n\n이 단계에서는 CDN으로부터 최신 버전의 index.html파일을 받아 웹서버에 배포한다. 최신버전은 전 단계의 url을 보면 알 수 있듯\n서비스명/페이즈/latest 규칙이다. 배포 스크립트는 각 환경에 맞게 작성하면 된다.\n\n# CDN에 업로드 한 latest파일의 내용에는 최신버전 패키지의 주소가 적혀 있다\nPACKAGE_DIR=$(curl https://mycdn.net/mycompany/$SERVICE/$PHASE/latest)\n\n# 그러므로 latest파일의 내용(최신버전 index.html이 있는 CDN의 폴더 경로)\n# 에 index.html을 붙이면 배포 대상이 된다\ncurl -o index.html $PACKAGE_DIR/index.html\n\n# 최신 index.html을 웹 서버에 배포한다\ncurl -T index.html -u userid:passswd ftp://10.10.1.55/www/publish\n\n위의 파일을 만들어 두고 SERVICE=service-a PHASE=beta sh deploy.sh 로 실행하면 배포할 수 있다.\n\n혹시 index.html만을 배포하는 이유를 궁금해할 수 있을 듯 한데. 내용을 보면 이해할 수 있을 것이다.\n\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Service A App</title>\n  </head>\n  <body>\n    <app-root></app-root>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/main.js\")}></script>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/polyfill.js\")}></script>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/vendor.js\")}></script>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/common.js\")}></script>\n  </body>\n</html>\n\nangular는 빌드할 때 —deploy-url인자를 전달하면 만들어내는 index.html에 위와 같이 script src들과 내부에서 쓰는\ncss의 url에 prefix(예제에서는 https://mycdn.net/mycompany/service-a/real/2b30274/) 를 붙여\n준다.\n\n따라서 index.html만 배포해도 된다. 만약 상대경로로 포함하는 방식이라면 관련 파일을 모두 업로드하도록 스크립트를 작성하면 된다.\n\n\nREAL 페이즈 배포 후 버저닝\n\n> ⚠️ 이 과정은 테스트 서버들이 아닌 real 페이즈 배포 직후에만 실행한다. 이유는 develop, master에서 자동 버저닝을 수행하면\n> 중복된 태그를 생성할 수 있기 때문이다.\n\n> —prerelease 인자를 사용하면 v1.3.1-alpha.0 형태의 태그를 만들 수 있다. 테스트 서버 브랜치에서는 배포시 이 인자를\n> 사용하면 리얼 브랜치와 태그가 겹치는 것을 예방할 수 있을 것으로 보이지만 필요한 부분이 아니라 추가 검토는 하지 않았다.\n\n만약 real에 배포했다면 버저닝을 할 차례이다. 이는 뒤에서 설명할 롤백을 위해 꼭 필요한 과정으로 standard-version을 이용할\n것이다.\n\n# 버저닝은 배포할 브랜치에서 수행해야만 한다\ngit checkout master\ngit pull\nnpx git-branch-is master\n\n# 자동 버저닝\nstandard-version \\\n  --release-as=minor \\\n  --releaseCommitMessageFormat=\"chore(release): {{currentTag}}\\n[[$deployUrl]]\"\n\n# package.json, CHANGELOG.md파일의 수정사항과 새 태그를 함께 푸시한다\ngit push origin HEAD --follow-tags --no-verify\n\nrelease-as 인자는 혹시 모를 핫픽스를 위함이다. 서비스의 배포는 minor이상을 올려야 patch로 긴급 배포를 할 수 있게 된다.\n어제 1.1.0 을 배포하고 오늘 1.2.0 을 배포했는데. 1.1.0 기준으로 핫픽스를 배포해야 한다면 1.1.0 에서 브랜치를 따고\n1.1.1로 배포하면 된다.\n\nreleaseCommitMessageFormat인자로 기본값 뒤에 개행과 **[[$deployUrl]]**를 붙여 주었다. 이는 롤백을 위해\n필요한 부분인데 이어지는 섹션에서 설명한다.\n\n\n롤백 프로세스\n\n롤백 프로세스 [/_astro/rollback-strategy.B6dGmbGO_Zo5GEx.webp]롤백 프로세스\n\nreal배포를 하면 태그가 쌓일 것이다 아래 명령으로 조회할 수 있다. v1.3.0로 롤백을 원한다고 가정해 보자.\n\ng tag --sort=-committerdate -l \"v*\"\n\nv1.3.1\nv1.3.0\nv1.2.5\nv1.2.4\nv1.2.3\nv1.2.2\nv1.2.1\nv1.2.0\nv1.0.0\n(END)\n\nv1.3.1의 태그 커밋 메시지에는 배포 단계에서 넣어주었던 **[[$deployUrl]]**이 기록되어 있을 것이다.\n\ngit tag -l --format='%(contents)' v1.3.1\n\nv1.3.1\n\n[[https://mycdn.com/myapp/eff782/]]\n\ndeployUrl은 index.html을 포함한 빌드 결과물들이 있는 CDN폴더 주소이다. url을 추출해 index.html을 붙여준 파일을\n다운로드 하여 배포하면 롤백이 완료된다.\n\nCONTENT=$(git tag -l --format='%(contents)' v1.3.1)\nDEPLOY_URL=$(node -p \"'$CONTENT'.match(/\\[\\[\\d\\]\\/])[1]\")\n\ncurl -o index.html $DEPLOY_URL/index.html\ncurl -T index.html -u userid:passswd ftp://10.10.1.55/www/publish\n\n\n요약 및 기타 고려사항\n\n일단 내용을 짧게 요약하자면 다음과 같다.\n\n * 리얼 배포 시점에 standard-version으로 버전 자동화를 적용한다\n * 버전 자동화 중에 추가로 CDN에 업로드 된 빌드 결과물의 주소를 태그에 기록한다\n * 롤백할 땐 원하는 태그의 메시지를 읽어 특정 포멧으로 기록된 CDN주소의 결과물을 배포하면 된다\n\n이번 개선 작업에서 하지 못한 작업들도 있다. 빠른 시일 내에 추가 적용하려고 한다.\n\n * standard-version은 changelog 훅도 제공하고 있어 배포 시점에 이메일로 변경사항을 공유하는 등의 작업이 가능할 것으로\n   보인다.\n\n글에서 소개한 배포 및 롤백 프로세스의 경우 쉘 스크립트를 작성하고 이를 package.json에 등록해 사용하는 편이 좋겠다.\n\n사내에서는 배포 및 롤백 스크립트 포함된 태스크 러너를 nodejs기반 cli로 만들어 사용하고 있다. 기회가 된다면 관련해서도 글을 쓸\n예정이다.\n\n혹시 내용관련 궁금한 사항이 있다면 이메일 [romz1212@gmail.com]을 보내 주기 바란다.","n":0.03},"1":{"v":"standard-version을 이용한 배포, 롤백 전략","n":0.447},"2":[{"v":"deploy strategy","i":1,"n":0.707},{"v":"standard-version","i":0,"n":1}]}},{"i":11,"$":{"0":{"v":"공유하는 코드를 어떻게 관리하지?\n\n현 직장에서 사내의 모든 FE 프로젝트를 하나의 팀에서 담당하게 되다 보니 어느 순간 의존성 관리 문제에 봉착하게 되었다. 라이브러리와 앱이\nN:N으로 늘어나 의존성 관리가 복잡해지고 있다.\n\n예를 들면. 쇼핑몰의 경우 보통 웹과 관리자 한 세트로 개발하여 운영한다. 만약 새로운 서비스를 오픈한다면 총 2세트가 되고. 서비스 간에\n회원, 주문을 하나로 통합하는 경우 2세트가 더 추가된다. 결국 서로의 의존성이 얽혀있는 8개 이상의 리포지토리를 운영하게 된다.\n\n서비스 사이드이펙트 문제 [/_astro/structure1.YBCaCW_Y_Z1GYS1X.webp]서비스 사이드이펙트 문제\n\n이 글에서는 이 문제를 해결하는 방법 중 Monorepo(이하 모노레포)도입을 검토하며 논의했던 내용을 정리한다. 일단은 앞서 언급했던 문제를\n고려하지 않고. 단순히 중복되는 코드를 공유하는 방법에 대해서 간단히 짚고 넘어가 보려 한다.\n\n 1. npm\n 2. git submodule, subtree\n 3. 모노레포 (Yarn Workspace + Lerna)\n\n\nNPM 사용\n\n라이브러리 빌드 결과물을 npm publish로 업로드하고 써야 하는 곳에서 npm install 로 설치해 사용하는 방법이다. npm에 올리는\n패키지들은 1.10.x, 2.1.0과 같이 버전을 명시하게 되어 있으므로 관리나 사용에 큰 도움이 된다.\n\n불편한 점이 있다. 개발할 땐 앱이건 라이브러리건 구분 없이 빈번하게 수정해야 하는데. 라이브러리를 수정하고 나면 항상 npm publish,\nnpm install을 통해 공유하는 절차를 거쳐야 하기 때문이다. 라이브러리 갯수가 두 개 이상이면 이런 절차에 시간이 많이 소요될 것이다.\n\n그나마 npm link를 이용하면 로컬에 빌드된 패키지를 전역에 설치하여 다른 패키지에서 설치해 쓸 수 있다. 한마디로 npm publish를\n하지 않아도 된다. 라이브러리 빌드 결과물 폴더에서 npm link실행 후 사용을 원하는 곳에서 npm link <패키지명>으로 연결하여 사용할\n수 있다. 마찬가지로 라이브러리 개수가 늘면 불편하고. 개발이 끝난 후 전역에 설치된 패키지를 일일이 제거해주어야 하는 번거로움이 있다.\n\n\nSUBMODULE, SUBTREE 사용\n\n리포지토리 내 특정 폴더에 다른 리포지토리를 사용하는 방식으로 부모 리포지토리에서는 폴더 자체를 특정 커밋으로 다루며 커밋은 자식과 별도로\n독립적으로 관리한다. 부모 리포지토리에서 diff를 보면 서브모듈 폴더는 단순히 서브모듈의 여러 커밋 중 한 군데를 가르키는 해시값만 보인다.\n\n따라서 서브모듈에 컨플릭이 일어났을 때 부모 리포지토리에서 바로 머지할 수 없고. 서브모듈 안에서 컨플릭을 해결하고 그 커밋을 가르키도록 부모\n리포지토리를 수정해야 하는데 이게 번거롭기도 하고 git에 익숙하지 않은 개발자라면 실수할 여지가 있다.\n\n서브트리는 부모 리포지토리에서 파일을 관리할 수 있다고 한다 사용해보지는 않았지만 Git subtree를 활용한 코드 공유\n[https://blog.rhostem.com/posts/2020-01-03-code-sharing-with-git-subtree]를 참고했을\n때 서브모듈보다 편리할 것으로 보인다.\n\n글에서 언급하는 컨플릭트는 코드리뷰를 하는 팀이라면 머지 전에 사이드 이펙이 예상되는지 담당자들이 잘 검토하는 것도 방법이고. 특정 패키지가\n수정되었을 때 영향이 있는 의존 패키지들을 알려주는 도구들도 있으니 응용한다면 효율적으로 관리할 수 있을 것으로 보인다.\n\n> typescript 프로젝트의 경우 각 모듈이 tsconfig.json에 정의된 root 경로 하위에 존재해야 참조할 수 있으므로.\n> 서브모듈, 서브트리 모두 참조가 가능하도록 폴더 구조를 잡아야 한다.\n\n\n모노레포 (YARN WORKSPACE + LERNA)\n\n하나의 리포지토리에서 사내의 모든 코드를 관리하는 방식이다. nodejs 프로젝트의 경우 일반적으로 yarn workspace를 이용하여 패키지\n의존성을 관리하고. lerna를 사용해 로컬 패키지 간 의존성 추가, 다수의 패키지에 특정 task를 수행, 사이드 이펙이 발생하는 앱들을\n추려내거나 하는 형태로 운영한다. 먼저 장단점을 정리해 보면 다음과 같다.\n\n\n코드 공유가 간편해진다\n\n코드들이 모두 하나의 리포지토리 내에 있으므로 패키지들의 추가 및 제거가 간단하다. 중복 코드는 그냥 패키지용 폴더를 만들어 옮기고 lerna\nadd로 의존성 추가 후 yarn install 하면 되고. 제거하는 경우 그냥 삭제하면 된다.\n\n로컬 패키지들 간의 의존성은 심볼릭 링크로 처리된다. 윈도우로 치면 폴더 바로 가기의 형태로 연결하기 때문에 npm install로 사용하는\n것과 똑같이 쓸 수 있지만, 공유를 위한 절차가 간단해져 편리해지는 것이다.\n\n두 개의 블로그가 공유하는 모듈을 node_modules내에 symlink로 추가해준 모습\n[/_astro/symlink.DIBjrqQF_1Bajs3.webp]두 개의 블로그가 공유하는 모듈을 node_modules내에 symlink로\n추가해준 모습\n\n또 단순히 심볼릭 링크로 연결되어 있고. 라이브러리 소스도 같은 리포지토리에 있어서 디버깅이 매우 간편하다. 앱을 띄워 두고 라이브러리 코드를\n직접 수정하면서 디버깅할 수 있다.\n\n\n버전관리가 편해진다\n\nlerna에는 각 패키지의 버전 관리를 쉽게 할 수 있는 기능도 있다. 커밋 메시지를 Conventional Commits\n[https://www.conventionalcommits.org/ko/v1.0.0/]에 맞게 작성하는 경우. 배포 시점에 추가적인 옵션을 주면\n쌓인 메시지를 읽어 각 패키지의 새로운 버전을 계산해 준다.\n\n예를 들어 feat(core): 렌더링 로직 성능 향상이라는 메시지라면 feat이므로 minor 업데이트이며. (core)로 여러 패키지 중\ncore 패키지가 업데이트 되는 것임을 인식한다. 따라서 다른 패키지의 버전은 올리지 않고. 오직 core 패키지의 버전만 올린다. 렌더링 로직\n향상이라는 메시지는 CHANGELOG.md 파일에 자동으로 정리해 준다. 그리고 변경사항이 있는 패키지를 추려 자동으로 npm에\npublish까지 해 준다.\n\n자동으로 생성된 CHANGELOG.md [/_astro/changelog.DUyFHgg2_Zcqc0c.webp]자동으로 생성된\nCHANGELOG.md\n\n모노레포 운영 시 변경사항을 관리하는 것이 더욱더 중요한데 이런 점에서 큰 도움이 될 것으로 보인다. 이 기능은 꼭 모노레포 운영을 하지\n않더라도. 하나의 리포지토리에서 여러 npm 패키지를 관리하는 경우 업무량을 크게 줄여줄 수 있을 것으로 보인다.\n\n\n리포지토리의 크기가 커짐\n\n일반적으로 많이 알려진 문제이다. angular의 소스를 clone 받을 때마다 느끼는 점으로 리포지토리 하나가 너무 커진다. 프로젝트 초기에는\n별문제가 없겠지만 오랫동안 쌓이면 git 액션들의 처리 속도가 조금씩 느려질 것이다. 하지만 angular 컨트리뷰팅 과정에서 크게 느려서\n못쓰겠다는 느낌을 받은 적은 없어서 괜찮을 것으로 생각된다.\n\n\n사이드 이펙트 문제\n\n도입 검토 중에 제일 논란이 되었던 문제다. 배포 시기가 다른 모바일 웹 서비스 A, B가 있다고 가정할 때. B의 기능 추가 커밋이 A에도\n영향을 줄 수 있다는 것이다. 만약 기능 추가 중에 라이브러리를 수정했다면 A는 사이드 이펙트가 있을 수 있는데. 담당자가 이것들을 모두 사전에\n검토할 수 있을지 모르겠다는 것이다.\n\n사이드 이펙트 문제 [/_astro/side-effect.wJ0eN_Vv_2oVHSw.webp]사이드 이펙트 문제\n\n현 회사는 모든 변경사항은 반드시 PR을 거치게 되어 있지만. 검토해야 하는 양이 많고 당장 해결해야 하는 업무가 많은 상황에서는 놓치는 경우가\n많을 것이다. 이 이슈는 모노레포의 문제 중 하나로 팀원 개개인에게 적정한 수준의 역량을 요구한다는 것이다. 결국 전면적 도입은 힘들다는 결론이\n났다.\n\n\n모노레포 적용이 어려운 이유\n\n한 회사에 서비스가 여럿이라면 보통 배포 시기가 다 다르기 마련이다. 앞서 언급한 대로 개개인의 역량이 따라준다고 하면 모르겠지만 서비스의\n안정적인 운영을 고려한다면 같은 리포지토리 내에서 사이드 이펙트가 있을지 없을지 모르는 커밋이 쌓이는 것 자체가 문제가 될 여지는 있다.\n\n또 현 회사는 테스트용으로 3개의 서버 (alpha, sandbox, beta), 실 배포용으로 1개의 서버(real)를 운영하는 4단계\nphase 운영을 하므로 모든 리포지토리가 최소 두 개 이상의 배포 브랜치를 가지고 있다.\n\n그러나 lerna publish 명령 자체가 만들어내는 버전 태그들은 유일해야 한다. 이런 단일 배포 브랜치 기반 프로세스를 보면 애초에 팀의\n배포 프로세스부터가 맞지 않는 것으로 보인다.\n\n\n결국은 전면적 도입은 하지 않기로\n\n결국 단일 배포 브랜치를 갖는 라이브러리만 적용하고 서비스는 기존대로 운영하게 되었다. lerna를 이용해 멀티 패키지 라이브러리 리포지토리\n관리를 하게 된 것만으로도 업무량이 크게 줄어서 도입을 검토했던 시간이 아깝지는 않았다. 환경이 맞지 않아 도입할 수 없었지만 적용에 따른\n이슈도 크리티컬한 부분이라 아쉽다는 생각이 들지는 않았다.\n\n이야기 중에 라이브러리에만 모노레포를 적용하자는 말을 많이 들었다. 그 자리에서 언급한 내용이기도 하지만 그렇게 되는 순간 이미 모노레포가\n아니게 된다. 본문에서 언급했던 모노레포의 장점이 희석되기 때문이다.","n":0.033},"1":{"v":"앱과 라이브러리 관리에 Monorepo?","n":0.5},"2":[{"v":"yarn","i":4,"n":1},{"v":"lerna","i":3,"n":1},{"v":"subtree","i":2,"n":1},{"v":"submodule","i":1,"n":1},{"v":"monorepo","i":0,"n":1}]}},{"i":12,"$":{"0":{"v":"AMPLIFY의 불편한 점\n\nAmplify는 웹 서비스를 개발하고 배포하기 위해 필요한 AWS의 기능들을 짜깁기한 서비스다. github을 연동하고 설정만 조금 만지면 웹\n앱 하나가 뚝딱 만들어진다. S3, CloudFront를 설정하고 연동해야 하는 부담이 없어 사용했다. 하지만 기능이 간소화 된 만큼 디테일한\n부분을 다루는 게 불가능했다. 나의 경우 배포 후 캐시를 삭제할 수 없어서 기다리면 되겠거니 했는데 거의 하루 반나절 동안 페이지를 볼 수\n없었다.\n\n또 리다이렉트 규칙을 세세히 적용할 수 없었다. gatsby의 정적 리소스들을 서빙하는 데 문제가 발생했다. 예를 들어 *.js, *.css로\n끝나는 요청의 경우 해당 파일을 서빙해야 하고. 그 외에는 index.html을 서빙해야 하는데. 해당 기능을 설정하면 이게 기능이 각 서버에\n배포 중인지 아닌지에 대한 정보도 표시되지 않고. 스크립트 오류는 계속 발생했다.\n\nAmplify Redirect 설정 [/_astro/aws-amplify-redirect.CitaV0rq_1E3eF6.webp]Amplify\nRedirect 설정\n\n위와 같이 정규식으로 Redirect를 설정하는데 배포 상태를 알 수 없으니 식을 맞게 작성한것인지 파악하기도 어려웠다. *.js 요청에\nhtml본문이 응답되는 현상이 해결되지 않아서 Amplify를 쓰지 않기로 결정했다.\n\n\nS3, CLOUDFRONT로 GATSBY 서빙하기\n\n그리하여 Gatsby를 사용하기 위한 AWS의 서비스들을 직접 설정하고 연동했다. 번거롭지만 확실하게 해당 문제들을 해결할 수 있게 되어\n만족스러웠다. 아래부터는 mnkim.com [https://mnkim.com]을 S3, CloudFront에 설정했던 내용에 대해 자세히\n다룬다.\n\n\nS3 버킷 설정하기\n\nS3는 AWS서비스 중 스토리지 관리와 모니터링을 제공하는 기능이다. 사실 나도 무슨 말인지 모르겠다. 다만 우리는 gatsby build\n명령을 통해 만들어진 public/* 파일들을 업로드하고 각 파일에 서비스 별 접근 권한을 설정하는 기능을 사용하게 될 것이다.\n\n먼저 버킷을 만든다. 버킷의 이름은 나중에 다른사람들이 블로그에 방문할 때 쓰는 주소로 한다. 예를 들어 www를 붙이지 않은\nmnkim.com을 원하는 경우 이름은 mnkim.com이 되어야 한다. 반대의 경우 www.mnkim.com로 이름을 짓는다.\n\n[/_astro/s3-1.BJ5cLgPQ_wwvex.webp]\n\n생성할 때는 위 화면처럼 퍼블릭 엑세스를 가능하게끔만 설정하면 된다. 생성이 완료되었다면 개요 탭에서 gatsby build를 통해 생성된\npublic/*의 하위 파일들을 모두 선택해 업로드한다. 다음 속성 탭에서 정적 웹 사이트 호스팅을 선택하고 아래와 같이 설정한다.\n\n[/_astro/perm.EApx57PJ_ZnA2AX.webp]\n\n다음으로는 권한 탭의 버킷 정책에서 ‘정책 편집기’ 타이틀 옆 ARN값을 복사해 두고 에디터 아래 정책 생성기를 클릭힌다. 그럼 다음곽 같은\n화면이 나오는데 위의 스샷처럼 입력한다. Actions는 GetObject를 선택하면 된다.\n\n이어서 Add Statement를 클릭하면 아래 정책목록이 추가되고. Generate Policy를 클릭하면 팝업에 json값이 출력되는데\n이것이 S3 버킷에 대한 권한 설정 내용이다. 복사하여 부모 창의 편집기에 넣고 저장을 누른다. 이 때 반드시 Resource뒤에 ‘/*’를\n붙여 버킷 내 파일 모두에 접근할 수 있도록 해 주어야 한다.\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"Policy1239293829383\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt15910279239283\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::<내 도메인>/*\"\n    }\n  ]\n}\n\n[/_astro/s3-2.DdCQGRky_1gqL5X.webp]\n\n이 버킷을 사용하여 웹 사이트를 호스팅합니다 체크, 인덱스 문서는 index.html, 오류 문서는 404.html로 적고 저장을 누른다. 그럼\n이제부터 상단에 있는 엔드포인트 주소로 사이트를 서빙할 수 있게 된다. 다만 이 주소는 기억하기 어려운 형태이고 http이기도 해서 이후\n과정에서 별도의 도메인을 연결한다.\n\n\nCERTIFICATE MANAGER로 인증서 만들기\n\n먼저 https로 서빙하기 위한 인증서를 만들어야 한다. Certificate Manager에서 인증서 요청 을 누르고 이어 공인 인증서 요청\n선택. 도메인 이름에 johnny.com. 아래 다른 이름 추가 를 누르고 *.johnny.com (소유한 도메인으로 입력한다)\n\n[/_astro/cm-1.WoaOUM3e_Z1T0P4X.webp]\n\n이후 과정에서 해당 도메인의 진짜 소유자인지를 검사하는 과정을 거친다. DNS인증을 선택하고 나오는 각 도메인 별 이름과 값을 도메인을 구입한\n사이트에서 제공하는 DNS구성 기능으로 CNAME으로 등록해야 한다. 드롭다운으로 A인지 CNAME인지 TXT인지 선택하고 이름과 값을 입력하는\nUI구성으로 아마 조금 살펴보면 알 수 있을 것이다.\n\n도메인을 AWS Route 53에서 구입했거나. 구입처가 다르지만 Route 53에서 관리하도록 설정한 경우에는 아래 버튼을 누르면 자동으로 그\n값들을 등록해준다.\n\n[/_astro/cm-2.C6leAfa9_1iqwp4.webp]\n\n\nCLOUDFRONT 설정하기\n\nCloudFront는 S3버킷에 담긴 파일을 전세계 서버에 캐싱해서 빠르게 서빙할 수 있도록 해 주는거 같다. 안해도 상관은 없는데 하면 사이트\n성능이 크게 개선되므로 해보는것도 나쁘지 않다.\n\nCreate Distribution클릭. Web 섹션의 Get Started클릭. 아래 필드에서 따로 언급하지 않은 것들은 기본값으로 내버려\n둔다.\n\n[/_astro/cf-1.DY_ffoiy_WOgG0.webp]\n\n아래 Distribution Settings에서 Alternative Domain Names에 사용할 도메인을 입력하고. 아래 Custom\nSSL Certificate 를 선택한다. 그 후 아래 텍스트박스에 포커스하면 만들어둔 인증서가 보일텐데 그것을 선택한다.\n\n[/_astro/cf-2.blwCps28_28MnWG.webp]\n\n마지막으로 Default Root Object를 반드시 index.html로 입력한 후 다음으로 넘어간다. 이렇게 되면 세팅이 끝나고 전세계의\n모든 서버에 해당 설정이 반영되는 시간 후 Distribution Status가 Deployed로 바뀌며 화면의 Domain Name 으로\n블로그에 접속할 수 있게 된다. 하지만 우리는 이 주소가 아닌 내 소유의 도메인 johnny.com을 연결해야 한다 이후에서 진행한다.\n\n[/_astro/cf-3.BG0kj8AB_ZjnOvn.webp]\n\n\nROUTE 53으로 커스텀 도메인 연결하기\n\n일단 나처럼 Godaddy의 도메인을 Route 53에서 관리하도록 설정하든, Route 53에서 직접 구입하든 도메인이 하나 있어야 한다.\n(위에서 인증서를 커스텀으로 설정했다면 있을 것이다) 호스팅 영역을 생성하고 레코드 세트를 하나 생성한다. 유형은 A. 값은 방금 설정했던\nCloudFront의 Domain Name을 입력하면 되는데.\n\n직접 입력해도 되지만 별칭 을 예로 선택하고 텍스트박스에 포커스 하면 목록이 출력되는데. 그 중 CloudFront배포 에 이전에 설정한 값이\n있으므로 선택해서 채워도 된다. Route 53 미사용자는 앞서 도메인을 인증할 때 썼던 페이지에서 주소를 직접 입력하면 된다.\n\n[/_astro/r-1.rwoWdY33_2pv8f9.webp]\n\n저장 후 10분 정도 기다리면 해당 도메인으로 블로그에 접속할 수 있다!\n\n\nWWW에서 NON-WWW로 보내기 (선택)\n\n안해도 되는데 나처럼 non-www로만 서빙을 원하는 경우 설정하면 된다. 아까 johnny.com으로 S3버킷을 만들었던 것 처럼 똑같이\nwww.johnny.com이름으로 버킷을 만든다. 이 때 퍼블릭 엑세스는 풀고. 파일은 업로드하지 않아도 된다.\n\n속성에서 정적 웹사이트 호스팅을 체크하고 이번엔 요청 리디렉션을 선택한다. 대상 버킷 또는 도메인에 johnny.com을 입력하고 프로토콜은\nhttps를 선택한다. 반대의 경우는 반대로 입력하면 된다. 물론 위의 세팅들이 모두 반대 (www)로 설정되었어야 한다.\n\n[/_astro/non-www.D56TUqGp_1nX8bL.webp]\n\n그 다음 Route 53에서 같은 호스팅 영역에 새 레코드 세트를 만든다. 이름은 www.johnny.com. 유형은 A. 값은 별칭으로\n동일하게 선택하되 **www.johnny.com**버킷을 선택한다.\n\n\n배포 설정\n\n이제 맨 처음 S3에 빌드 결과물을 직접 업로드 했던 것을 명령으로 할 수 있게 할 차례다. 그런데 별도의 인증 없이 내 버킷에 업로드가\n가능하다면 다른사람도 업로드할 수 있다는 말이 되니까 인증을 해야 한다. 먼저 AWS서비스 중 IAM에서 인증 토큰을 발급한다.\n\n대시보드에서 사용자를 누르고 상단 사용자 추가를 누른다. 사용자 이름을 입력하고 AWS액세스 유형은 프로그래밍 방식 액세스. 그룹 생성을 눌러\n그룹 이름은 원하는 대로 짓고 AdministratorAccess 권한을 부여한다.\n\n모든 과정을 완료하면 마지막에 액세스 키 ID와 시크릿 액세스 키가 보이고 따로 인증 csv를 다운로드 받을 수 있는 화면이 보인다. 이\n페이지를 벗어나면 더 이상 키를 확인할 수 없으므로 csv파일은 잘 보관해둔다.\n\n먼저 프로젝트에 dotenv패키지를 설치한다.\n\nnpm install --save-dev dotenv\nyarn add -D dotenv\n\n다음 .env파일을 생성하고 다음 내용을 입력한다. 해당 파일은 git에 커밋되지 않도록 해야 한다. 만약 커밋을 한다면 다른사람들이 이 내용을\n볼 수 없도록 repository자체를 private으로 만들어야 한다.\n\nAWS_ACCESS_KEY_ID=xxxx\nAWS_SECRET_ACCESS_KEY=xxxx\n\n이어서 gatsby-plugin-s3패키지를 설치하고 gatsby-config.js의 플러그인 항목에 아래 내용을 추가한다.\n\nrequire('dotenv').config() // .env 파일 읽어서 환경변수에 추가한다\n\nmodule.exports = {\n  plugins: [\n    {\n      resolve: 'gatsby-plugin-s3',\n      options: {\n        bucketName: 'johnny.com', // 업로드 대상 S3 버킷 이름\n        protocol: 'https', // 프로토콜\n        hostname: 'johnny.com', // 호스트명\n      },\n    },\n  ],\n}\n\n그 후 gatsby build && npx -n \"-r dotenv/config\" gatsby-plugin-s3 deploy 명령을 실행하면\n빌드 후 내용이 S3 버킷에 업로드 된다. 아래처럼 package.json에 설정해두고 쓰면 편하다.\n\n\"scripts\": {\n    \"deploy\": \"npx -n \\\"-r dotenv/config\\\" gatsby-plugin-s3 deploy\"\n}\n\n\n작업 후기\n\n * johnny.com으로는 접속이 잘 되는데 johnny.com/blog와 같이 서브도메인에서 새로고침했을 때 AccessDenied에러가\n   나는 경우는 S3버킷 정책이 올바르게 설정되지 않았기 때문일 수 있다. 최종적으로 설정했을 때 정책이 바뀌어 있다면 맨 위 S3버킷 설정과\n   동일하게 다시 설정한다.\n * 이제 캐시를 직접 제거할 수 있다. CloudFront에서 해당 Distribution을 선택하고 아래 처럼 Invalidation을\n   만들어 주면 된다.\n\n[/_astro/cache.Dbk-dXt3_1vz8ek.webp]\n * CloudFront의 내용 수정이 모든 Edge에 반영되기까지 시간이 꽤 걸린다. 그리고 삭제를 원하는 경우 Distribution이\n   비활성화가 되어야 한다.","n":0.032},"1":{"v":"Amplify에서 S3 CloudFront로 전환하기","n":0.5},"2":[{"v":"cloudfront","i":3,"n":1},{"v":"s3","i":2,"n":1},{"v":"amplify","i":1,"n":1},{"v":"aws","i":0,"n":1}]}},{"i":13,"$":{"0":{"v":"전환 배경\n\n두달 전 우연히 Gatsby라는 Static Site Generator(이하 SSG)를 알게 되었고. 프로토타이핑 후 기존 블로그를\nGatsby로 전환하는 작업을 하기 시작했다. Gatsby의 전반적인 시스템을 이해하는데 총 2주 정도 걸렸고. 나머지는 두 개의 블로그\n프로젝트 마이그레이션, 최적의 호스팅 서비스 찾기와 연동, 모노레포 구성, 빌드 및 배포설정에 6주 정도 걸렸다.\n\n\nFROM JEKYLL\n\n익숙한 그 디자인. 심지어 featured image도 없다면 완전 클론의 습격이다.\n[/_astro/minimal-mistakes.CxHOiQRj_gAAuF.webp]익숙한 그 디자인. 심지어 featured image도 없다면\n완전 클론의 습격이다.\n\n지금 보고 있는 기술 블로그 johnny-mh.github.io [https://johnny-mh.github.io]는 원래 jekyll로\n운영하고 있었다. jekyll은 ruby기반의 SSG이며 이제는 역사가 깊은 도구인 듯 싶다. github.io 에 호스팅을 전제로 개발하는 듯\n접근하기 쉽고 제공 기능들도 부족함 없고 디자인 템플릿도 많다.\n\n하지만 많이 쓰다 보니 비슷한 디자인의 사이트가 많아 재미가 없다는 점과. 그래서 바꿔보려니 익숙하지 않아 커스터마이징이 어렵다는 것이 계속\n걸렸다. 그러다 보니 글을 올리고 싶다는 생각도 잘 들지 않았다.\n\n\nFROM SQUARESPACE\n\n사진 블로그 mnkin.com [https://mnkim.com]은 squarespace.com\n[https://www.squarespace.com]를 이용하여 운영하고 있었다. 웹 기반으로 블로그 및 쇼핑몰 사이트를 만들고 관리할 수 있는\n서비스이다. 처음 발견했을 때 서비스의 웹 기반 에디터에 반해버려서 1년에 이용료와 호스팅 비용 18만원을 지불하며 3년을 사용했다.\n\n사진만 조금 넣고 글 조금 쓰면 이런 룩앤필이 뚝딱\n[/_astro/squarespace-fillmore.QZkblWAH_ZI0lOv.webp]사진만 조금 넣고 글 조금 쓰면 이런 룩앤필이 뚝딱\n\n개발자라 근본이 사대주의라 그런지 모르겠으나 디자인도 대부분 너무 이쁘고. 반응형 기본제공, SEO자동 최적화, 아이폰 안드로이드용 관리 툴,\n통합 google analytics등 서비스 운영에 필요한 모든 것이 기본으로 제공된다.\n\n지금봐도 서비스 내 위지윅 에디터는 국내 IT대기업들보다 훨씬 직관적이고 사용하기 좋게 만들었다고 생각한다. 만약 외국에서 사이트를 서비스한다면\n그냥 여기를 사용하면 될 정도다.\n\n하지만 1년에 많아봐야 10개 이내의 글을 쓰게 되면서 비용이 부담스럽다는 생각이 들었고 서비스를 외국에서만 하다 보니 국내에서는 심각하게 느린\n것도 불편했다. 이미지를 주로 서빙하는 블로그인 특성 상 이 부분이 치명적이었다.\n\n\nGATSBYJS의 특징 및 장점\n\n\nREACT, GRAPHQL\n\n현재는 업무 상 angular만을 사용하고 있지만 커리어 문제로 React와 GraphQL을 항상 공부하고 싶었던 차에 Gatsby는 신기하게도\n시기적절하게 발견한 SSG 프레임웍이었다. 내부적으로 리소스 파일들을 GraphQL로 쿼리하고. NodeJS API를 통해 가공하여 React로\nhtml페이지를 만들어내는 형태라 너무 가볍지도 무겁지도 않게 기술을 접해볼 수 있어서 즐거웠다.\n\nReact Hook은 말로만 좋다고 들어왔는데 실제로 써 보니 신세계라는 말이 아깝지 않았다. 이전 회사에서 React기반으로 개발할 때\n가려웠던 부분을 기초에 충실하면서도 강력하게 개선했다고 생각한다. 그냥 함수였을 뿐인데. 그것만 가지고 이런 시스템을 만들어내었다는것이 놀랍다.\n\n\n강력한 플러그인\n\nGatsby 플러그인 중 gatsby-plugin-sharp을 이용하면. 직접 구현하기는 조금 까다로운 이미지 서빙 최적화를 간편하게 적용 할\n수 있다. medium.com의 글을 보다보면 로딩 중에는 이미지가 뿌옇다가 완료 후 부드럽게 선명해지는 그 효과 말이다.\n\n사진 전문 블로그인 mnkim.com [https://mnkim.com]는 컨텐츠의 대부분이 이미지라 큰 도움이 되었다. 아래처럼 플러그인 몇\n개만 설정하면 마크다운의 이미지들에 자동 점진적 로딩 최적화가 적용된다. prerender 시점에 저 품질의 base64이미지를 생성하여\nhtml을 먼저 서빙하고. 로딩 후에 페이드 인으로 교체해 준다.\n\n// gatsby-config.js\n// 사이트의 html페이지들을 생성할 때 적용되는 설정 및 플러그인을 추가하는 인터페이스\nmodule.exports = {\n  plugins: [\n    'gatsby-plugin-sharp',\n    {\n      resolve: 'gatsby-transformer-remark', // .md파일을 html 컨텐츠로 변환하는 플러그인\n      options: {\n        plugins: [\n          {\n            resolve: 'gatsby-remark-images', // .md파일을 변환할 때 이미지들에 최적화를 적용한다\n            options: { maxWidth: 1300, showCaptions: ['alt'] },\n          },\n        ],\n      },\n    },\n  ],\n}\n\nGatsby를 한 마디로 표현하자면 FE개발자에게 치명적인 만능 장난감인 듯 하다. 마이그레이션을 하면서 React, GraphQL은 물론이고\n성능 최적화, SEO, lerna, syntax-tree [https://github.com/syntax-tree] 등 많은 도구에 대한 공부를\n하게 되어서 너무 즐거웠다. 블로그를 만들 계획이 있다면 적극 추천한다.","n":0.046},"1":{"v":"블로그들 Gatsby로 전환 후기","n":0.5},"2":[{"v":"react","i":1,"n":1},{"v":"gatsby","i":0,"n":1}]}},{"i":14,"$":{"0":{"v":"CentOS 7.7 기준으로 작성하였으며, Master, Slave 노드 공통으로 해야하는 일과 각각 해야하는 일들로 나누어 정리했다\n\n\nMASTER, SLAVE 공통\n\nJDK설치, git 2.x 설치\n\n> lerna처럼 근래에 나온 도구들은 git 2.x이상을 요구하는 경우가 있으므로 업데이트 한다\n\nsudo yum update\nsudo rpm -Uvh http://opensource.wandisco.com/centos/7/git/x86_64/wandisco-git-release-7-2.noarch.rpm\nsudo yum install -y java-1.8.0-openjdk-devel git\n\n사내망일 경우 프록시를 설정해야 할 수 있다\n\nvim .bash_profile\n\n# 맨밑에 아래 내용 추가\nexport http_proxy={{프록시 서버 주소}}\nexport HTTP_PROXY=$http_prox\nexport https_proxy=$http_proxy\nexport HTTPS_PROXY=$http_proxy\nexport no_proxy=\"localhost,127.0.0.1\"\nexport NO_PROXY=$no_proxy\n\n\nMASTER 설정\n\n\nJENKINS 설치 및 포트 설정\n\nhttps://linuxize.com/post/how-to-install-jenkins-on-centos-7\n[https://linuxize.com/post/how-to-install-jenkins-on-centos-7]를 참고하여 설치한다\n\n만약 서버에 80포트가 관리자 권한으로 막혀 있다면 아래 명령으로 우회 사용할 수 있도록 한다\n\nsudo -i\niptables -A PREROUTING -t nat -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8080\niptables-save\n\njenkins계정의 패스워드 삭제 및 로그인 가능하게 하기\n\nsudo -i\npasswd -d jenkins\nsudo vim /etc/passwd\n\n# jenkins 라인 마지막 /bin/false를 /bin/bash로 변경\n\n\nGITHUB 액세스용 인증 키 생성\n\nsu - jenkins\nmkdir .ssh\ncd .ssh\nssh-keygen\n\n# id_rsa, id_rsa.pub생성됨\n\ngithub.com 로그인 후 Settings > Developer settings > Personal access tokens 에서 새 토큰을\n등록하고 해당 값을 복사한다\n\nJenkins 설정 중 Crediential 에서 Secret text 로 해당 값을 등록해둔다. 등록할 때 description을 잘 적어서\n다른 키들과 혼동하지 않게 한다 나는 **‘Personal Access Token’**으로 적었다\n\n\n플러그인 설치 및 설정\n\n아래 플러그인들 설치\n\nNodeJS, AnsiColor, GitHub Pull Request Builder\n\nJenkins 관리 > Global Tool Configuration > NodeJS 항목 추가한다. 버전은 어떤것이든 상관없으나 가능하면\nLTS (Long Term Support)버전을 선택한다\n\nName: NodeJS 10.15.3\nVersion: NodeJS 10.15.3\nGlobal npm packages to install: typescript@3.5.3 ts-node@8.8.2 @angular/cli@8.2.0 @sentry/cli@1.52.1 yarn lerna\n\n위에는 Angular프로젝트 빌드 및 배포용으로 적었다. 각 전역 패키지들의 버전을 명시적으로 적어두면 좋다 빌드 시점에 매번 설치하기 때문\n\nprivate npm을 사용중이라면 주소를 Jenkins 관리 > Managed Files > Npm config file 을 추가하여 설정한다\n\nregistry=<private npm url>\n\n\n환경변수 설정\n\nJenkins 관리 > 시스템 설정 > Global properties > Environment variables 에 아래 값들을 추가한다\n\n위의 두 값은 빌드 콘솔이 출력될 때 유니코드 문자열들을 제대로 보기 위함이고. 아래 프록시 설정은 사내망 등 해당할때만 추가한다\n\n이름: JAVA_TOOLS_OPTIONS\n값: -Dfile.encoding=UTF-8\n\n이름: LANG\n값: ko_KR.UTF-8\n\n이름: HTTP_PROXY\n값: 프록시 주소\n\n이름: HTTPS_PROXY\n값: 프록시 주소\n\n이름: NO_PROXY\n값: 프록시 타면 안되는 도메인들\n\n\nSLAVE 설정\n\njenkins 계성 생성 및 패스워드 삭제\n\nsudo i\nuseradd -d /var/lib/jenkins jenkins\npasswd -d jenkins\nexit\n\njenkins 계정에 ssh 키 추가후 Master 노드가 접근할 수 있도록 등록\n\nsu - jenkins\nmkdir .ssh\ncd .ssh\nvim authorized_keys\n\n# Master 노드의 인증서 설정에서 만든 id_rsa.pub의 내용을 복사하여 붙여넣고 저장한다.\n# 만약 기존에 파일이 있다면 맨 아랫줄에 추가하면 된다","n":0.055},"1":{"v":"FrontEnd 개발을 위한 Jenkins CI서버 세팅하기","n":0.408},"2":[{"v":"ci","i":2,"n":1},{"v":"frontend","i":1,"n":1},{"v":"jenkins","i":0,"n":1}]}},{"i":15,"$":{"0":{"v":"CAPSLOCK과 CONTROL키 바꾸기\n\n처음에는 게임과 영화를 보기 위한 용도로 데스크탑을 조립했는데 이제는 개발도구로 사용하고 있다. Window는 터미널을 사용할 때의 제약이 많아\n개발할때는 꺼렸는데 wsl의 등장으로 지금은 현업에서도 큰 불편 없이 사용할 수 있는 정도가 되었다.\n\n특히 vscode의 wsl 플러그인을 사용하면 wsl의 리눅스 파일시스템에 있는 프로젝트들을 마치 호스트의 파일시스템에 있는 것 처럼 사용할 수\n있어서 집에서는 이제 맥북으로 개발하지 않을 것 같다. 다만 몇 가지 불편한 부분이 있었다.\n\n첫번째는 Capslock키와 Control키의 위치였다. 해피해킹 키보드에 익숙해져 Control대신 Capslock을 마구 눌러댔다.\n윈도우에서 이를 해결할 수 있는 방법은 두가지가 있다. 첫번째로 레지스트리를 수정하는 방법인데. 대부분의 프로그램에서는 잘 동작하지만 특정\n게임들 (몬스터 헌터, 토탈워 삼국)에서는 Control키를 아예 누를 수 없는 상태가 되어 버린다.\n\n두 번째는 AutoHotKey [https://www.autohotkey.com/]를 사용하는 것이다. 이 방법으로 지금까지 만족스럽게 사용하고\n있다. 사이트에서 프로그램을 설치하고 바탕화면에 우클릭 후 ‘새로 만들기’ > **‘AutoHotkey Script’**를 선택한 후 생성된\n파일에 아래 내용을 붙여 넣고 저장한다.\n\n; CapsLock, Control 전환\nCapsLock::Ctrl\nCtrl::CapsLock\n\n다음 해당 파일을 우클릭하여 **‘Run Script’**로 실행한다. 시스템 트레이 아이콘에 ‘H’아이콘이 나타나면 된 것이다. 그럼 이제\nCapslock과 Control이 바뀌었을 것이다. 이 동작은 언제까지나 스크립트가 실행되어 있는 상태만 유효하다.\n\n\n수정모드를 빠져나갈 때 영문으로 전환하기\n\n이전에 Spacemacs를 사용할 땐 에디터 내장 언어 입력기가 존재하여 수정모드를 빠져나갈 때 자동으로 영문으로 바꿔 주었는데. 이 기능이\n정말 편리했다. VSCODE를 사용한 뒤로는 그 기능을 쓸 수 없어 Esc로 수정모드를 빠져나온 후 항상 언어 전환 키를 눌러줘야만 했다.\n\n이 문제를 해결하기 위한 설정 [https://github.com/daipeihust/im-select]이 있긴 하지만 이게 IME입력기를 쓰는\n환경에서는 잘 동작하지 않는다. 이 문제도 AutoHotKey를 이용해 해결할 수 있었다. 위에서 했던 방법과 마찬가지로 아래 스크립트를 쓰면\n된다.\n\n; vscode에서 vim insert 모드 종료시 한글이면 영문으로 전환\n#IfWinActive, ahk_exe Code.exe\nEscape::\n if (ImeCheck(\"A\") = 1)\n  Send {vk15sc138}\n Send {Escape}\nReturn\n#IfWinActive\n\n; 키보드 언어 상태 확인 1이면 한글 0이면 영문\nImeCheck(WinTitle) {\n WinGet,hWnd,ID,%WinTitle%\n Return SendImeControl(ImmGetDefaultIMEWnd(hWnd),0x005,\"\")\n}\nSendImeControl(DefaultIMEWnd, wParam, lParam) {\n DetectSave := A_DetectHiddenWindows\n DetectHiddenWindows,ON\n SendMessage 0x283, wParam,lParam,,ahk_id %DefaultIMEWnd%\n if (DetectSave <> A_DetectHiddenWindows)\n  DetectHiddenWindows,%DetectSave%\n return ErrorLevel\n}\nImmGetDefaultIMEWnd(hWnd) {\n return DllCall(\"imm32\\ImmGetDefaultIMEWnd\", Uint,hWnd, Uint)\n}\n\n그럼 이제 매 부팅시마다 위의 스크립트들이 자동실행만 되면 된다. 방법은 스크립트 파일 우클릭 후 **‘Compile Script’**를\n선택한다. 그럼 같은 경로에 exe파일이 생겼을 것이다. 이제 Window의 시작 버튼에 우클릭 후 ‘실행’ 을 열고 거기에\nshell:startup을 입력하고 ‘열기’ 를 누른다.\n\n그럼 폴더가 하나 뜨는데 여기에 exe파일들을 넣으면 된다. 참고로 위의 두 스크립트를 하나의 파일에 넣어도 된다.","n":0.055},"1":{"v":"Windows에서 VS Code vim플러그인 자동 한영전환","n":0.408},"2":[{"v":"autohotkey","i":1,"n":1},{"v":"vscode","i":0,"n":1}]}},{"i":16,"$":{"0":{"v":"🚀 ROUTEREUSESTRATEGY\n\nAngular는 라우팅 시점마다 RoutingModule에 제공된 Routes 중 이전 페이지와 다음 페이지에 해당하는 Route 객체를 찾아\n서로 비교하여 변경이 있을 때만 컴포넌트를 교체한다.\n\n동일한 Route 간 이동 시. 같은 컴포넌트가 렌더링 되며 이뤄지는 API 호출은 중복으로 판단하는 것으로 보인다. 가이드 문서에는 없지만,\n이 동작은 개발자가 커스터마이징 할 수 있다.\n\n예를 들면 상세에서 목록으로 뒤로 가기로 이동했을 때는 원래 Route 설정이 달라 컴포넌트를 새로 만들어야 하지만 목록에서 상세로 진입 시점에\n목록 컴포넌트 상태를 캐시 했다가 뒤로 가기 시점에 복원하여 API 호출을 줄일 수 있다.\n\n이 RouterReuseStrategy API 상세 설명\n[https://itnext.io/cache-components-with-angular-routereusestrategy-3e4c8b174d5f]을\n간략히 설명하면 아래와 같다.\n\nexport abstract class RouteReuseStrategy {\n  /**\n   * 현재 이동에 컴포넌트 재사용 '여부'를 확인한다\n   * false반환 시 재사용없이 컴포넌트를 새로 만들고\n   * true를 반환하면 아래 4개의 메서드를 상황별로 호출하여 캐시 및 복원한다\n   * (캐시, 복원 로직은 직접 구현해야 한다)\n   */\n  abstract shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ): boolean\n\n  /**\n   * 페이지를 빠져나갈 때 현재 컴포넌트 캐시 '여부'를 반환한다\n   * false 반환 시 캐시 안해도 되는것으로 판단\n   * true 반환 시 아래 store메서드를 호출한다\n   */\n  abstract shouldDetach(route: ActivatedRouteSnapshot): boolean\n\n  /**\n   * 페이지 빠져나가기 전 상태를 캐시한다\n   * 캐시를 위해서 두 번째 인자인 DetachedRouteHandle을 어딘가에 저장하면 된다\n   */\n  abstract store(\n    route: ActivatedRouteSnapshot,\n    handle: DetachedRouteHandle | null\n  ): void\n\n  /**\n   * 페이지 진입 시점에 복원 '여부'를 반환한다\n   * false 반환 시 복원 안해도 되는것으로 판단\n   * true 반환 시 아래 retrieve메서드를 호출한다\n   */\n  abstract shouldAttach(route: ActivatedRouteSnapshot): boolean\n\n  /**\n   * 페이지 진입 시 캐시된 데이터를 복원한다\n   * 위에서 구현한 store 메서드 호출 시점에 어딘가에 저장했던 캐시를 반환하면 된다\n   */\n  abstract retrieve(route: ActivatedRouteSnapshot): DetachedRouteHandle | null\n}\n\n위의 인터페이스를 상속받아 최상위 NgModule에 Providing 해 주면 된다.\n\nAngular의 DefaultRouteReuseStrategy\n[https://github.com/angular/angular/blob/5bc39f8c8d5238a9be9bd968cf18ea4b738bd6be/packages/router/src/route_reuse_strategy.ts#L65]는\nRouteConfig [https://angular.io/api/router/Route]가 같을 때 shouldReuseRoute의 실행 결과를\ntrue로 반환하고 있지만. 각 메서드에서 캐시 여부(shouldDetach), 복원 여부(shouldAttach) 모두 false를 반환하고\n있어 아무 일도 일어나지 않는다.\n\n\nROUTERLINK가 동작하지 않아요\n\nAngular는 앞서 설명한 대로 라우팅 전, 후의 Route 객체를 비교하여 다를 때만 컴포넌트를 교체한다. 따라서 같은 Route 객체 간\n이동이라면 컴포넌트가 교체되지 않는다.\n\nconst routes = [\n  {\n    path: 'detail/:id',\n    component: DetailComponent,\n  },\n]\n\n위의 라우팅 설정에서 아래 컴포넌트의 링크 클릭해서 '/detail/3'에서 '/detail/12'로 이동했다면 같은 Route객체(정확히는\n같은 ActivatedRouteSnapshot)를 비교한다.\n\n따라서 컴포넌트가 교체되지 않아 ngOnInit 을 비롯한 라이프사이클 메서드들이 실행되지 않는다. 만약 ngOnInit에서 id를 받아\nAPI를 호출해 상세 데이터를 보여주도록 개발했다면 문제가 될 수 있다.\n\n@Component({\n  selector: 'app-detail',\n  template: `\n    <h1>detail component</h1>\n    <a routerLink=\"/detail/12\">go to '/detail/12'</a>\n    <div>{{ content }}</div>\n  `,\n})\nexport class DetailComponent {\n  content = ''\n\n  // '/detail/12'로 이동했을 때는 호출되지 않아 3번 데이터를 계속 보여준다\n  ngOnInit() {\n    this.http\n      .get(`/detail/${this.activatedRoute.snapshot.params.id}`)\n      .subscribe((o) => (this.content = o))\n  }\n}\n\n아래 데모를 통해 문제를 확인해 볼 수 있다.\n\n\n\n이런 경우 ngOnInit이 재실행되지 않아도 갱신되도록 스트림을 이용하여 수정하는 것이 일반적이지만. RouteReuseStrategy를\n이용하여 Route 객체 비교를 커스터마이징 할 수 있고. 강제로 컴포넌트를 교체하도록 할 수 있다.\n\nexport class CustomRouteReuseStrategy extends RouteReuseStrategy {\n  shouldDetach(route: ActivatedRouteSnapshot) {\n    return false\n  }\n\n  store(route: ActivatedRouteSnapshot, detachedTree: DetachedRouteHandle) {}\n\n  shouldAttach(route: ActivatedRouteSnapshot) {\n    return false\n  }\n\n  retrieve(route: ActivatedRouteSnapshot) {\n    return null\n  }\n\n  shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ) {\n    const [futureUrl, currUrl] = [future, curr].map((o) =>\n      o.url.map((seg) => seg.path).join('/')\n    )\n\n    /**\n     * Route비교 시 둘 다 'detail'을 포함한 path라면 컴포넌트를\n     * 재사용하지 않도록 false를 반환한다.\n     */\n    if (futureUrl.includes('detail') && currUrl.includes('detail')) {\n      return false\n    }\n\n    return future.routeConfig === curr.routeConfig\n  }\n}\n\n아래는 위 CustomRouteReuseStrategy를 이용한 강제 컴포넌트 교체의 예제이다.\n\n\n\n\nSHOULDREUSEROUTE는 여러 번 호출된다\n\n> 👀 Angular v8 버전 기준으로 작성했으나 v9 에서도 비슷하게 동작한다\n\nAngular의 RouteConfig [https://angular.io/api/router/Route]는 재귀적으로 선언할 수 있게 되어있다.\n그래서 shouldReuseRoute의 future, curr파라미터는 path에 대한 정보를 트리 구조로 담고 있다. 자세한 내용은 예제를\n통해 파악해 보자.\n\n먼저 앱의 라우팅 설정이 아래처럼 선언되어 있다고 가정한다.\n\n// app-routing.module.ts\nconst routes = [\n  { path: 'list', component: ListComponent },\n  { path: 'detail/:id', component: DetailComponent },\n  {\n    path: 'delivery',\n    loadChildren: () =>\n      import(\"'./delivery/delivery.module'\").then((mod) => mod.DeliveryModule),\n  },\n]\n\n// delivery-routing.module.ts\nconst routes = [{ path: 'detail/:id', component: DeliveryDetailComponent }]\n\n아래 RouteReuseStrategy는 각 호출 단계에서 url과 해당 Route와 연결된 컴포넌트 이름을 출력한다. 이 strategy를\n사용하여 위의 Route 설정에서 발생할 수 있는 이동들에 대한 호출 로그를 분석해 보자.\n\nexport class CustomRouteReuseStrategy implements RouteReuseStrategy {\n  shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ) {\n    // 분석을 위해 파라미터를 로깅함\n    console.log(\n      `[future]\\n${getInfo(future)}\\n\\n[curr]:\\n${getInfo(curr)}\\n\\n----------`\n    )\n\n    return future.routeConfig === curr.routeConfig\n  }\n}\n\n1. 앱 진입\n\n[future]\n  → '' / null\n[curr]:\n  → '' / null\n----------\n\n앱 진입 시점에 한번 호출된다. 큰 의미는 없다\n\n2. ” 에서 ‘list’로 이동하는 경우\n\n[future]\n  → '' / AppComponent\n  → 'list' / ListComponent\n[curr]:\n  → '' / null\n----------\n\nfuture를 보면 ''는 AppComponent, 'list'는 ListComponent에 제공되는 것을 알 수 있다.\n\n3. ‘list’에서 ‘detail/2’로 이동하는 경우\n\n[future]\n  → '' / AppComponent\n  → 'detail/2' / DetailComponent\n[curr]:\n  → '' / AppComponent\n  → 'list' / ListComponent\n----------\n[future]\n  → 'list' / ListComponent\n[curr]:\n  → 'detail/2' / DetailComponent\n----------\n\n * shoudReuseRoute가 두 번 호출되고 있다.\n * 이상한 점이 있는데 두 번째 호출에서는 future, curr값이 뒤바뀌었다.\n * future를 보면 AppComponent에는 :id에 해당하는 문자열이 없다. 라우팅 설정 자체도 그러한데. AppComponent가\n   DI 받는 ActivatedRouteSnapshot에서는 id를 가져올 수 없는 이유이기도 하다.\n\n4. ‘detail/2’에서 ‘delivery/detail/4’로 이동하는 경우\n\n[future]\n  → '' / AppComponent\n  → 'delivery' / null\n  → 'detail/4' / DeliveryDetailComponent\n[curr]:\n  → '' / AppComponent\n  → 'detail/2' / DetailComponent\n----------\n[future]\n  → 'detail/2' / DetailComponent\n[curr]:\n  → 'delivery' / null\n  → 'detail/4' / DeliveryDetailComponent\n----------\n\n * 3번처럼 두 번째 호출의 future, curr값이 뒤바뀌어 있다. 관련 PR\n   [https://github.com/angular/angular/issues/16192]이 있는데 아직 별다른 업데이트가 없다.\n * 첫 shouldReuseRoute의 호출에서 future파라미터를 보면loadChildren을 사용한 Route에는 컴포넌트가 없다.\n\n두 번씩 호출되는 이유는 어디에도 나와 있지 않지만. 관련 코드\n[https://github.com/angular/angular/blob/3e51a1998304ab6a15e5bea6bc66e7a8c636a8ad/packages/router/src/create_router_state.ts]를\n볼 때 두 번씩 호출하더라도 컴포넌트에 올바른 라우팅 상태를 줄 수 있기 때문에 따로 정리하지 않은 것으로 보인다. 따라서 구현 할 때 주의가\n필요하다.\n\n\n상품상세, 목록 간 컴포넌트 캐싱 예제\n\n상세, 목록 페이지의 경우 상세에서 뒤로가기 시 이전에 보고 있던 목록과 스크롤을 유지하면 페이지 탐색 사용성을 크게 개선할 수 있다. 특히\n전자상거래 서비스의 경우 매출과 직결되는 부분이기도 하다.\n\n캐싱을 위해 일반적으로 bfcache에 의존하거나. 상세 진입 전의 앱 상태를 persist로 저장했다가 복원하는 방법을 사용하는데. 두 방법은\n코드베이스 외적인 부분에 의존하기 때문에 관리가 어렵고 사이드이펙트가 있을 수 있다.\n\nRouteReuseStrategy를 이용한 방법은 캐싱이 필요한 구간에 부분적으로 적용해야 하지만 구현이 코드베이스 안에 있으므로 앞서 언급한\n문제에서 자유롭다. 가능한 이 방법을 도입하는 것이 좋아 보인다.\n\n\n\n위 데모는 상품목록 상세 예제이다. 원래라면 상세에서 뒤로 가기로 목록으로 돌아왔을 때 컴포넌트가 교체어 버리므로 화면이 깜빡이며 1번째\n페이지부터 새로 그리고. 스크롤 위치도 잃어버린다.\n\nexport class CustomRouteReuseStrategy extends RouteReuseStrategy {\n  private cache = new Map<string, DetachedRouteHandle>()\n\n  shouldDetach(route: ActivatedRouteSnapshot) {\n    // 목록에서 빠져나갈 때 true반환하여 store를 호출한다\n    if (getPath(route).startsWith('list')) {\n      return true\n    }\n\n    return false\n  }\n\n  store(route: ActivatedRouteSnapshot, detachedTree: DetachedRouteHandle) {\n    // 컴포넌트 상태 캐시\n    this.cache.set(getPath(route), detachedTree)\n  }\n\n  shouldAttach(route: ActivatedRouteSnapshot) {\n    const path = getPath(route)\n\n    // 목록 재진입 시 캐시가 있다면 true반환하여 retrieve를 호출한다\n    if (path.startsWith('list') && this.cache.has(path)) {\n      return true\n    }\n\n    return false\n  }\n\n  retrieve(route: ActivatedRouteSnapshot) {\n    // 컴포넌트 상태 복원\n    return this.cache.get(getPath(route))\n  }\n\n  shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ) {\n    return future.routeConfig === curr.routeConfig\n  }\n}\n\n하지만 본문에서 설명한 RouteReuseStrategy를 상속한 커스텀 클래스를 구현하면 캐시된 컨텍스트를 복원하기 때문에 상품목록 컴포넌트가\n깜빡이지 않고 곧바로 렌더링 되는 것을 볼 수 있다.","n":0.031},"1":{"v":"Angular의 RouteReuseStrategy","n":0.707},"2":[{"v":"routereusestrategy","i":1,"n":1},{"v":"angular","i":0,"n":1}]}},{"i":17,"$":{"0":{"v":"Louqe Ghost S1과 Synology DS216j [/_astro/R0000467.XB_GIjxw_Z19zpUl.webp]Louqe\nGhost S1과 Synology DS216j\n\n어렸을 때 PC게임을 좋아했다. 100만원 남짓이었던 첫 월급으로 게임용 PC를 구입하고. 상상도 못했던 크라이시스\n[https://www.ea.com/games/crysis/crysis?isLocalized=true]를 돌렸을 때의 감격이 아직도 기억난다.\n\n얼마 전 정말 오랜만에 최신 PC게임을 무리없이 할 수 있는 컴퓨터를 조립했다. 사실은 웹 서핑중에 Ghost S1\n[http://www.louqe.com/]이라는 작고 매우 이쁜 케이스를 발견하고는 한달 정도 고민하다가 부품까지 같이 주문했다.\n\n조립하고 잘 사용하다가. 얼마 후 Google Chrome을 실행하는 과정에서 블루스크린이 뜨더니 재부팅되며 윈도우 진입이 불가한 상태가\n되었다. 😢\n\n매 부팅시마다 BIOS에서 M.2 SSD가 인식이 되었다 안되었다 했고. 인식이 되더라도 설치 파티션 선택 과정에서 0.0MB 로 디스크가\n인식되었다가. 선택해서 설치하려 하면 설치할 수 없다는 경고 메시지가 출력되고 진행이 불가했다.\n\n결국 SSD(삼성 970 Evo Plus 500GB), 메인보드(GIGABYTE Z390 I AORUS PRO WIFI) 를 모두 A/S\n보냈다. 그런데 기간이 너무 길어져 그냥 SSD, 메인보드를 구입하고 교환한 제품을 중고로 팔았다.\n\n그런데 새로 구입한 조합에서도 윈도우 설치 1%일 때 0xC0000005 오류가 발생하며 설치를 할 수 없었다. 슬슬 스트레스를 받기 시작했다.\n🤬\n\n찾다 보니 RAM 오류라는 말이 있어. A/S센터에 연락해 보니. 그럴 확률이 높다는 대답을 들었고. RAM도 A/S를 보냈다. 그리고 또\n귀찮아서 중간에 RAM을 새로 구입했다 (삼성 16GB RAM 두장) 그런데 동일한 현상이 발생했다.\n\n수리를 시작한 지 2주가 지나 알게 된 사실인데. 설치를 위해 만든 USB자체에 문제가 있었나보다. USB를 새로 구입해 설치를 진행하니 별\n문제 없이 설치가 되었다.","n":0.072},"1":{"v":"Ghost S1 조립컴퓨터 수리","n":0.5},"2":[{"v":"ghost s1","i":0,"n":0.707}]}}]},"list":[{"content":"프로젝트 진행 중 새로운 상태 관리 라이브러리를 도입해야 하는 상황이다.\n\n보통 서드파티 라이브러리 중 하나를 고를텐데. 이 때 보통 하는 것이 아래 사항들을 검토하는 것이다.\n\n * 필요성: 직접 구현보다 라이브러리를 쓰는 게 명확히 이점이 있는가?\n * 안정성: 최근까지 활발히 유지보수되고 있는가?\n * 커뮤니티: 이슈 대응 속도와 커뮤니티 지원이 활발한가?\n * API 안정성: 업데이트 시 Breaking Change 위험이 낮은가?\n * 유지보수 용이성: 라이브러리 교체/제거가 쉽게 가능하도록 구조화했는가?\n * 그 외 호환성, 번들 크기, 성능 영향 등.\n\n검토 이유, 항목 별 우선순위는 상황마다 다르지만 근본적인 이유는 개발 조직이 이 라이브러리를 도입하여 얻는 실직적인 이득이 있는가? 이겠다.\n\n그런데 유독 이런 조건을 빡빡하게 따지지 않는 경우가 있다. 바로 nextjs, angular와 같은 프론트엔드 프레임웍들이다.\n\n\n보통 NEXTJS가 선택되는 이유\n\n * react 기반이다\n * ssr 을 쉽게 구현할 수 있다\n   * seo 최적화, 첫 페이지 로딩 속도 개선\n * 간단한 백엔드 구현 가능 -> 서버리스 및 엣지\n * 뛰어난 DX 및 생태계, 호스팅\n * 유지보수 및 채용\n * 최근 트렌드임\n\n보통 위의 조건으로 nextjs 가 선택된다. 개발 조직 입장에서는 꽤 합리적인 조건이라고 생각할 수 있다.\n\n\n누락된 검토 사항\n\n어떤 프론트엔드 프레임웍을 선택하는가에 따라 구현가능한 기술, 구현이 어려운 기능, 지불해야하는 비용 등이 천차만별로 달라진다.\n\n따라서 선택 조건에는 위 개발조직 입장에서의 항목 뿐만 아니라 이런 부분도 반영되어야 한다.\n\n자본금을 관리하는 재무 부서, 제품 퀄리티를 최종적인 입장에서 챙겨야 하는 마케팅, 기획 부서의 요구사항도 반영할 수 있는가가 중요하다는\n것이다.\n\n사실 다른 부서들은 프론트엔드 개발팀에 이런 부분을 알고 요구할 수 없다. 거의 대부분은 그냥 믿고 맞기는 것이다.\n\n고객은 본인이 뭘 원하는 지 본인도 모른다라는 말이 있지 않은가? 비슷한 맥락이라 볼 수 있다.\n\n\n불필요한 서버비 지출\n\nnextjs 는 nodejs를 실행할 수 있는 웹 서버가 필요하다. aws 기준으로 ec2 or lambda, alb 여기에 오토 스케일까지\n고려할 경우 그 비용은 엄청 늘어난다.\n\n반면 CSR 정적 호스팅은 어떤가. S3, CloudFront 사용료만 내면 되고 인스턴스같은 건 없다.\n\n그럼 여기서 따져봐야 할 것은. 그 압도적인 비용 차이를 납득시킬만 한 가치를 제공할 수 있는가 이다.\n\n최소 10배 이상의 서버비를 납부하며 얻은 서버 측 렌더링 혹은 그 부수적인 가능성을 통해 회사가 목표로 하는 임팩트를 만들거나, 그에 가까워질\n수 있나?\n\n잠깐 스켈레톤 보였다 내용이 천천히 나오는 것을 로딩 없이 보여주도록 변경한 것이. 저 위의 비용과 맞먹는 임팩트를 주는 건가?\n\n추가로 로딩이 없어 지는것이 맞는가?\n\n\n요구사항 구현의 어려움\n\n페이지 이동 취소 기능 [/_astro/page_leave.DlXWnbfc_BLe8r.webp]페이지 이동 취소 기능\n\nnextjs 는 웹 앱 기획에서 거의 필수적으로 요구하는 페이지 이동 취소 기능을 공식적으로 제공하지 않고 있다.\n\ntrick 을 사용하면 가능한데. 이 경우 주소창의 경로와 보이는 페이지가 달라지는 문제가 발생할 수 있다.\n\n보통 고맙게도 노운이슈 처리하여 개별 안내하거나 하는 등으로 넘어가는데, 한편으로는 안타깝기 그지 없다.\n\n이 기능은 고객이 정보를 한창 입력하다 실수로 페이지를 벗어나버려 입력된 정보를 날리는 문제를 해결하기 위해 추가되는데. 상황에 따라 큰 문제가\n될 수 있다.\n\n입력값을 세션에 담아 주면 되지 않느냐란 의문이 있을 수 있는데, 그렇게 해서 해결이 되는 곳이 있고 아닌 곳이 있다.\n\n\n그래서 어떻게 하라고\n\n당연히 위의 내용은 구성원 혹은 구성 조직의 방향성에 따라 전혀 문제가 되지 않을 수 있다.\n\n이런 부분들도 함께 고려하여 조직이나 회사에 기여하는것이 진정한 값어치를 하는 것이다.\n\nnextjs 를 쓰지 말라는 이야기도 더더욱 아니다. 이왕 쓸것이라면 사용 방법과 주의사항, 문제가 될 수 있는 부분에 대한 대비를 충분히 해야\n한 다는 것이다.\n\n라우팅 경로 별 렌더링 전략을 달리 할 수 있는 것은 정말 멋진 기능이긴 하다. 멋진 기능이긴 하다.","frontmatter":{"title":"Next.js 를 사용해야 할 이유","description":"next.js 를 사용해야 할 이유에 대해 생각해 봅니다.","cover":"../../assets/postImages/20250727/cover.webp","categories":["Development"],"tags":["next.js","angular","remix","astro","frontend"],"publishedAt":"2025-07-27T15:04+09:00","readingTime":406200,"summary":"프로젝트 진행 중 새로운 상태 관리 라이브러리를 도입해야 하는 상황이다.보통 서드파티 라이브러리 중 하나를 고를텐데. 이 때 보통 하는 것이 아래 사항들을 검토하는 것이다.필요성: 직접 구현보다 라이브러리를 쓰는 게 명확히 이점이 있는가?안정성: 최근까지 활발히 유지보","coverColors":["#dfdfdf","#8b8b8b","#635640","#513f38","#33353a"]},"pathname":"/blog/nextjs-를-사용해야-할-이유/"},{"content":"MCP는 Anthropic이 24년 11월 공개한 오픈소스 프로토콜이다. AI모델이 외부 도구를 사용하는 개념은 이전에도 있었지만, 개방적인\n설계로 커뮤니티 참여를 유도하여 현 시점 꽤 핫한 기술이 되었다.\n\n당장 실무에 도움이 되지 않더라도, 트렌드는 파악해야겠다 싶어 Open WebUI, MCPO, MCP Server를 활용해 외부 html문서를\n요약하는 시나리오를 테스트 해 보았다.\n\n현재까지 Youtube등의 활용 사례들은 대부분 claude desktop이나 cursor, vscode를 사용하고 있는데, 로컬 모델을\n사용하기 위해 Open WebUI를 사용했다.\n\n\nOLLAMA\n\nollama [https://ollama.com] 설치하고 gemma3:27b, mistral-small3.1 모델을 받는다.\n\nollama run gemma3:27b\nollama run mistral-small3.1\n\n\nOPEN WEBUI\n\nInstallation with Default Configuration\n[https://github.com/open-webui/open-webui?tab=readme-ov-file#installation-with-default-configuration]\n참고. Nvidia GPU Support로 설치.\n\nWSL2 에서 쓰면 성능 감소폭이 크다는 글도 있고 큰 차이가 없다는 글도 있긴 한데 찍먹이므로 스킵.\n\ndocker run -d -p 3000:8080 --gpus all -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:cuda\n\n\nMCPO\n\nMCPO는 Open WebUI에게 사용 가능한 tool목록과 인자, 데이터를 OpenAPI 포멧으로 주고받고. MCP 서버와는 호환되는 데이터\n포멧으로 데이터를 주고받는 중계 서버 역할을 한다.\n\n자세한 내용은 MCPO: Supercharge Open-WebUI /Ollama with MCP Tools\n[https://mychen76.medium.com/mcpo-supercharge-open-webui-with-mcp-tools-4ee55024c371]\n참고.\n\nInstalling UV [https://docs.astral.sh/uv/getting-started/installation/] python\n사용해야 하므로 UV설치. python만 쓰면 되므로 필요에 따라 다른 것 써도 무방할 듯.\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\nbackend 라는 폴더 하나 만들고 안에서 프로젝트 설정 및 mcpo, mcp server들 설치.\n\ncd ~/Projects/\nmkdir backend\ncd backend\n\nuv venv\nuv pip install mcpo mcp-server-fetch mcp-server-time\n\nrun.sh 파일 만들고 아래 내용 기입.\n\n#!/usr/bin/env bash\n\nuvx mcpo --config ./config.json --host localhost --port 8080\n\nconfig.json 파일 만들고 아래 내용 기입.\n\n{\n  \"mcpServers\": {\n    \"fetch\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-fetch\"]\n    },\n    \"time\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-time\", \"--local-timezone=Asia/Seoul\"]\n    }\n  }\n}\n\n그 다음 MCPO 서버 실행\n\n./run.sh\n\nStarting  MCP OpenAPI Proxy with config file: ./config.json\nINFO:     Started server process [30187]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://localhost:8080 (Press CTRL+C to quit)\n\n\nOPEN WEBUI 세팅\n\n 1. 좌측 하단 사용자 이름 눌러 설정 클릭.\n\n 2. 모달 좌측 중간의 도구 클릭.\n\n 3. Manage Tool Servers 우측 + 버튼 클릭.\n\n 4. 위 config.json 에 2개 등록했으므로 아래 두개 항목 추가. (새로고침 아이콘 눌러서 연결 성공 토스트 노출되면 OK)\n    \n    1. url: \"http://localhost:8080/fetch/openapi.json\"\n    2. url: \"http://localhost:8080/time/openapi.json\"\n\n 5. 이제 프롬프트 입력창 좌측 하단에 렌치 아이콘과 함께 tool을 사용할 수 있게 됨\n\n위에서 추가한 두개의 tool에 대한 아이콘이 노출됨 [/_astro/open-webui-tools.hMaYaYzT_Zl9PDs.webp]위에서\n추가한 두개의 tool에 대한 아이콘이 노출됨 config.json에 추가했던 tool목록이 노출됨. 참고로\nmcp-server-fetch-python 인 건 mcp-server-fetch 대신 다른 tool추가해서 그럼\n[/_astro/open-webui-tools2.CPKYvj34_ZTqrwv.webp]config.json에 추가했던 tool목록이 노출됨.\n참고로 mcp-server-fetch-python 인 건 mcp-server-fetch 대신 다른 tool추가해서 그럼\n\n\nMCP FETCH 사용 소감\n\n> 블로그 발행 후 다시 테스트 해 보니 gemma3, mistral-small3.1 둘 다 tool을 잘 활용하길래 내용을 수정했다.\n\n * 테스트에 사용한 GPU는 RTX 4090 Founders Edition (24GB)\n * 로컬 모델은 gemma3, mistral-small3.1 두개 돌렸고 추가로 개발자 설정에서 MCP를 사용할 수 있는 claude\n   desktop을 사용해 보았다\n * fetch는 mcp-server-fetch\n   [https://github.com/modelcontextprotocol/servers/tree/main/src/fetch],\n   mcp-server-fetch-python [https://github.com/tatn/mcp-server-fetch-python] 둘 다\n   테스트해 보았다\n\n--------------------------------------------------------------------------------\n\n * 일단 ChatGPT는 붙여넣은 링크를 인식하지 못 했다. 다만 해당 주소를 웹 검색하여 내용을 찾아내고. 본문에 대한 질문에 답변해주었다.\n   이런 걸 보면 tool들을 상호 보완하여 사용해야 할 듯.\n * claude.ai 는 아예 붙여넣은 링크는 인식 자체를 하지 못 했다. 웹 검색을 시도하지도 않았다.\n * 로컬 모델 및 claude desktop의 경우 MCP를 잘 활용하여 요약을 해 주었다. 다만 크롤링이 안 되면 요약 불가했음.\n * 당연한 말이지만 MCP서버의 품질에 따른 경험 차이가 클 것으로 보인다. Youtube에서 MCP를 소개할 때 Figma 를 예시로 드는\n   이유가 있는 듯. MCP는 결국 서드파티이기 때문에 경우에 따라서 희망고문만 당하다 끝날지도 모르겠다.\n\nmcp-server-fetch-python이 제공하는 tool_get_markdown_post를 사용하여 블로그 본문을 정확히 요약해주었다.\n[/_astro/using-tool.Cp7qob7Q_16ttnf.webp]mcp-server-fetch-python이 제공하는\ntool_get_markdown_post를 사용하여 블로그 본문을 정확히 요약해주었다. 네이버 블로그 글은 MCP 서버가 크롤링을 못 해서\n내용을 알 수 없다고 안내해 주었다. [/_astro/using-tool2.DIqzRlCG_2qvo1t.webp]네이버 블로그 글은 MCP\n서버가 크롤링을 못 해서 내용을 알 수 없다고 안내해 주었다. claude desktop도 mcp-server-fetch를 사용해 본문 내용을\n요약해 주었다. [/_astro/using-tool3.CR-RxKZy_wnisn.webp]claude desktop도\nmcp-server-fetch를 사용해 본문 내용을 요약해 주었다. MCP 서버가 크롤링을 못 하니 claude desktop도 요약이\n불가능했다. [/_astro/using-tool4.Ds_MtmEr_Z29dhJ4.webp]MCP 서버가 크롤링을 못 하니 claude\ndesktop도 요약이 불가능했다.","frontmatter":{"title":"Open WebUI - MCP fetch 체험기","description":"Open WebUI 와 MCPO를 활용한 mcp-fetch-server 의 사용 경험과 짦은 소감","categories":["AI"],"tags":["Open WebUI","MCP","AI Model"],"publishedAt":"2025-04-09T03:05+09:00","cover":"../../assets/postImages/20250409/what-is-mcp.png","coverAlt":"Image from https://norahsakal.com/blog/mcp-vs-api-model-context-protocol-explained/","readingTime":332100,"summary":"MCP는 Anthropic이 24년 11월 공개한 오픈소스 프로토콜이다. AI모델이 외부 도구를 사용하는 개념은 이전에도 있었지만, 개방적인 설계로 커뮤니티 참여를 유도하여 현 시점 꽤 핫한 기술이 되었다.당장 실무에 도움이 되지 않더라도, 트렌드는 파악해야겠다 싶어 ","coverColors":["#faf3eb","#f0b729","#9b948e","#c64551","#19292c"]},"pathname":"/blog/open-webui-mcp-fetch/"},{"content":"얼마 전부터 AI를 활용한 개발을 Vibe Coding이라 부르기 시작했다. 단어가 적절한 듯 하면서도, 무언가 불필요하게 포장된 느낌이라 입에\n딱 붙지는 않는다. 업무상으로는 아예 다르게 표현하고 싶을 정도…\n\n2024년 중순부터 내가 속한 조직에서도 copilot을 시작으로 cursor 등의 AI를 보안 검토 후 사용할 수 있게 되었다. 하지만 개인,\n조직별로 실제 실무에서 적극적으로 활용하는 사례는 찾아보기 어려운 듯 하다.\n\n개발자 커뮤니티, YouTube등 AI를 활용한 개발 대해 검색해 보면. 효율에 대해서는 좀 의견이 갈린다. 실제로 과거에 함께 일했던 분들의\n이야기를 들어보았을 때는 부정적인 쪽으로 기울어있는 느낌이다.\n\n실효성이 떨어진다는 의견의 이유를 대략 정리하자면 “답변의 품질이 떨어진다.”, “업무 프로세스 상 번거롭다.” 였다.\n\n\n생각 외로 높은 진입장벽\n\nAI로 효율적으로 사용하는 것이 생각보다 만만하지 않다. 그나마 IDE에 붙어 다양한 정보를 주고받게 되어 단순 채팅만 가능했던 이전보다 쓰기는\n훨씬 편해졌지만, 그것만으로 실무에서 큰 도움이나 성과를 만들기엔 무리가 있다.\n\n효과를 보려면 의외로 개발자의 역량이 중요하다. 일을 시키는것도 아는 사람이 시킬수 있고. AI모델의 답변을 다듬는데에도 역량이 필요하다.\n\n또한 업무 프로세스도 이에 맞춰 변경해야 한다. 단순히 지금 필요한 알고리즘을 짜는 것이 아니라, 업무 시작 전. AI로 효율을 볼 수 있는\n작업과 직접 하는게 더 효율적인 작업을 현실감있게 구분한다. AI에 대한 지식이 필요한 부분이다.\n\n업무 환경도 중요하다. 협업 부서와의 R&R정리라던가, AI사용에 대해 편견이 없는 분위기인지 등등 생각보다 이 부분도 쉽지 않다.\n\n그 동안 대부분의 개발자가 행해 왔던 업무 패턴도 걸림돌이 될 수 있다. 이를테면 자체 구축한 디자인 시스템을 AI가 알 수 있나? 당연 방법은\n있지만 AI활용에 대한 지식이 필요하다.\n\n\n앞으로의 VIBE CODING\n\n앞서 언급한 내용들로 인해. 현실적으로 소, 중규모의 시스템이 갖춰지지 않은 곳에서 먼저 적극적으로 활용할 듯 하다. 다만 대기업들이\ncopilot이나 cursor 등 보안에 민감한데도 도입하는 것을 보면 중요성을 알게 되는 순간 금방 따라잡긴 할 것이다.\n\n과거 2010년 중반만 해도 Virtual Scroll, Web Based Editable Grid 들은 어느정도 FE에 투자할 수 있는 의지나\n규모가 있는 회사들의 전유물이었다. 그런데 지금은 AI모델 하나에 스크린샷만 보여주면 눈 깜짝할새에 구현해주는것은 물론이며 SSR도 완벽하다.\n\n그리고 AI는 정말 빠르게 새로운 기술이 나오고 있다. 지금도 그렇지만 23년만 해도 AI에 관련된 논문이 쉴 새 없이 쏟아져 나왔는데. 이제는\n개발자가 실제로 적용할만한 기능으로 쏟아져 나오고 있다.\n\n당연히 FE개발 역량은 똑같이 쌓아야 하며, AI에 대한 트렌드도 적극적으로 따라가야 한다. 대기업이 허튼데 돈 쓰는것 본 적이 없다.","frontmatter":{"title":"Vibe Coding","description":"Vibe Coding에 대한 13년차 프론트엔드 개발자의 생각","cover":"../../assets/postImages/20250331/pexels-googledeepmind-17483848.jpg","coverAlt":"Image from Google DeepMind","categories":["AI"],"tags":["Frontend","Development","AI","Vibe Coding"],"publishedAt":"2025-03-31T20:32+09:00","readingTime":293100,"summary":"얼마 전부터 AI를 활용한 개발을 Vibe Coding이라 부르기 시작했다. 단어가 적절한 듯 하면서도,\n무언가 불필요하게 포장된 느낌이라 입에 딱 붙지는 않는다. 업무상으로는 아예 다르게 표현하고 싶을 정도…2024년 중순부터 내가 속한 조직에서도 copilot을 시","coverColors":["#5fdbb3","#e09978","#569dd4","#1ea84e","#0c2053"]},"pathname":"/blog/vibe-coding/"},{"content":"큰 규모의 개발 조직에서는 보통 예산이나 리소스를 여유롭게 사용한다. 이런 조직은 대개 비용보다 기술적인 가치에 중점을 두고 업무를 진행하고\n평가도 그 기준에 따른다.\n\n그렇다 보니 소속 개발자들도 자연스럽게 기술 중심으로 업무를 진행하게 된다. 예를 들면 최신 기술 스택들을 빠르게 습득하여 서비스에 적용하고.\n그 과정에서 경우에 따라 예산이나 리소스를 초과하여 사용하기도 한다.\n\n회사 입장에서는 요구한 바를 충실히 이행하였고 초과분에 대한 소명도 방향성에 어긋나지 않으므로 딱히 문제 될 것이 없다. 하지만, 이런 방식은\n나한테도 회사에게도 장기적으로는 전혀 도움이 되지 않을 가능성이 높다.\n\n이런 부류의 오류들 중. 가장 흔한 형태의 오류를 꼽자면 Server Side Rendering의 적용이 있다.\n\n서비스에 왜 SSR을 적용해야 하는지. Next.js를 사용해야 하는지 “기술적”, “환경적” 두 가지 측면의 고려가 필요하다. 기술적 측면은\n익숙한 개발자들이 많지만, 환경적 측면을 고려하지 못 하면 주니어 개발자에 머무를 수 밖에 없다.\n\n\n고려해야 할 사항들\n\n이제 “SSR적용” 이라는 주제로 고려할 수 있는 내용에 대해 이야기 해 보자. 먼저 보면서 생각할만한 내용이 필요하므로 각 방식의 장단점을\n정리한다.\n\n“SSR의 장단점”을 구글링하면 정말 많은 글을 볼 수 있다. 일부 글에서는 SSR을 php같은 전통적인 방식으로 설명하는 경우도 있고.\n부정확하거나 모호하게 정리한 경우가 대부분이긴 하나. 대략 정리해보면 아래와 같다.\n\n단점은 장점들의 반대이므로 따로 적지 않았다. 또 SSG나 iSSG(Next.js에서는 ISR이라 함)은 사실 SSR을 사용할 때 경우에 따라\n적용할 수 있는 기법이므로 따로 구별하지 않는다.\n\n\nCSR\n\n * 인프라 구성이 비교적 단순하고 저렴하다.\n * INP가 짧다.\n\n\nSSR\n\n * SEO에서 조금 더 유리하다.\n * FCP가 짧다.\n\n\n기술적 측면\n\n보통 주니어 시절에 집착하는 기술 중심의 고민들이다.\n\n * CSR은 INP가 짧은가? 반대로 SSR은 INP가 길 수밖에 없나?\n * SSR은 FCP가 짧은가? 반대로 CSR은 FCP가 길 수밖에 없나?\n   * SSR의 TTFB는 CSR에 비교해 길 수밖에 없나? 반대로 CSR은 TTFB가 짧나?\n * CSR은 Waterfall Request에 취약한가? 해결책이 정말 SSR, Suspense 뿐인가?\n * SEO의 적용이 서비스에 어떤 이득을 주는가?\n * CSR은 UI/UX측면에서 제한되는가? 반대로 SSR은 제한이 없는가?\n * CWV는 어떠한가? 모바일의 경우 클라이언트 측에 전송되는 전체 리소스 크기는 어떠한가?\n * 빌드 속도는 어떠한가?\n * 트래픽이 늘어나도 안정적인가?\n\n\n환경적 측면\n\n조직과 회사 입장에서의 고민이 중심이다.\n\n * CSR의 기술 및 인프라 구성이 단순한가? 단순함의 기준이 무엇인가?\n   * 조직 내 모든 구성원이 이해할 수 있을 정도로 단순한가? (쉽나)\n * 반대로 SSR의 기술 및 인프라 구성은 어려운가? 어려움의 기준은 무엇인가?\n   * 도입 후 유지보수는 어떠한가? 새로운 비즈니스의 빠른 실현에 발목을 잡지는 않는가?\n   * 클라우드 쓰면 된다고? 비용은 공짜인가?\n * CSR의 인프라 구성이 정말로 저렴한가? 반대로 SSR은 비용이 높을 수 밖에 없나?\n * SSR도입 대비 얻을 수 있는 이득을 돈으로 환산하면 얼마나 될까? 혹시 마이너스가 되지는 않는가?\n * CSR(혹은 SSR)도입이 개발자 커리어와 회사 모두에게 이득이 되는가?\n * CSR(혹은 SSR)도입이 협업에 도움이 되는가?\n\n\n정말 중요한 것\n\n연차가 쌓여 기술에 대한 결정권을 갖게 되면. 예시로 다뤘던 위의 내용 외에 디자인 시스템의 자체 개발 여부 등 다양한 선택들에서도 깊은 고민이\n필요하다.\n\n기술 트렌드를 쫓는것도 중요하다. 새 기술이 만들어지고 유행을 타는 것은 대부분 개발자들의 고민을 해결해 주거나 해결을 위한 아이디어를 떠오르게\n해 줄수 있기 때문이다.\n\n하지만. 그런 기술이나 방법론을 무작정 따르기 보다는 해당 기술의 탄생 배경에 대한 이해. 그리고 현재 서비스에 제공할 수 있는 기회 혹은\n기회비용을 정확히 파악하고 필요에 따라 적용해야 한다.\n\n충분한 이해 후에는 단순 도입 뿐만 아니라 아이디어만 차용하는 등 여러 선택지도 생기며. 그에 따라 모두에게 이득을 주는 진정한 기여에 더\n가까워질 수 있다.\n\n또. 개발중인 소스코드 포함하여 회사 뿐만 아니라 개발자 본인도 영원하지 않다는 것을 기억해야 한다. 언제까지 지금 둘러진 울타리 안에서 풍족한\n리소스를 소비해가며 서비스를 개발하고 운영할 수 있을까?","frontmatter":{"title":"연차가 쌓일수록 중요해지는 것","description":"시니어 개발자라면 꼭 고민해야 하는 것에 대한 이야기","cover":"../../assets/postImages/20241120/cover.webp","categories":["Mindset"],"tags":["development cost management"],"publishedAt":"2024-11-20T20:32+09:00","readingTime":435000,"summary":"큰 규모의 개발 조직에서는 보통 예산이나 리소스를 여유롭게 사용한다. 이런 조직은 대개 비용보다 기술적인 가치에 중점을 두고 업무를 진행하고 평가도 그 기준에 따른다.그렇다 보니 소속 개발자들도 자연스럽게 기술 중심으로 업무를 진행하게 된다. 예를 들면 최신 기술 스택","coverColors":["#a19e9b","#54595b","#424142","#633921","#1f201b"]},"pathname":"/blog/연차가-쌓일수록-중요해지는-것/"},{"content":"최근 담당한 서비스의 CWV개선작업 중 이상한 현상을 발견했다. 페이지에 <img>태그를 사용하면 이미지가 강제로 preload처리되는\n현상이었다. CWV개선을 위해서는 리소스의 우선순위 최적화\n[https://patterns-dev-kr.github.io/performance-patterns/loading-sequence/]가\n필수인데. 위의 현상때문에 주요 리소스의 로드를 방해하도록 html이 렌더되었다.\n\nexport default function Page() {\n  return (\n    <>\n      <img src=\"a.png\" />\n    </>\n  )\n}\n\n// 렌더링 결과:\n// \"<html><head>\n//   <link rel=\"preload\" href=\"a.png\" /> ???\n// </head> ...생략... </html>\"\n\n위 현상 때문에 LCP를 많이 깎아먹고 있었고 이리저리 소스코드를 보다 보니 이 문제가 next.js가 아니라 react-dom의 의도된\n동작이라는 것을 알았다.\n\n[Fizz] Preload “suspensey” images [https://github.com/facebook/react/pull/27191]\n\n해당 PR은 이미지들의 loading, fetchPriority 속성에 따라 몇 가지 조건에 해당할 경우 이미지들을 preload처리하는\n내용이다.\n\n특이한 건 이후에 <picture>안의 <img>들은 preload하지 않는 PR도 발견했다.\n\n[Fizz][Float] <img> inside <picture> should not preload during SSR\n[https://github.com/facebook/react/pull/27346]\n\n위의 내용을 정리하면. react-dom/server 의 스트리밍 버전인 ReactDomFizzServer의\nrenderToPipableStream함수는. 이미지가 <picture>바깥에 있거나, fetchPriority=\"low\" 속성을 주지 않을\n경우. 강제로 preload처리한다.\n\n리소스 우선순위를 조절해야 하는 경우. <img>태그를 직접 사용하려면 위의 조건을 만족하도록 적절히 수정하거나, next.js가 제공하는\n<Image>태그를 사용해야 한다.\n\n--------------------------------------------------------------------------------\n\n위 현상의 경우 별다른 가이드가 없기도 하고. 사실 <img>태그들을 왜 preload처리하는지 이해가 안 된다. 심지어 이미지 태그가 20개면\n20개 다 preload하는데 도대체 왜 그런지…\n\nnext.js는 개발자들이 이미지를 사용할 때 대부분 <Image> 등 제공된 API들만 사용하길 바란 듯 하다. 그랬다면 위 문제는 없었을\n것이긴 하다…\n\n사실 next.js덕분에 개발자 입장에서는 다양한 렌더링 전략을 요구사항에 맞춰 쉽게 구현할 수 있어 좋긴 하다, 그런데 일전의\nSuspense사태도 그렇고. 올바른 방향으로 가고 있는지는 잘 모르겠다.","frontmatter":{"title":"Next.js에서 img태그 사용 시 주의사항","description":"next.js에서 img태그 사용 시 LCP에 악영향을 줄 수 있는데 이 내용에 대해 설명합니다","cover":"../../assets/postImages/20241101/cover.webp","categories":["Troubleshooting"],"tags":["next.js","performance"],"publishedAt":"2024-11-01T21:07+09:00","readingTime":172200,"summary":"최근 담당한 서비스의 CWV개선작업 중 이상한 현상을 발견했다. 페이지에 <img>태그를 사용하면 이미지가 강제로 preload처리되는 현상이었다. CWV개선을 위해서는 리소스의 우선순위 최적화가 필수인데. 위의 현상때문에 주요 리소스의 로드를 방해하도록 html이 렌","coverColors":["#f9f9f9","#7c8489","#707388","#534f4e","#050404"]},"pathname":"/blog/nextjs-img-tag/"},{"content":"페이지에 고해상도 이미지를 출력할 때 img요소는 본문에 삽입되었지만, 이미지가 바로 보이지 않고 잠시 후에 출력되는 현상을 본 적 있을\n것이다.\n\n이는 이미지 다운로드가 완료되었지만, 아직 디코딩이 되기 전이라 페이지에 나타나지 않는 현상이다.\n\n요소 추가와 동시에 이미지가 보여지길 원한다면 아래 코드를 사용해야 한다.\n\nconst img = new Image()\n\nimg.src = '...'\n\nimg.decode().then(() => {\n  document.body.appendChild(img)\n})\n\n\n이미지 디코딩\n\njpg, png, webp 등 대부분의 이미지 파일들은 기본적으로 인코딩이 되어 있다. 쉽게 말하자면 압축이 되어 있는 것이다. 브라우저는\n서버로부터 응답받은 이미지들을 화면에 그리기 위해 비트맵으로 디코딩. 즉 압축을 해제한다.\n\n디코딩 결과는 브라우저가 알아서 캐시 했다가 화면에 그려야 할 때 알아서 메모리에 올리고. 화면에서 안 보이면 다시 해제하는 식으로 동작한다.\n\n\n디코딩 시간에 영향을 주는 요소들\n\n먼저 CPU와 RAM에 영향을 많이 받으므로. 모바일에서 성능 감소가 두드러질 수 있다.\n\n제일 큰 영향을 주는 요소는 이미지의 크기이다. 크기가 큰 이미지일수록 디코딩 시간이 많이 소요되므로. 서버에서 적절한 크기의 이미지를 서빙하는\n것이 매우 중요하다.\n\n이미지 포멧 역시 중요하다. jpg나 png들은 오랫동안 사용되었기 때문이 이미 다양한 플랫폼에서 사용 가능한 효율적인 디코더(압축 해제\n프로그램으로 이해하면 쉽다)들이 존재하지만, 비교적 최신 포멧인 jpeg-xr이나 webp의 경우 아직 그렇지 않으므로 디코딩 시간이 증가할 수\n있다.\n\n호텔, 호스텔 등 다양한 숙박시설의 요금을 비교하는 서비스인 trivago 에서는 고객으로부터 jpeg-xr 포멧에 관련된 성능 이슈\n[https://calendar.perfplanet.com/2018/dont-use-jpeg-xr-on-the-web/]를 접수했었다고 한다.\n\n브라우저의 휴리스틱 알고리즘을 통해 일부는 GPU를 사용하여 디코딩되기도 하고. 이미지를 실제로 그려내기 위해서도 GPU를 사용하고 있기도\n하다.\n\n> 장축 3000정도 되는 크기의 이미지를 png, webp포멧으로 디코딩 시간을 측정해보았더니. png가 8ms, webp가 12ms 정도로\n> 증가하였다 (M2 macbook pro 에서 측정)\n> \n> 각 포멧별로 하드웨어 자체에서 디코더를 제공하거나 그렇지 않아 소프트웨어 디코더를 적용해야 하는 등. 브라우저마다 차이가 있으므로 결정 전에\n> 한번 찾아 보길 바란다.\n\n\n이미지 디코딩 시간 측정하기\n\n 1. 크롬 개발자 도구의 Performance탭에서 새로고침 측정으로 확인 가능.\n 2. webpagetest의 측정 결과를 chrome://tracing페이지에서 로드하여 확인 가능.\n\n\nDECODING 속성을 활용한 비동기 디코딩\n\n이미지 디코딩은 메인 쓰레드 혹은 래스터라이징 쓰레드를 사용한다. 해당 쓰레드에 여러 작업들이 할당될 것인데. 당연하게도 이미지 디코딩이 오래\n걸리면 다른 작업들은 지연된다.\n\n다른 이미지의 디코딩 뿐만 아니라, 화면에 텍스트를 출력하는 작업들도 지연될 수 있고. 60fps로 밑돌게 되면 사용자는 끊기는 느낌을 받게\n된다. 이 때 이미지 디코딩을 비동기 처리한다면 도움이 될 수 있다.\n\ndecoding 지정 전 [/_astro/before.DIyUZACy_Z1rtrfK.webp]decoding 지정 전\n\n위 이미지는 고해상도 이미지의 decoding을 적용하기 전의 측정 결과인데. 이미지 디코딩 과정으로 래스터라이저 쓰레드 1번의 다른 작업들이\n밀린것을 확인할 수 있다.\n\n여기서 img태그에 decoding=async를 할당할 경우 아래와 같은 측정 결과를 확인할 수 있다\n\ndecoding=async 지정 후 [/_astro/after.CZunzkoJ_ZezIXQ.webp]decoding=async 지정 후\n\n개선 후의 측정 결과에서 디코딩 과정이 별도의 프로세스로 분리되어 다른 태스크들이 방해되지 않는 것을 볼 수 있다.\n\n\n.DECODE() 메서드를 활용한 PREDECODING\n\n서론에서 언급한 문제는 주로 이미지 갤러리등 이미지가 주요 컨텐츠이거나, 스크롤 등에 따른 점진적인 이미지 렌더링 과정에서 두드러지며 UX에\n악영향을 끼칠 수 있다.\n\n이 때 앞서 언급한 코드와 같이 .decode()를 호출해 디코딩이 완료된 후 본문에 삽입할 경우 이런 문제들을 해결할 수 있다.\n\nnext.js의 이미지 컴포넌트의 decode적용\n[https://github.com/vercel/next.js/blob/v14.2.15/packages/next/src/client/image-component.tsx#L75]를\n참고해서 필요에 따라 서비스에 구현하도록 하자.\n\n--------------------------------------------------------------------------------\n\n참고\n\n 1. Don’t use JPEG-XR on the Web\n    [https://calendar.perfplanet.com/2018/dont-use-jpeg-xr-on-the-web/] jpeg-xr\n    은 소프트웨어 디코더를 사용하다 보니 렌더링 성능에 악영향이 있다고 함.\n 2. Best practices from open source: Use img.decode() in image-heavy\n    applications\n    [https://www.linkedin.com/pulse/best-practices-from-open-source-use-imgdecode-ramu-narasinga-qukie/]","frontmatter":{"title":"Image Decoding","description":"알고 있다면 UX를 개선할 수 있는 이미지 디코딩에 대해 설명합니다","cover":"../../assets/postImages/20241026/cover.webp","categories":["Web API"],"tags":["image","ui optimzation","performance"],"publishedAt":"2024-10-26T00:33+09:00","readingTime":399600,"summary":"페이지에 고해상도 이미지를 출력할 때 img요소는 본문에 삽입되었지만, 이미지가 바로 보이지 않고 잠시 후에 출력되는 현상을 본 적 있을 것이다.이는 이미지 다운로드가 완료되었지만, 아직 디코딩이 되기 전이라 페이지에 나타나지 않는 현상이다.요소 추가와 동시에 이미지가","coverColors":["#cbd2e9","#8f82af","#568578","#ae6474","#262a2c"]},"pathname":"/blog/image-decoding/"},{"content":"서론\n\nXMLHttpRequest기반 RIA의 개발부터. Selective Hydration을 구현하고 있는 웹 기술의 변천사를 하나하나 돌이켜 보다\n보면. 어떻게 이런 생각까지 할 수 있을까라는 생각으로 자연스럽게 흘러 가게 된다.\n\n웹 기술의 변화를 이끄는 조직은 수도 없겠지만 가장 눈에 띄는 곳은 단연 facebook, vercel, shopify일 것이다. 이 곳의\n협업을 통해 개선되는 react, nextjs 의 신기능 시연은 여가 시간에 한번이라도 더 컴퓨터 앞에 앉게 만드는 자극제가 되고 있다.\n\n소개된 기술 중 Streaming Server Rendering with Suspense\n[https://youtu.be/8dUpL8SCO1w?t=4174] 와 같은 기능들은 이 글을 쓰고 있는 2023년에도 아직 실험적 기능이라는\n딱지를 달고 있고 완전하게 동작하지는 않고 있지만. (nextjs 13.2 에는 라우팅 단위로 비슷하게 구현될 수 있도록 업데이트 되었다)\n\n사실 Hydration 과정을 스트리밍하는 개념 자체는 react의 발표 전에도 존재했으며 오늘 글에서 소개할 Astro는 그런 개념 중\nIsland Architecture\n[https://patterns-dev-kr.github.io/rendering-patterns/the-island-architecture/]를\n프레임워크 레벨에서 구현하고 있다.\n\n용어가 생소할 수 있는데 내용을 보다 보면 익숙한 느낌이 들 것이다. 기본적인 개념 설명과 Gatsby로 개발했던 블로그를 Astro로 옮기며\n생긴 변화들에 대해서 정리한다.\n\n\nISLAND ARCHITECTURE\n\npreactjs의 개발자 Jason Miller가 쓴 Island Architecture에 대한 소개\n[https://jasonformat.com/islands-architecture/]를 보면 Etsy의 프론트엔드 엔지니어 Katie\nSylor-Miller [https://twitter.com/ksylor] 와의 미팅 중에 관련된 개념에 대한 이야기를 처음 나눴다고 한다.\n\n출처: Islands Architecture: Jason Miller\n[/_astro/island-architecture.DV2hn7vO_Z1PLSdv.webp]출처: Islands Architecture:\nJason Miller\n\nIsland Architecture의 요점은 페이지 내에 동적인 영역에 대한 html은 서버가 렌더링하여 내려주고. 인터렉션을 위한 JS를\n후속하여 내려받도록 해 점진적으로 페이지의 기능을 이용할 수 있도록 하는 것이다.\n\nSSR이 SEO를 고려해 자주 구현되곤 하지만, SSR은 인터렉션에 관련된 JS가 다운로드되고 실행되기 전 까지 페이지가 동작하지 않는 부정적인\n경험을 사용자에게 줄 수 있다. 일반적으로 개발되는 JS를 처리하기 위한 리소스 소모가 생각보다 많다고 한다.\n\n또한. 서버에서 렌더링해 내려주는 html에는 뉴스의 경우 본문이, 상품 페이지의 경우 상품 설명 등. 사용자 입장에서 필수적인 내용이 꼭\n포함되어 있어야 한다는 내용을 포함하고 있다.\n\nreact의 스트리밍 서버 렌더링 소개 영상이 2021년 발표되었으니. 이 개념은 알려진 것 보다 프론트엔드 엔지니어들 사이에서 훨씬 전에\n논의되고 구현되어 왔던 것이다.\n\n따라서 Island Architecture는 어느 프레임웍 혹은 라이브러리를 지칭하는 용어가 아니라 디자인 패턴일 뿐이다. 해당 디자인 패턴을\n구현하는 프레임웍이나 라이브러리는 많다. 그런데 Astro가 State Of JS 2022 기준으로는 가장 높은 Ratio를 기록하고 있다.\n\nState Of JS 2022 Rendering Frameworks\n[/_astro/ratios_over_time.B6TXcGVP_1SY1lA.webp]State Of JS 2022 Rendering\nFrameworks\n\n\nASTRO의 COMPONENT ISLAND\n\nAstro는 기본적으로 모든 웹을 zero client-side JS으로 생성한다. Astro는 React, Preact, Svelte,\nVue, SolidJS, AlpineJS, Lit 등으로 만든 UI컴포넌트들도 HTML을 만들긴 하지만 마찬가지로 JS를 걷어낸 상태로\n렌더링한다.\n\n렌더링 된 HTML에는 placeholder혹은 slot역할을 하는 <astro-island /> 요소가 자리잡게 된다. 아래는 현재 블로그에서\npreact로 구현된 Table Of Content (목차) 컴포넌트의 SSR결과 스크린샷이다. 보면 알겠지만 주요 컨텐츠를 포함하고 있다.\n\npreact로 개발된 TOC컴포넌트의 SSR 결과 [/_astro/astro-island.BD2bmuZr_1sLmtU.webp]preact로\n개발된 TOC컴포넌트의 SSR 결과\n\nAstro는 각 island에 대해 어느 순간에 어떻게 로드될 것인지에 대한 지시자를 사용할 수 있다. 이 블로그의 경우 뷰포트의 크기가\n1388px 이상일 때에만 우측에 TOC를 노출하고 있는데 따라서 아래와 같이 지시자를 추가하였다.\n\n<Layout>\n  <div class=\"wrapper\">\n    <div class=\"content\">{/* 본문 */}</div>\n    <TOC headings={headings} client:visible />\n  </div>\n</Layout>\n\n@media (max-width: 1388px) {\n  .toc {\n    display: none;\n  }\n}\n\n미디어쿼리를 통해 뷰포트가 작으면 <TOC />가 노출되지 않고. 조건에 해당될 경우 client:visible 지시자를 통해 화면에 노출될\n때에만 컴포넌트에 필요한 JS를 동적으로 로드하여 Hydration하게 된다.\n\n동적으로 로드된 JS들. react 런타임마저도 동적으로 로드되고 있다.\n[/_astro/network.CLUbBT6s_1h184y.webp]동적으로 로드된 JS들. react 런타임마저도 동적으로 로드되고 있다.\n\n이런 클라이언트 지시자\n[https://docs.astro.build/en/reference/directives-reference/#client-directives]들은\n위에서 소개한 뷰포트 노출 여부 뿐만 아니라 브라우저 활성화 상태, 미디어 쿼리 등 다양하게 지원하고 있으므로 적절하게 잘 사용하는 것 만으로도\n브라우저의 FCP를 위해 열심히 일하고 있는 메인 스레드를 방해하지 않을 수 있다.\n\n필요한 곳에 직접 구현해 적용해야 했던 과거와 달리 엄청 간편해진 셈이다.\n\n\nZERO CLIENT-SIDE JS\n\n어떤 기술을 사용하던 결국 모던 웹 앱 개발에서 TTI(인터렉티브가 가능해지기까지의 시간)를 개선하는 것은 JS를 얼만큼 줄이느냐의 싸움으로\n귀결된다. Netflix가 서비스를 Vanilla JS로 전환하고 TTI가 50% 감소했다\n[https://medium.com/dev-channel/a-netflix-web-performance-case-study-c0bcde26a9d9]는\n사례를 보면 JS가 차지하는 비중이 작지 않다는 것을 알 수 있다.\n\n아무런 내용 없이 create-react-app과 react-router-dom만을 사용한 앱의 번들 크기는 gzipped기준 약 70kB\n정도가 된다. history를 응용해 인앱 라우팅을 구현하는 코드만으로도 번들 크기가 꽤 늘어나는 것을 볼 수 있다.\n\nastro로 옮긴 후 측정한 결과. 이전에도 좋았지만 훨씬 좋아졌다.\n[/_astro/blog-lighthouse.N4Mr4sXd_ZpoXWb.webp]astro로 옮긴 후 측정한 결과. 이전에도 좋았지만 훨씬\n좋아졌다.\n\n이는 Astro로 개발한 이 블로그의 홈에서 이미지와 GA를 제외한 초기 사이즈 (약 30KB)대비 57.14%나 안 좋은 수치이다. (이\n블로그 뿐만 아니라 Astro공식 홈페이지의 Showcase에 있는 사이트들은 모두 매우 작은 크기를 자랑하고 있다)\n\nAstro가 용량이 작을 수 있는 건. 초기 설계 단에서 신경을 쓴것도 영향이 있겠지만 단지.. 이런 코드들이 없기 때문이다. Astro\nRouting API문서\n[https://docs.astro.build/en/core-concepts/routing/#navigating-between-pages]를\n확인해 보면 알겠지만 기본적으로 SPA를 위한 js가 포함되어 있지 않다. 페이지 이동은 <a href=\"..\" />를 사용한 전통적인\n리다이렉트에 의존하고 있다.\n\n또 Astro에서는 위에서 소개한 서드파티 컴포넌트 라이브러리 없이 Astro로 컴포넌트를 만들 경우에 대해서는. DOM에 이벤트를 붙이기 위해\ndocument.querySelector와 addEventListener를 사용해야 한다.\n\n그럼 이벤트 제거는 어떻게 하나? 안해도 된다. 왜냐면 <a href=\"..\" />로 페이지 자체가 이동되어 버리기 때문이다. (물론 이런\n경우가 필요하면 위에서 언급한 react, preact, solidjs 등을 사용하면 된다)\n\n페이지 로드 후 실행되는 코드들에 대해 트랜스파일은 해 주지만 별도 추가 코드가 포함되어 있지는 않다. 말 그대로 Zero Client-Side\nJS에 가까운 셈이다.\n\n\nASTRO를 도입해야 하나 말아야 하나?\n\ngatsby 공식 스타터 중 gatsby-starter-blog\n[https://www.gatsbyjs.com/starters/gatsbyjs/gatsby-starter-blog] 데모는 내용이 별로 많지\n않음에도 웹 폰트 제외 초기 JS의 크기만 100kB이다.\n\n이 블로그도 이전에 gatsby로 개발했었는데. 블로그 본질적인 기능만을 남긴다고 했을 때 불필요한 JS가 너무 많았다. 스타터 템플릿이 그런데\n예전 gatsby버전은 더 했을 것이다.\n\n글을 보여주는 기능 외 다른 리소스들은 아무리 사용자 경험을 개선한다는 목적이라고 해도 꼼꼼히 따져본다면 결국 불필요한 리소스가 되고 만다.\n이는 앱을 기획할 때 부터 앱의 성격에 따라 전략을 달리하는것이 올바르다 할 수 있다.\n\n현재까지 Astro를 포함 Island Architecture를 구현한 프로젝트들은 모두 적은 TTI를 강점으로 내새우고 있고 이는 정적 컨텐츠\n위주의 사이트에서 극대화된다. JS를 최소화하기 용이하도록 만들어졌기 때문이다.\n\n반면에 대시보드 앱과 같이 인터렉션이 필요한 UI가 많은 상황에서는 페이지 로드와 동시에 모든 JS를 받게 될 확률이 높을 것이다. 이렇게 되면\nAstro의 모든 장점을 극대화하기가 어려워진다. 이럴 땐 핸들러 단위로 JS를 쪼개주는 qwik\n[https://qwik.builder.io/]을 고려해보는 것이 좋겠다.\n\n\n결론\n\n아래는 위의 내용을 바탕으로 정리해본 생각들과 장점 및 단점이다\n\n장점\n\n * 정적 컨텐츠 위주의 사이트에서 JS를 최소하하여 TTI를 크게 개선할 수 있다 (페이지 로드 속도가 드라마틱하게 빨라진다)\n * react, preact, solidjs, lit 등 기존에 만들어진 컴포넌트를 그대로 쓸 수 있다\n * Island 별로 지시자를 통해 Hydration을 적절히 컨트롤할 수 있다\n\n단점\n\n * 동적 컨텐츠 위주의 사이트에서는 초기에 모든 JS를 로드해야 할 가능성이 높아 장점이 퇴색된다\n * 커뮤니티가 아직 많이 활성화되지 않았다\n\n티저 사이트, 블로그, 포트폴리오 사이트에는 무조건 쓰는 것이 좋겠다.\n\n> 참고\n> \n>  * Islands Architecture [https://jasonformat.com/islands-architecture/]\n>  * The Island Architecture\n>    [https://patterns-dev-kr.github.io/rendering-patterns/the-island-architecture/]","frontmatter":{"title":"웹 성능 관점에서 본 Astro","description":"블로그를 Astro로 옮기며 느꼈던 장점들을 웹 성능 관점에서 정리해 본다","cover":"../../assets/postImages/20230319/cover.png","coverAlt":"Astro의 성능 측정 결과 (출처: 공식 사이트)","categories":["Framework"],"tags":["astro","performance","island architecture"],"publishedAt":"2023-03-19T20:00+09:00","readingTime":759300,"summary":"XMLHttpRequest기반 RIA의 개발부터. Selective Hydration을 구현하고 있는 웹 기술의 변천사를 하나하나 돌이켜 보다 보면. 어떻게 이런 생각까지 할 수 있을까라는 생각으로 자연스럽게 흘러 가게 된다.웹 기술의 변화를 이끄는 조직은 수도 없겠지","coverColors":["#cccad2","#62b7be","#456eaf","#5f5f6a","#0e121b"]},"pathname":"/blog/gatsby-to-astro/"},{"content":"서론\n\n현재 Web Push는 무료[1]이며 아래 조건들만 만족하면 이메일보다는 훨씬 나은것은 물론이고. SMS, 카카오톡 알림급의 전달력을 가진\n메시지를 무제한으로 보낼 수 있다.\n\n * 브라우저가 Web Push를 지원할 것\n * 사용자가 웹 알림 권한을 허용할 것\n * 웹 워커를 등록한 브라우저 프로세스가 실행되어 있을 것\n\nWeb Push를 구현하려면 원래 firebase 패키지를 설치하고. 사용자가 웹 알림 권한을 허용할 때 Firebase Cloud\nMessaging (이하 FCP) 플랫폼에 엔드포인트를 등록하는 코드를 직접 작성해야 한다.\n\n하지만 Angular는 Google에서 개발하는 것이라 그런지 @angular/service-worker 패키지에 이미 해당 기능이 구현되어\n있어 쉽게 Web Push를 구현할 수 있다.\n\nAngular의 @angular/service-worker패키지는 해당 기능을 포함해 App Shell, Runtime Caching,\nSmart Updates기능을 쉽게 구현할 수 있도록 기능을 제공하고 있다. 17년 12월 발표한 Service Worker 소개자료\n[https://javascript-conference.com/wp-content/uploads/2017/12/Automatic_Progressive_Web_Apps_using_Angular_Service_Worker_Maxim_Salnikov.pdf]에\n따르면 이를 Automatic Progressive Web Apps 로 소개하고 있다.\n\n\nWEB PUSH의 시스템 구조\n\nWeb Push 데이터 흐름 [/_astro/webpush.D_BqVeDG_ZQADYV.webp]Web Push 데이터 흐름\n 1. 사용자가 웹 서비스에 접속하면 웹 워커를 설치하게 된다\n 2. 알림 버튼을 클릭하면 SwPush.requestSubscription메서드를 호출하여 FCP에 등록하고\n    PushSubscription객체를 응답받는다\n 3. PushSubscription객체를 서버에 전송하여 어딘가에 저장해둔다\n 4. 서버에서 메시지를 PushSubscription을 참고하여 전송한다\n 5. FCP에서 PushSubscription 객체에 해당하는 웹 워커에게 메시지를 전달한다. 해당 웹 워커는 이를 읽어 알림을 출력한다\n\nSwPush [https://angular.io/api/service-worker/SwPush]는 Angular에서 제공하는 푸시 알림 등록\n구현체이다. 이 서비스를 통해 알림 구독 자체를 나타내는 PushSubscription\n[https://developer.mozilla.org/en-US/docs/Web/API/PushSubscription]객체를 만들 수 있다.\n이 객체는 json으로 변환 가능하므로 서버에 보내 저장했다가 원할 때 메시지를 보내도록 하는것이다.\n\n\nWEB PUSH 구현하기\n\n\nWEB-PUSH 설치 밎 키 생성하기\n\nnpm install --save web-push\n\nnpx web-push generate-vapid-keys\n\n=======================================\n\nPublic Key:\n{VAPID 공개키}\n\nPrivate Key:\n{VAPID 비밀키}\n\n=======================================\n\n\n위 명령 수행시 나오는 공개키와 비밀키는 해당 웹 서버기준으로 발급되는 것이므로 잘 기억해둔다. 서비스 도중에 이것이 바뀌면 기존 구독들에게\n메시지를 보낼 수 없게 된다.\n\n공개키는 environment.ts에 추가하고. .dotenv파일에는 둘 다 저장하여 백엔드에서 읽어다 쓸 수 있도록 하면 된다.\n\n// This file can be replaced during build by using the `fileReplacements` array.\n// `ng build` replaces `environment.ts` with `environment.prod.ts`.\n// The list of file replacements can be found in `angular.json`.\n\nexport const environment = {\n  production: false,\n  VAPIDPublicKey: '{VAPID 공개키}',\n}\n\n/*\n * For easier debugging in development mode, you can import the following file\n * to ignore zone related error stack frames such as `zone.run`, `zoneDelegate.invokeTask`.\n *\n * This import should be commented out in production mode because it will have a negative impact\n * on performance if an error is thrown.\n */\n// import 'zone.js/plugins/zone-error';  // Included with Angular CLI.\n\n# .dotenv\nVAPID_PUBLIC_KEY={VAPID 공개키}\nVAPID_PRIVATE_KEY={VAPID 비밀키}\n\n\nSERVICE WORKER 패키지 추가\n\nng add @angular/pwa --project <project-name>\n\n위 명령어를 사용하여 앱에 기본 서비스워커를 추가한다. 프로젝트에 서비스워커 추가하기\n[https://angular.io/guide/service-worker-getting-started#adding-a-service-worker-to-your-project]에\n해당 명령의 변경사항이 나와 있는데 직접 수정해도 된다.\n\n이제 앱을 빌드하면 결과물에 ngsw-worker.js가 추가되고. 이 파일이 AppModule의 ServiceWorkerModule\nimport에 추가되어 있다. 이 파일이 위 소개자료에 있는 기능들의 Angular 구현체이다.\n\n@NgModule({\n  declarations: [AppComponent, NotiComponent],\n  imports: [ServiceWorkerModule.register('ngsw-worker.js')],\n  providers: [WindowService],\n  bootstrap: [AppComponent],\n})\nexport class AppModule {}\n\n\n알림 버튼 만들기\n\n먼저 브라우저가 Web Push를 지원하는지 확인해야 한다. ua-parser-js와 compare-versions패키지를 활용하여 구분하고\n가능한 경우 알림 버튼을 노출한다.\n\n> ‘22 7/27 기준 web-push는 safari를 지원하지 않는다 web-push 브라우저 지원 범위\n> [https://github.com/web-push-libs/web-push#browser-support] 참고\n\nimport { compare } from 'compare-versions'\nimport UAParser from 'ua-parser-js'\n\n@Injectable()\nexport class WindowService {\n  isSupportNotification = false\n\n  constructor() {\n    const {\n      browser: { name, version },\n    } = new UAParser().getResult()\n\n    // 공식 문서 기준으로 필터링\n    this.isSupportNotification =\n      'Notification' in window &&\n      !!version &&\n      (name === 'Chrome' ||\n        compare(version, '52', '>=') ||\n        name === 'Edge' ||\n        compare(version, '17', '>=') ||\n        name === 'Firefox' ||\n        compare(version, '46', '>='))\n  }\n}\n\n다음 컴포넌트에서 서비스를 DI받아 버튼 노출을 제어한다. 클릭 시 위에 설명한대로 SwPush.requestSubscription메서드를\n호출하여 PushSubscription객체를 받고. 서버에 전송하여 저장하도록 한다.\n\nimport { SwPush } from '@angular/service-worker'\nimport { environment } from '../environments/environment'\n\n@Component({\n  template: `\n    <button\n      type=\"button\"\n      (click)=\"onClick()\"\n      *ngIf=\"win.isSupportNotification; else notSupport\"\n    >\n      알림 구독하기\n    </button>\n    <ng-template #notSupport> 알림을 지원하지 않는 브라우저입니다 </ng-template>\n  `,\n})\nexport class NotificationComponent {\n  constructor(\n    private swPush: SwPush,\n    private http: HttpClient,\n    protected win: WindowService\n  ) {}\n\n  onClick() {\n    this.swPush\n      .requestSubscription({ serverPublicKey: environment.VAPIDPublicKey })\n      .then((pushSubscription) => {\n        this.http.post('/api/subscribe', pushSubscription).subscribe()\n      })\n  }\n}\n\n\n백엔드 WEBPUSH 설정\n\n위에서 생성한 키를 읽어다 서버 실행과 동시에 web-push모듈을 초기화한다. 예제에서는 express 웹 서버를 사용했다.\n\n> web-push 모듈 사용시 메서드를 찾지 못한다는 류의 에러가 발생할 경우 예제 코드처럼 defaultExport를 사용하도록 한다.\n\nimport webpush from 'web-push'\n\n// .dotenv 읽기\nrequire('dotenv').config()\n\n// web-push 모듈 초기화\nwebpush.setVapidDetails(\n  'mailto: {관리용 이메일 주소 기입}',\n  process.env.VAPID_PUBLIC_KEY,\n  process.env.VAPID_PRIVATE_KEY\n)\n\n그 다음 클라이언트에서 전송한 PushSubscription 객체를 받아 저장하는 라우터를 작성한다.\n\nimport { Router, json, urlencoded } from 'express'\n\nconst router = Router()\n\nrouter.use(json({ limit: '5mb' }))\nrouter.use(urlencoded({ limit: '5mb', extended: true }))\n\nlet pushSubscription = null\n\nrouter.get('/api/subscribe', (req, res) => {\n  pushSubscription = req.body\n  res.send('ok').end()\n})\n\n알림을 보내야 할 땐 아래와 같이 저장했던 pushSubscription을 sendNotification\n[https://github.com/web-push-libs/web-push#sendnotificationpushsubscription-payload-options]의\n인자로 보내면 된다. 두 번째 인자에 메시지를 문자열이나 버퍼로 만들어 넘기면 되는데.\nServiceWorkerRegistration.showNotification\n[https://developer.mozilla.org/en-US/docs/Web/API/ServiceWorkerRegistration/showNotification]메서드로\nOS알림을 출력할 것이므로 json으로 만들어 넣는게 일반적이다.\n\nimport webpush from 'web-push'\n\nwebpush\n  .sendNotification(\n    pushSubscription,\n    JSON.stringify({ title: 'hello', body: 'world' })\n  )\n  .then((res) => {\n    console.log(res)\n  })\n  .catch((err) => {\n    console.error(err)\n  })\n\n\n웹 워커에서 OS알림 띄우기\n\n이제 마지막으로 웹 워커에서 FCP에서 전송한 알림을 OS에 띄워주면 된다. @angular/pwa설치 후 번들에 만들어지는\nngsw-worker.js에는 이 코드가 없으므로. 이 부분을 직접 구현하여 사용해야 한다.\n\n주석에 명시된대로 해당 js파일은 window가 아니라 WorkerGlobalScope\n[https://developer.mozilla.org/en-US/docs/Web/API/WorkerGlobalScope]에서 실행되기 때문에\n코드 작성시 주의해야 한다.\n\n/**\n * WorkerGlobalScope에서 동작하는 코드임. 컨텍스트에 대한 정보는 아래 문서 참고할 것\n * https://developer.mozilla.org/en-US/docs/Web/API/WorkerGlobalScope\n *\n * ngsw-worker.js 는 angular.json의 빌더 설정에 \"serviceWorker\" \"ngswConfigPath\"\n * 값이 설정되어 있으면 빌드 완료 후 자동으로 생성되는 Angular의 서비스 워커 구현 스크립트임\n *\n * 웹 푸시만 필요하다면 없어도 되지만 Automatic Service Worker의 기능 활용을 위해 포함한다\n */\nimportScripts('ngsw-worker.js')\n\n/**\n * Angular의 SwPush 서비스를 이용해 생성한 구독은 아래 인스턴스의 형대로 저장됨\n * https://developer.mozilla.org/en-US/docs/Web/API/PushSubscription\n *\n *\n * 서버에서 각 이벤트 발생 시 `web-push`라이브러리를 사용해 위에서 만든 구독 정보로\n * 데이터를 보내는데. 해당 구독 정보와 매칭되는 웹 워커에서 아래 이벤트가 발생함\n * https://developer.mozilla.org/en-US/docs/Web/API/PushEvent\n */\nself.addEventListener('push', (e) => {\n  const { title, body, ...data } = e.data.json()\n\n  if (!title || !body) {\n    return\n  }\n\n  self.registration.showNotification(title, { body, data })\n})\n\n/**\n * OS에 뜬 알림을 클릭했을 때 하는 동작. 예제는 url이 있다면 열어주는 코드이다\n */\nself.addEventListener('notificationclick', (e) => {\n  const url = e.notification?.data?.url\n\n  if (!url) {\n    return\n  }\n\n  self.clients.openWindow(url)\n})\n\n위 파일을 my-worker.js 로 만들어 src폴더 밑에 두고. angular.json과 app.module.ts를 수정한다\n\n{\n  \"targets\": {\n    \"build\": {\n      \"options\": {\n        \"assets\": [\n          \"src/my-worker.js\" // 빌드에 포함될 수 있도록 추가\n        ]\n      }\n    }\n  }\n}\n\n@NgModule({\n  declarations: [AppComponent, NotiComponent],\n  imports: [\n    ServiceWorkerModule.register('my-worker.js'), // 수정\n  ],\n  providers: [WindowService],\n  bootstrap: [AppComponent],\n})\nexport class AppModule {}\n\nWeb Push 구현 결과 [/_astro/webpush-demo.WegUnp9Z_ZQfst9.webp]Web Push 구현 결과\n\n위처럼 http 에 localhost 에서도 잘 동작하는것을 확인할 수 있다. cu 로고는 OS알림을 띄울 때 추가로 넣을 수 있으니\nshowNotification\n[https://developer.mozilla.org/en-US/docs/Web/API/ServiceWorkerRegistration/showNotification]\nAPI를 참고하면 된다.\n\n글을 작성하며 테스트해본 코드를 Angular 14 웹 푸시 테스트 리포지토리\n[https://github.com/johnny-mh/ng14-web-push-example]에 푸시해 두었다. 서버 실행 방법을\nREADME.md 에 적어두었으니 클론받아서 직접 테스트해볼 수 있다. 본문과 다르게 백엔드를 SSR로 구현해놓았으므로 참고 바란다.\n\n\n주석\n\n> 1. https://firebase.google.com/docs/cloud-messaging\n> [https://firebase.google.com/docs/cloud-messaging]","frontmatter":{"title":"Angular앱에 Web Push 구현하기","description":"Angular의 Service Worker에 대한 짧은 소개와 Web Push를 구현하는 방법에 대해 설명합니다","cover":"../../assets/postImages/20220727/cover.jpg","categories":["Web API"],"tags":["angular","web push","push notification"],"publishedAt":"2022-07-27T20:00+09:00","readingTime":645300,"summary":"현재 Web Push는 무료[1]이며 아래 조건들만 만족하면 이메일보다는 훨씬 나은것은 물론이고. SMS, 카카오톡 알림급의 전달력을 가진 메시지를 무제한으로 보낼 수 있다.브라우저가 Web Push를 지원할 것사용자가 웹 알림 권한을 허용할 것웹 워커를 등록한 브라우","coverColors":["#6ba47a","#798fc8","#a5778f","#5d628e","#2c1939"]},"pathname":"/blog/angular앱에-web-push-구현하기/"},{"content":"GATSBY에 검색을 추가하는 방법\n\n블로그에 글이 늘어나면 카테고리와 태그만으로는 원하는 내용을 찾기 어려워진다. 지금 블로그가 그 정도까지 내용이 많지는 않지만 학습을 위해\n운영하는것도 있어서 검색 기능을 구현해 보았다.\n\n구현 완료된 검색 화면 (메뉴바의 검색 버튼을 눌러 사용해볼 수 있다)\n[/_astro/gatsby-search.DTDkpCdy_Z1uCTJJ.webp]구현 완료된 검색 화면 (메뉴바의 검색 버튼을 눌러 사용해볼 수\n있다)\n\n공식 사이트의 gatsby 에 검색 기능 추가\n[https://www.gatsbyjs.com/docs/how-to/adding-common-features/adding-search/] 문서를\n보면 검색에 필요한 기본 요소들에 대한 설명과 함께 gatsby에 검색을 구현하는 2가지 방법을 소개하고 있다.\n\n첫 번째 방법은 클라이언트 측 검색이다. 빌드 혹은 런타임에 데이터를 인덱싱하고 이를 이용해 로컬에서 검색을 수행하는 방법이다. 위의 공식\n문서에서는 js-search, gatsby-plugin-elasticlunr-search, gatsby-plugin-local-search 를\n활용하라고 안내하고 있다.\n\n두 번째 방법은 API기반 검색엔진을 활용하는 방법으로 Algolia와 같은 외부 서비스를 활용한다. 빌드 시점에 검색 대상 데이터들을 인덱싱해\n외부 서비스에 업로드해 두고 런타임에는 API로 검색한다.\n\nAPI기반 검색엔진사용 시 블로그의 빌드 배포 프로세스에 인덱스를 전송해야 하는 번거로움과. 사용 시 비용이 발생하거나. 무료인 경우 횟수에\n제약이 있어 사용하고 싶지 않았다. 결국 클라이언트 측 검색 방법으로 검색을 구현했다. 이 글에서는 구현 과정에 대해서 다룬다.\n\n\n검색 라이브러리와 한글 이슈\n\n사실 검색은 단순히 글 목록을 순회하며 텍스트가 포함되어있는지를 검사해도 된다. 하지만 실제 사용시에는 포함 뿐만 아니라 상황에 따라 아래의\n기능들을 필요로 하는데 이 경우 추가 구현이 필수이므로. 대개 별도의 텍스트 검색 라이브러리를 사용하게 된다.\n\n * 배열의 모든 요소를 순회하지 않고도 빠르게 검색할 수 있는 기능 (인덱스 생성 및 사용)\n * 제목보다 본문에 조금 더 가중치를 두고 검색 (필드 가중치 적용)\n * 검색어 하이라이팅을 위한 매칭 텍스트 위치 배열\n * and, or 논리 연산 검색 등\n\ngatsby 공식 사이트의 gatsby 에 검색 기능 추가\n[https://www.gatsbyjs.com/docs/how-to/adding-common-features/adding-search/]\n문서에는 js-search, flexsearch, lunr 세가지 라이브러리를 추천하고 있는데. 설명 자체가 장황하거나 flexsearch,\nlunr는 둘 다 한글 검색 관련 문제[1]가 있어 사용이 어려웠다.\n\n한참을 디버깅하다 결국 실무에서 특별한 문제없이 잘 사용하고 있는 fuse.js를 활용해보기로 했다.\n\n\nFUSE.JS의 사용\n\nfuse.js [https://fusejs.io/]는 텍스트 검색 라이브러리로 위에 적힌 모든 기능을 제공하며. 브라우저 / NodeJS환경\n둘다에서 사용할 수 있으므로 gatsby의 빌드 과정에서는 인덱스를 생성해 두고. 런타임에 사용하여 검색하도록 구성하면 된다.\n\nfuse.js에 대한 자세한 내용은 공식 사이트에서 확인할 수 있으니 생략하고. 본문에서는 이 글의 핵심인 인덱스의 생성 및 사용 방법에\n대해서만 다룬다.\n\n\n인덱스 생성 및 사용\n\nimport Fuse from 'fuse.js'\n\nconst books = [\n  {\n    title: \"Old Man' War\",\n    author: { firstName: 'John', lastName: 'Scalzi' },\n  },\n  {\n    title: 'The Lock Artist',\n    author: { firstName: 'Steve', lastName: 'Hamilton' },\n  },\n  // {...}, {...}, ...\n]\n\n// index 생성\nconst index = Fuse.createIndex(['title', 'author.firstName'], books)\n\n// 예시를 위해 로컬스토리지에 저장한다\nlocalStorage.setItem('index', JSON.stringify(index.toJSON()))\n\ncreateIndex메서드 호출로 만들어진 index객체는 fusejs가 books를 검색하는데 있어 순회를 줄일 수 있는 정보를 담고 있는\n인스턴스이다. 이 인스턴스는 toJSON()메서드를 통해 json문자열로 변환도 가능하여. 스토리지에 담거나 API로 보내거나 하는 등 유용하게\n쓸 수 있다\n\nimport Fuse from 'fuse.js'\n\nconst index = Fuse.parseIndex(JSON.parse(localStorage.getItem('index')))\n\n// 원본 데이터와 인덱스를 가지고 최적화 된 검색을 수행할 수 있는 인스턴스 생성\nconst fuse = new Fuse(books, undefined, index)\n\nconst result = fuse.search('검색어')\n\n이렇게 만들어진 인덱스 인스턴스와 원본 데이터를 가지고 텍스트 검색을 수행할 수 있다.\n\n\nGATSBY에서 FUSE.JS 활용하기\n\n앞서 언급했던 대로 빌드 과정에서는 목록을 인덱싱하여 어딘가 저장해두어야 하고 이렇게 생성된 데이터는 블로그의 런타임에 fuse.js 인스턴스를\n만들어 사용하도록 구성해야 한다.\n\n이때 빌드 과정은 플러그인을 활용하면 되고. 런타임 검색은 훅을 활용하면 된다.\n\n\nGATSBY-PLUGIN-FUSEJS\n\nnpm install gatsby-plugin-fusejs\n\n설치 후 gatsby-config.js에 인덱스가 만들어지기 원하는 데이터의 쿼리, 데이터 중에서도 검색이 되었으면 하는 프로퍼티,\ngraphql결과물을 단순 객체 배열로 변환하기 위한 함수를 옵션으로 전달한다\n\nmodule.tsports = {\n  plugins: [\n    {\n      resolve: `gatsby-plugin-fusejs`,\n      options: {\n        // 인덱스를 만들고자 하는 데이터의 쿼리\n        query: `\n          {\n            allMarkdownRemark {\n              nodes {\n                id\n                rawMarkdownBody\n                frontmatter {\n                  title\n                }\n              }\n            }\n          }\n        `,\n\n        // 인덱스를 만들고자 하는 데이터의 프로퍼티\n        keys: ['title', 'body'],\n\n        // graphql의 결과물을 단순 객체 배열로 변환하는 함수\n        normalizer: ({ data }) =>\n          data.allMarkdownRemark.nodes.map((node) => ({\n            id: node.id,\n            title: node.frontmatter.title,\n            body: node.rawMarkdownBody,\n          })),\n      },\n    },\n  ],\n}\n\n옵션으로 만들어진 인덱스를 아래 스크린샷과 같이 활용할 수 있게 되었다\n[/_astro/gatsby-plugin-fusejs.CLCx_raF_1QrTIO.webp]옵션으로 만들어진 인덱스를 아래 스크린샷과 같이\n활용할 수 있게 되었다\n\n\nREACT-USE-FUSEJS\n\nnpm install react-use-fusejs\n\n다음은 만들어진 인덱스를 활용하기 위해 fuse.js 훅을 설치한다. 예제에서는 Search라는 컴포넌트를 만들고. useStaticQuery로\n컴포넌트 단위 쿼리로 인덱스와 원본 데이터를 불러온 후 검색어 입력시마다 결과를 화면에 노출하고 있다.\n\nimport { graphql, useStaticQuery } from 'gatsby'\nimport * as React from 'react'\nimport { useGatsbyPluginFusejs } from 'react-use-fusejs'\n\nexport function Search() {\n  const data = useStaticQuery(graphql`\n    {\n      fusejs {\n        index\n        data\n      }\n    }\n  `)\n\n  const [query, setQuery] = React.useState('')\n\n  // fusejs 객체를 가공 없이 그대로 넘긴다\n  const result = useGatsbyPluginFusejs(query, data.fusejs)\n\n  return (\n    <div>\n      <input\n        type=\"text\"\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n      />\n      <ul>\n        {result.map(({ item }) => (\n          <li key={item.id}>{item.title}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n\nexport default Search\n\n위 예제의 경우 검색어 입력시 검색결과가 즉시 화면에 노출되는데. 이 것을 조절하려면 query의 변화에 throttle이나 debounce를\n활용하면 해결할 수 있을 것이다.\n\n\n인덱스를 LAZY LOADING하기\n\nSearch컴포넌트는 렌더링 즉시 인덱스를 파싱하여 인스턴스를 만들기 때문에 검색을 하지 않더라도 자원을 소모하게 된다. 문서량이 적다면\n괜찮겠지만 많아지는 경우 신경이 쓰일 수 있는데. 그 경우를 위해 Lazy Loading을 적용할 수 있다.\n\n아래 코드는 실제 검색 키워드가 입력될 때 즉시 인덱스를 다운로드 받고 파싱하여 검색을 수행하는 예제이다.\n\nimport { graphql, useStaticQuery } from 'gatsby'\nimport * as React from 'react'\nimport { useGatsbyPluginFusejs } from 'react-use-fusejs'\n\nexport function Search() {\n  const data = useStaticQuery(graphql`\n    {\n      fusejs {\n        publicUrl\n      }\n    }\n  `)\n\n  const [query, setQuery] = React.useState('')\n  const [fusejs, setFusejs] = React.useState(null)\n  const result = useGatsbyPluginFusejs(query, fusejs)\n\n  const fetching = React.useRef(false)\n\n  React.useEffect(() => {\n    if (!fetching.current && !fusejs && query) {\n      fetching.current = true\n\n      fetch(data.fusejs.publicUrl)\n        .then((res) => res.json())\n        .then((json) => setFusejs(json))\n    }\n  }, [fusejs, query])\n\n  return (\n    <div>\n      <input\n        type=\"text\"\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n      />\n      <ul>\n        {result.map(({ item }) => (\n          <li key={item.id}>{item.title}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n\nexport default Search\n\n\n인덱스 재사용하기\n\n위의 예제에서 다운로드 받아 만든 fuse.js 데이터는 컴포넌트가 제거되면 함께 사라진다. 따라서 데이터를 컨텍스트에 담아 재사용하도록 한다.\n\n// src/context/app.jsx\nimport { createContext, useState } from 'react'\n\nexport const AppContext = createContext({\n  fusejs: null,\n  setFusejs: () => {},\n})\n\nexport const AppProvider = ({ children }) => {\n  const [fusejs, setFusejs] = useState(null)\n\n  return (\n    <AppContext.Provider value={{ fusejs, setFusejs }}>\n      {children}\n    </AppContext.Provider>\n  )\n}\n\n// gatsby-browser.js\nimport { AppProvider } from './src/context/app'\n\nexport const wrapRootElement = ({ element }) => {\n  return <AppProvider>{element}</AppProvider>\n}\n\n// src/components/Search.jsx\nimport { AppContext } from '../context/app'\nimport { graphql, useStaticQuery } from 'gatsby'\nimport * as React from 'react'\nimport { useGatsbyPluginFusejs } from 'react-use-fusejs'\n\nexport function Search() {\n  const data = useStaticQuery(graphql`\n    {\n      fusejs {\n        publicUrl\n      }\n    }\n  `)\n\n  const [query, setQuery] = React.useState('')\n  const [fusejs, setFusejs] = React.useContext(AppContext)\n  const result = useGatsbyPluginFusejs(query, fusejs)\n\n  const fetching = React.useRef(false)\n\n  React.useEffect(() => {\n    if (!fetching.current && !fusejs && query) {\n      fetching.current = true\n\n      fetch(data.fusejs.publicUrl)\n        .then((res) => res.json())\n        .then((json) => setFusejs(json))\n    }\n  }, [fusejs, query])\n\n  return (\n    <div>\n      <input\n        type=\"text\"\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n      />\n      <ul>\n        {result.map(({ item }) => (\n          <li key={item.id}>{item.title}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n\nexport default Search\n\n\n> 주석\n> \n> 1. lunr의 한글 검색 이슈는 Jekyll에 lunr.js 붙이기 (+ 한국어 검색 문제 해결)\n> [https://cjeon.com/2016/05/29/Jekyll-lunr.html] 에서 볼 수 있고. flexsearch의 한글 검색\n> 이슈는 gatsby를 이용한 Github blog 개발후기\n> [https://jaeseokim.dev/React/gatsby-blog-%EA%B0%9C%EB%B0%9C-%ED%9B%84%EA%B8%B0/#%EB%91%90-%EB%B2%88%EC%A7%B8-%EA%B3%A0%EB%AF%BC---%EA%B2%80%EC%83%89-%EA%B8%B0%EB%8A%A5]에서\n> 볼 수 있으며. 공식 문서\n> [https://github.com/nextapps-de/flexsearch#cjk-word-break-chinese-japanese-korean]를\n> 따라해도 동작하지 않는다.","frontmatter":{"title":"gatsby 블로그에 검색을 붙여보자","description":"gatsby블로그에 fusejs를 이용하여 로컬 검색을 구현하는 방법에 대해 설명합니다","cover":"../../assets/postImages/20220709/cover.webp","categories":["Framework"],"tags":["gatsby","search","fusejs"],"publishedAt":"2022-07-09T20:00+09:00","readingTime":681600,"summary":"블로그에 글이 늘어나면 카테고리와 태그만으로는 원하는 내용을 찾기 어려워진다. 지금 블로그가 그 정도까지 내용이 많지는 않지만 학습을 위해 운영하는것도 있어서 검색 기능을 구현해 보았다.공식 사이트의 gatsby 에 검색 기능 추가 문서를 보면 검색에 필요한 기본 요소","coverColors":["#d9d7d6","#b38f99","#7a5344","#823037","#394334"]},"pathname":"/blog/gatsby-블로그에-검색을-붙여보자/"},{"content":"BLOOM FILTER\n\nBloom Filter는 어떤 Set안에 특정 값이 존재하는지 여부를 빠르게 계산할 수 있는 알고리즘이다. 속도도 빠르면서 저장 공간을 적게\n차지하기 때문에 대규모 서비스에서 신규회원 여부, 크롬 브라우저의 멀웨어 사이트 여부, hbase, redis 등 여러 군데에 활용되고 있다.\n\n재미있는 점은 이 Bloom Filter 알고리즘은 확률적 자료 구조라는 점이다. 계산의 정확도가 100%가 아니다. False Positive\nProbability(이하 FPP) 즉 거짓을 반환할 확률이 있다. 정확도를 희생하고 메모리 사용량을 얻은 셈이다.\n\n이 FPP는 0%가 될 순 없지만 일반적으로 알고리즘의 각 변수를 제어하여 0% 에 수렴하는 최적의 값을 찾아서 도입한다. 이에 관련한 자세한\n내용은 뒤에서 다루고. 먼저 원리에 대해 간단히 설명한다.\n\n\n동작 원리\n\n이 알고리즘은 요소들이 담길 공간, 필터, 해시 함수가 필요하다. 공간은 앞서 언급한 대로 회원 DB등 특정 값이 존재하는지 검사할 집합이다.\n필터는 0, 1을 표현할 수 있는 Bit의 배열이다. 해시 함수는 특정 값을 넣었을 때 항상 똑같은 길이의 값을 반환하는 함수를 말한다.\n\n설명을 위해 공간은 문자열 배열, 필터는 6칸을 사용하고. 해시는 길이 6의 값을 출력하는 hashA, hashB 를 사용한다고 하자. 그리고\n코드는 이해를 돕기 위해 비트를 나열한 수도 코드를 사용한다.\n\n\n배열에 값 추가하기\n\n먼저 배열에 요소를 추가할 땐 항상 해시함수 2개를 돌린 값을 OR 비트 연산하고 이를 필터와 OR연산하여 필터를 업데이트한다.\n\narr = []\nfilter = 000000\nnewValue = 'owl'\n\nh1 = hashA(newValue) // 010000\nh2 = hashB(newValue) // 001000\n\n(filter |= h1) |= h2 // 011000\narr.push(newValue)\n\n요소 owl을 추가하여 필터 값은 011000이 되었다. 그리고 이어 값을 하나 더 추가한다.\n\nnewValue = 'hawk'\n\nh1 = hashA(newValue) // 100000\nh2 = hashB(newValue) // 000010\n\n(filter |= h1) |= h2 // 111010\narr.push(newValue)\n\n이제 필터의 값은 111010이 되었다.\n\n\n🍳 값 존재 여부 검사하기\n\n이제 처음 추가한 owl이 존재하는지 여부를 검사해 보자.\n\ncheckValue = 'owl'\n\nh1 = hashA(checkValue) // 010000\nh2 = hashB(checkValue) // 001000\n\nh = h1 | h2 // 011000\n\n!!(filter & h) // 011000 => true\n\n해시 함수를 돌린 값 011000 과 현재 필터 값 111010을 AND연산하면 값은 011000으로 Boolean변환 시 true가 된다. 이\n값은 배열에 존재할 확률이 높다. 다음은 존재하지 않는 값에 대한 검사이다.\n\ncheckValue = 'duck'\n\nh1 = hashA(checkValue) // 000100\nh2 = hashB(checkValue) // 000001\n\nh = h1 | h2 // 000101\n\n!!(filter & h) // 000000 => false\n\n해시 함수를 돌린 값 000101과 현재 필터 값 111010을 AND연산하면 값은 000000이므로 Boolean변환 시 false가 된다.\n이 값은 배열에 존재하지 않는다. 👀 여기까지 따라왔다면 중요한 문제인 다음 예제를 보자.\n\ncheckValue = 'crow'\n\nh1 = hashA(checkValue) // 001000\nh2 = hashB(checkValue) // 000010\n\nh = h1 | h2 // 001010\n\n!!(filter & h) // 001010 => true\n\n값 **‘crow’**는 배열에 없지만 해시 값은 001010이므로 연산 결과에서 true가 나왔다. 이것이 바로 Bloom Filter의\nFPP특성. 없는 값을 있는 것으로 계산할 수 있는 특성이다. 이런 특성에도 불구하고 이 알고리즘 사용하는 이유는 메모리 사용량이 적다는 점\n때문이다.\n\n일반적으로 배열에 특정 값이 존재하는지 빠르게 찾기 위해서 값을 키로 갖는 객체를 만들어 사용한다. 해당 객체 안에 키 값이 있다면 배열 내에\n값이 존재하는 것이다. 이 경우 객체외 키의 갯수만큼의 메모리 공간을 더 사용해야 한다. 하지만 예제의 필터에서는 6칸의 배열만 필요했다.\n실무에서도 256길이의 필터면 대 부분 해결되는 수준이다.\n\n> 혹시 위에 언급하는 예제를 데모로 보고 싶다면 Bloom Filters by Example\n> [https://llimllib.github.io/bloomfilter-tutorial/]을 참고하기 바란다.\n\n실제로 서비스에 적용할 땐. 이 FPP를 0%에 근접하도록 각 변수의 값을 계산해 적용한다. 위의 예제에서 바꾼다면 필터의 길이를 256과 같이\n매우 길게 하고. 해시 함수도 20개 이상으로 많이 사용하면 정확도가 올라갈 것이다. 계산식도 나와 있는데 Bloom Filter 사이즈 계산기\n[https://hur.st/bloomfilter]에서 확인해 볼 수 있다.\n\n요소에 들어가는 아이템의 개수 (n), 원하는 FPP값(p), 필터의 비트 개수(m), **해시 함수의 개수(k)**를 부분적으로 입력하면\n입력하지 않은 변수의 최적의 값을 뽑아 준다.\n\nBloom Filter는 Backend뿐만 아니라 Frontend에서도 얼마든지 사용할 수 있다. 해시 함수의 정확도나 비용을 고려했을 때는\n대량의 데이터를 다루긴 어려울 수 있지만. 웹어셈블리를 활용한다면 이 제약도 없다.\n\n\nANGULAR의 BLOOM FILTER\n\n> 로직 설명은 Angular DI: Getting to know the Ivy NodeInjector\n> [https://medium.com/angular-in-depth/angular-di-getting-to-know-the-ivy-nodeinjector-33b815642a8e]\n> 에 상세히 나와 있다. 그런데 글이 조금 어려워서 알기 쉽게 풀어 설명해 본다.\n\n> ⚠ 예제는 Angular v8.3.28 기준이다.\n\nAngular는 v8버전부터 Ivy Renderer가 추가되면서 대대적인 성능 향상 작업이 이루어졌다. 그 수 많은 개선사항 중 Bloom\nFilter는 NodeInjector에서 특정 Provider의 존재 여부를 검사하는 로직에 적용되어 있다.\n\nIvy가 템플릿 렌더링에 사용하는 IncrementalDOM [https://github.com/google/incremental-dom]은\n전통적으로 사용하던 VirtualDOM 에 비해 메모리 사용량이 적다는 특징이 있는데. Bloom Filter 알고리즘도 메모리 사용량이\n적으므로 시너지가 있을 것으로 보인다.\n\nAngular는 아래 코드처럼 특정 Component의 생성자에 특정 타입의 파라미터를 추가하면. 런타임에 알아서 타입에 맞는 의존 인스턴스를\n찾아 없으면 만들어서 주고 있으면 주는 DI 시스템이 있다.\n\n// PARENT\n@Component({\n  template: `<app-hello></app-hello>`,\n})\nclass AppComponent {}\n\n// CHILD\n@Component({\n  selector: 'app-hello',\n})\nclass HelloComponent {\n  // AppComponent 를 생성자에서 주입받고 있다\n  constructor(private app: AppComponent) {}\n}\n\n위 코드에서 HelloComponent는 자신을 그린 상위 컴포넌트 AppComponent를 DI받고 있다(생성자에 같은 타입의 파라미터를\n기입함). 따라서 HelloComponent는 인스턴스 생성 시점에 첫 번째 인자로 AppComponent의 인스턴스를 받을 수 있다.\n\n이 때 AppComponent타입의 인스턴스를 찾기 위해 Angular내부적으로 먼저 NodeInjector를 거슬러 올라가며 찾고 찾지 못하면\n이어 ModuleInjector를 찾아 올라가는 동작을 하게 된다.\n\n이 중 NodeInjector는 Component들의 계층 구조에서 bootstrap된 최 상위 컴포넌트 까지 마치 DOM트리에서 버블링이\n일어나는 것 처럼 각 Component마다의 providers, viewProviders를 살펴본다.\n\nAngular의 Injector Chain [/_astro/injector-chain.JJ_q07XQ_1sRPUB.webp]Angular의\nInjector Chain\n\n이 때 Component의 메타데이터에 설정한 배열을 직접 살피는 게 아니라. 각 노드에 있는 Injector 인스턴스에게 특정 타입으로 생성된\n인스턴스가 있는지 묻게 된다. 바로 이 과정\n[https://github.com/angular/angular/blob/8.2.14/packages/core/src/render3/di.ts#L332]에서\nBloom Filter를 사용\n[https://github.com/angular/angular/blob/8.2.14/packages/core/src/render3/di.ts#L588]한다.\n\n아래 코드는 해시와 필터를 AND연산하여 존재 유무를 판별하는 로직을 가져와서 조금 해석해 보았다.\n\nexport function bloomHasToken(\n  // 컴포넌트 ID의 해시 값 (입력값)\n  // 위의 데모 코드에서 AppComponent는 처음으로 처리되기에 값이 0이다.\n  // 따지고 보면 Provider의 종류가 공식에서 (n)이 된다.\n  bloomHash: number,\n  // JS의 비트연산은 32비트라 256짜리 필터를 담을 수 없어 필터를 32비트씩 8개로 쪼갠 것으로 보인다\n  injectorIndex: number,\n  // 순수함수라 배열을 세 번째 인자로 넘기고 있다\n  injectorView: LView | TData\n) {\n  // Create a mask that targets the specific bit associated with the directive we're looking for.\n  // JS bit operations are 32 bits, so this will be a number between 2^0 and 2^31, corresponding\n  // to bit positions 0 - 31 in a 32 bit integer.\n  const mask = 1 << bloomHash\n  const b7 = bloomHash & 0x80\n  const b6 = bloomHash & 0x40\n  const b5 = bloomHash & 0x20\n\n  // Our bloom filter size is 256 bits, which is eight 32-bit bloom filter buckets:\n  // bf0 = [0 - 31], bf1 = [32 - 63], bf2 = [64 - 95], bf3 = [96 - 127], etc.\n  // Get the bloom filter value from the appropriate bucket based on the directive's bloomBit.\n  let value: number\n\n  // 8개 필터를 모두 사용하는 것이 아니라. 위에서 만든 플래그 기준으로\n  // 어떤 것을 써야 하는지 분기하고 있다. 즉 실제로 사용하는 필터 자체는 32비트이다.\n  if (b7) {\n    value = b6\n      ? b5\n        ? injectorView[injectorIndex + 7]\n        : injectorView[injectorIndex + 6]\n      : b5\n        ? injectorView[injectorIndex + 5]\n        : injectorView[injectorIndex + 4]\n  } else {\n    value = b6\n      ? b5\n        ? injectorView[injectorIndex + 3]\n        : injectorView[injectorIndex + 2]\n      : b5\n        ? injectorView[injectorIndex + 1]\n        : injectorView[injectorIndex]\n  }\n\n  // 앞서 공부했던 대로 AND연산하여 존재 여부를 확인하고 있다.\n  // If the bloom filter value has the bit corresponding to the directive's bloomBit flipped on,\n  // this injector is a potential match.\n  return !!(value & mask)\n}","frontmatter":{"title":"Bloom Filter알고리즘과 Angular DI 성능 개선","description":"Bloom Filter알고리즘에 대해 알기 쉽게 설명하고. Angular 내부에서 이를 어떻게 활용하고 있는지 소개합니다","cover":"../../assets/postImages/20200714/cover.webp","categories":["Framework"],"tags":["bloomfilter","algorithm","angular"],"publishedAt":"2020-07-04T20:00+09:00","readingTime":714000,"summary":"Bloom Filter는 어떤 Set안에 특정 값이 존재하는지 여부를 빠르게 계산할 수 있는 알고리즘이다. 속도도 빠르면서 저장 공간을 적게 차지하기 때문에 대규모 서비스에서 신규회원 여부, 크롬 브라우저의 멀웨어 사이트 여부, hbase, redis 등 여러 군데에 ","coverColors":["#c8c4b9","#94acb3","#7e8f9a","#866d52","#2e271c"]},"pathname":"/blog/bloom-filter알고리즘과-angular-di-성능-개선/"},{"content":"STANDARD-VERSION 소개\n\nlerna에는 커밋 메시지를 읽어 자동으로 새로운 버전과 CHANGELOG를 작성하고 git 태그를 작성해 주는 기능(이하 버전 자동화)도\n포함되어 있는데. 이 기능을 활용하면 운영에 큰 도움이 된다.\n\n이 기능을 사용하려면 —conventional-commits 인자를 전달해야 한다. 이름에서 알 수 있듯 이 기능을 사용하려면 커밋 메시지들을\nConventional Commit [https://www.conventionalcommits.org/ko/v1.0.0/]컨벤션에 맞춰 작성해야\n한다.\n\nlerna말고도 같은 기능을 제공하는 도구가 몇개 있는데. 그 중 standard-version\n[https://github.com/conventional-changelog/standard-version]은 단일 리포지토리에 버전 자동화를\n제공하는 도구이다.\n\n이 도구는 태생 자체가 버전 자동화라서 lerna보다 직관적이고. 다루기 쉽고. 조금 더 다양한 기능들을 제공한다. 만들어 내는 태그의 커밋\n메시지 포멧을 변경하거나 changelog를 뽑아 API를 호출하는 등 여러가지로 응용할 수 있다.\n\n이 글에서는 최근에 이 standard-version을 이용해 사내 서비스 배포와 롤백 시스템을 구축했던 경험을 정리한다.\n\n\n서비스를 위한 버전 자동화\n\n지난 글 모노레포 도입 검토\n[/post/%EC%95%B1%EA%B3%BC-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EA%B4%80%EB%A6%AC%EC%97%90-Monorepo]\n단계에서는 lerna를 완전히 적용하며 자동 버저닝을 서비스에서도 사용할 수 있을 것이라 예상했다.\n\n하지만 서비스들의 커밋이 섞여 각 담당자들이 라이브러리 수정 코드만을 보고 사이드 이펙트를 예측할 수 있을 지 확실하지 않다는 문제로 진행하지\n않기로 했고. 서비스는 다른 방법을 찾아야만 했다.\n\n서비스 사이드이펙트 문제 [/_astro/side-effect.wJ0eN_Vv_2oVHSw.webp]서비스 사이드이펙트 문제\n\n그 때 Conventional Commit 사이트에 소개된 툴 중에 standard-version을 도입하여 현재 문제 없이 운영하고 있다.\n\n부끄러운 이야기지만 도입 전 약 5년 동안이나 프론트 코드의 롤백은 수동으로 할 수밖에 없었다. 대부분의 배포 후 사이드이펙트는 서버API의\n문제였기 때문에 크게 필요성을 느끼지 못했던 부분도 있다.\n\n하지만 FE개발파트가 커지고 사내 모든 서비스를 담당하게 되면서 시스템의 필요성이 생겨 지금에라도 도입하게 되었다.\n\n\n새로 도입한 배포 프로세스\n\n> 먼저 그림으로 보고 각 단계에서 하는 일들을 아래에서 설명한다\n\n배포 프로세스 [/_astro/deploy-strategy.Dtp7EMUc_ZmgYzT.webp]배포 프로세스\n\n\n1. 기능 개발\n\n커밋 메시지는 Conventional Commit [https://www.conventionalcommits.org/ko/v1.0.0/]을\n준수해야 한다. 작성이 어렵다면 도움을 주는 commitizen [https://github.com/commitizen/cz-cli]을 사용하면\n좋다. 만약 cli에 익숙하지 않다면 어댑터 [https://github.com/commitizen/cz-cli#adapters]를 사용하여\n원하는 도구에서도 commitizen을 이용할 수 있다.\n\n# 커밋 메시지 작성을 대화형으로 도와주는 도구 설치\nnpm install -g commitizen\n\n# 프로젝트를 commitizen friendly 하게 만들어야 한다\ncommitizen init cz-conventional-changelog --save-dev --save-exact\n\n# 설치하고 나면 아래 명령으로 커밋 메시지를 대화형 도구로 작성할 수 있다\ngit cz\nSelect the type of change that you're commiting: (use arrow keys)\n> feat:     A new feature\n  fix:      A bug fix\n  docs:     Documentation only changes\n  styles:   Changes that do not effect the meaning of the code\n            (white-space, formatting, missing semi-colons, etc)\n  refactor: A code change that neither fixes a bug or add a feature\n  perf:     A code change that improves performance\n\n\n커밋 전에 커밋 메시지들이 컨벤션에 맞게 작성되었는지 검사해 주면 좋다. commitlint\n[https://github.com/conventional-changelog/commitlint]와 husky\n[https://github.com/typicode/husky]를 이용한다.\n\n# 커밋 시 커밋 메시지들을 검사하기 위한 도구 설치\nnpm install -D commitlint husky\n\n// package.json 에 커밋할 때 메시지들을 검사하도록 수정\n{\n  \"husky\": {\n    \"hooks\": {\n      \"commit-msg\": \"commitlint -E HUSKY_GIT_PARAMS\"\n    }\n  }\n}\n\n\n2. CI PR CHECK\n\nPull Request를 만들어 코드리뷰를 한다면 PR Check 기능을 이용해 커밋 메시지를 검사해 준다. 실수로 잘못된 커밋 메시지의 커밋이\n푸시될 수 있기 때문이다. PR Check을 하지 않는다면 이 단계는 건너뛰고 다음 빌드 과정에서 하면 된다.\n\n대부분의 유명 오픈소스 프로젝트는 모두 PR Check에서 이 검사를 한다\n[/_astro/pr-build.CiUSv7a0_Z8lDR8.webp]대부분의 유명 오픈소스 프로젝트는 모두 PR Check에서 이 검사를 한다\n\n# 커밋 메시지들이 컨벤션에 맞게 작성되었는지 검사한다\ngit log -1 --pretty=format:\"%s\" | npx commitlint\n\n\n3. CI 빌드 설정\n\n> 전 단계를 건너뛰었다면 이 단계에서 커밋 메시지를 검사하도록 해 준다.\n\n배포 빌드에서는 특정 커밋 메시지엔 빌드가 돌지 않도록 설정해야 한다. 위의 그림에서 B-1커밋은 개발 장비에서 배포를 위해\nstandard-version명령을 실행하여 package.json의 버전 범핑과 CHANGELOG.md의 수정사항 커밋이 추가된 경우인데 이때\n빌드는 무의미하다.\n\nJENKINS기준으로는 Git Plugin > Polling ignores commits with certain messages 항목을 추가하고\n값은 (?s).*chore\\(release\\).*로 설정하면 된다. 비슷한 기능을 Git Actions, Travis나 Circie 에서도\n지원하므로 똑같이 설정하면 된다.\n\n\n4. CDN 업로드\n\nCI가 만든 css, js, jpg, html등의 리소스 파일을 CDN에 업로드하도록 한다. 이 때 서비스가 여러개라면 겹치지 않도록 나누어야\n한다. 또 테스트 서버가 여럿이라면 또 페이즈별로 겹치지 않도록 폴더를 나누어 주어야 할 것이다. 아래는 실제로 사용하고 있는 주소의 형태를\n변형한 예제이다.\n\n> 서비스 A 페이즈: beta\n> \n>  1. https://mycdn.net/mycompany/service-a/beta/2b30274/\n>     \n>     위 경로에 빌드 결과물들을 업로드한다.\n> \n>  2. https://mycdn.net/mycompany/service-a/beta/latest\n>     \n>     이 파일에는 1번의 url (최신 버전이 무엇인지)를 기록해 둔다\n> \n> 서비스: B, 페이즈: real\n> \n>  1. https://mycdn.net/mycompany/service-b/real/2b30274/\n>  2. https://mycdn.net/mycompany/service-b/real/latest\n\n> 이 단계에서는 빌드 결과물을 어디에 그리고 얼마동안 보관해야 하는지에 대한 고민이 있었다. 나의 경우 사내 인프라로 CDN이 제공되고 있었고\n> 용량이 무제한이라 CDN에 저장하기로 했고 오래된 빌드를 따로 삭제하지는 않았다. 다만 CDN은 위처럼 빌드 결과물의 경로를 신경써야 한다.\n> 하지만 NPM을 사용한다면 압축 파일을 풀면 바로 결과물을 확인할 수 있기 때문에 이후 배포 과정의 스크립트를 더 단순하게 작성할 수 있다.\n\n\n5. 배포 및 버저닝\n\n\n웹 서버에 리소스 배포하기\n\n이 단계에서는 CDN으로부터 최신 버전의 index.html파일을 받아 웹서버에 배포한다. 최신버전은 전 단계의 url을 보면 알 수 있듯\n서비스명/페이즈/latest 규칙이다. 배포 스크립트는 각 환경에 맞게 작성하면 된다.\n\n# CDN에 업로드 한 latest파일의 내용에는 최신버전 패키지의 주소가 적혀 있다\nPACKAGE_DIR=$(curl https://mycdn.net/mycompany/$SERVICE/$PHASE/latest)\n\n# 그러므로 latest파일의 내용(최신버전 index.html이 있는 CDN의 폴더 경로)\n# 에 index.html을 붙이면 배포 대상이 된다\ncurl -o index.html $PACKAGE_DIR/index.html\n\n# 최신 index.html을 웹 서버에 배포한다\ncurl -T index.html -u userid:passswd ftp://10.10.1.55/www/publish\n\n위의 파일을 만들어 두고 SERVICE=service-a PHASE=beta sh deploy.sh 로 실행하면 배포할 수 있다.\n\n혹시 index.html만을 배포하는 이유를 궁금해할 수 있을 듯 한데. 내용을 보면 이해할 수 있을 것이다.\n\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Service A App</title>\n  </head>\n  <body>\n    <app-root></app-root>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/main.js\")}></script>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/polyfill.js\")}></script>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/vendor.js\")}></script>\n    <script src={import(\"../../../https://mycdn.net/mycompany/service-a/real/2b30274/common.js\")}></script>\n  </body>\n</html>\n\nangular는 빌드할 때 —deploy-url인자를 전달하면 만들어내는 index.html에 위와 같이 script src들과 내부에서 쓰는\ncss의 url에 prefix(예제에서는 https://mycdn.net/mycompany/service-a/real/2b30274/) 를 붙여\n준다.\n\n따라서 index.html만 배포해도 된다. 만약 상대경로로 포함하는 방식이라면 관련 파일을 모두 업로드하도록 스크립트를 작성하면 된다.\n\n\nREAL 페이즈 배포 후 버저닝\n\n> ⚠️ 이 과정은 테스트 서버들이 아닌 real 페이즈 배포 직후에만 실행한다. 이유는 develop, master에서 자동 버저닝을 수행하면\n> 중복된 태그를 생성할 수 있기 때문이다.\n\n> —prerelease 인자를 사용하면 v1.3.1-alpha.0 형태의 태그를 만들 수 있다. 테스트 서버 브랜치에서는 배포시 이 인자를\n> 사용하면 리얼 브랜치와 태그가 겹치는 것을 예방할 수 있을 것으로 보이지만 필요한 부분이 아니라 추가 검토는 하지 않았다.\n\n만약 real에 배포했다면 버저닝을 할 차례이다. 이는 뒤에서 설명할 롤백을 위해 꼭 필요한 과정으로 standard-version을 이용할\n것이다.\n\n# 버저닝은 배포할 브랜치에서 수행해야만 한다\ngit checkout master\ngit pull\nnpx git-branch-is master\n\n# 자동 버저닝\nstandard-version \\\n  --release-as=minor \\\n  --releaseCommitMessageFormat=\"chore(release): {{currentTag}}\\n[[$deployUrl]]\"\n\n# package.json, CHANGELOG.md파일의 수정사항과 새 태그를 함께 푸시한다\ngit push origin HEAD --follow-tags --no-verify\n\nrelease-as 인자는 혹시 모를 핫픽스를 위함이다. 서비스의 배포는 minor이상을 올려야 patch로 긴급 배포를 할 수 있게 된다.\n어제 1.1.0 을 배포하고 오늘 1.2.0 을 배포했는데. 1.1.0 기준으로 핫픽스를 배포해야 한다면 1.1.0 에서 브랜치를 따고\n1.1.1로 배포하면 된다.\n\nreleaseCommitMessageFormat인자로 기본값 뒤에 개행과 **[[$deployUrl]]**를 붙여 주었다. 이는 롤백을 위해\n필요한 부분인데 이어지는 섹션에서 설명한다.\n\n\n롤백 프로세스\n\n롤백 프로세스 [/_astro/rollback-strategy.B6dGmbGO_Zo5GEx.webp]롤백 프로세스\n\nreal배포를 하면 태그가 쌓일 것이다 아래 명령으로 조회할 수 있다. v1.3.0로 롤백을 원한다고 가정해 보자.\n\ng tag --sort=-committerdate -l \"v*\"\n\nv1.3.1\nv1.3.0\nv1.2.5\nv1.2.4\nv1.2.3\nv1.2.2\nv1.2.1\nv1.2.0\nv1.0.0\n(END)\n\nv1.3.1의 태그 커밋 메시지에는 배포 단계에서 넣어주었던 **[[$deployUrl]]**이 기록되어 있을 것이다.\n\ngit tag -l --format='%(contents)' v1.3.1\n\nv1.3.1\n\n[[https://mycdn.com/myapp/eff782/]]\n\ndeployUrl은 index.html을 포함한 빌드 결과물들이 있는 CDN폴더 주소이다. url을 추출해 index.html을 붙여준 파일을\n다운로드 하여 배포하면 롤백이 완료된다.\n\nCONTENT=$(git tag -l --format='%(contents)' v1.3.1)\nDEPLOY_URL=$(node -p \"'$CONTENT'.match(/\\[\\[\\d\\]\\/])[1]\")\n\ncurl -o index.html $DEPLOY_URL/index.html\ncurl -T index.html -u userid:passswd ftp://10.10.1.55/www/publish\n\n\n요약 및 기타 고려사항\n\n일단 내용을 짧게 요약하자면 다음과 같다.\n\n * 리얼 배포 시점에 standard-version으로 버전 자동화를 적용한다\n * 버전 자동화 중에 추가로 CDN에 업로드 된 빌드 결과물의 주소를 태그에 기록한다\n * 롤백할 땐 원하는 태그의 메시지를 읽어 특정 포멧으로 기록된 CDN주소의 결과물을 배포하면 된다\n\n이번 개선 작업에서 하지 못한 작업들도 있다. 빠른 시일 내에 추가 적용하려고 한다.\n\n * standard-version은 changelog 훅도 제공하고 있어 배포 시점에 이메일로 변경사항을 공유하는 등의 작업이 가능할 것으로\n   보인다.\n\n글에서 소개한 배포 및 롤백 프로세스의 경우 쉘 스크립트를 작성하고 이를 package.json에 등록해 사용하는 편이 좋겠다.\n\n사내에서는 배포 및 롤백 스크립트 포함된 태스크 러너를 nodejs기반 cli로 만들어 사용하고 있다. 기회가 된다면 관련해서도 글을 쓸\n예정이다.\n\n혹시 내용관련 궁금한 사항이 있다면 이메일 [romz1212@gmail.com]을 보내 주기 바란다.","frontmatter":{"title":"standard-version을 이용한 배포, 롤백 전략","description":"standard-version을 사용하여 서비스에 자동 버저닝, 배포, 롤백 시스템을 적용하는 방법에 대해 설명합니다","categories":["Development"],"tags":["standard-version","deploy strategy"],"publishedAt":"2020-06-25T20:00+09:00","readingTime":873000,"summary":"lerna에는 커밋 메시지를 읽어 자동으로 새로운 버전과 CHANGELOG를 작성하고 git 태그를 작성해 주는 기능(이하 버전 자동화)도 포함되어 있는데. 이 기능을 활용하면 운영에 큰 도움이 된다.이 기능을 사용하려면 —conventional-commits 인자를 "},"pathname":"/blog/standard-version을-이용한-배포-롤백-전략/"},{"content":"공유하는 코드를 어떻게 관리하지?\n\n현 직장에서 사내의 모든 FE 프로젝트를 하나의 팀에서 담당하게 되다 보니 어느 순간 의존성 관리 문제에 봉착하게 되었다. 라이브러리와 앱이\nN:N으로 늘어나 의존성 관리가 복잡해지고 있다.\n\n예를 들면. 쇼핑몰의 경우 보통 웹과 관리자 한 세트로 개발하여 운영한다. 만약 새로운 서비스를 오픈한다면 총 2세트가 되고. 서비스 간에\n회원, 주문을 하나로 통합하는 경우 2세트가 더 추가된다. 결국 서로의 의존성이 얽혀있는 8개 이상의 리포지토리를 운영하게 된다.\n\n서비스 사이드이펙트 문제 [/_astro/structure1.YBCaCW_Y_Z1GYS1X.webp]서비스 사이드이펙트 문제\n\n이 글에서는 이 문제를 해결하는 방법 중 Monorepo(이하 모노레포)도입을 검토하며 논의했던 내용을 정리한다. 일단은 앞서 언급했던 문제를\n고려하지 않고. 단순히 중복되는 코드를 공유하는 방법에 대해서 간단히 짚고 넘어가 보려 한다.\n\n 1. npm\n 2. git submodule, subtree\n 3. 모노레포 (Yarn Workspace + Lerna)\n\n\nNPM 사용\n\n라이브러리 빌드 결과물을 npm publish로 업로드하고 써야 하는 곳에서 npm install 로 설치해 사용하는 방법이다. npm에 올리는\n패키지들은 1.10.x, 2.1.0과 같이 버전을 명시하게 되어 있으므로 관리나 사용에 큰 도움이 된다.\n\n불편한 점이 있다. 개발할 땐 앱이건 라이브러리건 구분 없이 빈번하게 수정해야 하는데. 라이브러리를 수정하고 나면 항상 npm publish,\nnpm install을 통해 공유하는 절차를 거쳐야 하기 때문이다. 라이브러리 갯수가 두 개 이상이면 이런 절차에 시간이 많이 소요될 것이다.\n\n그나마 npm link를 이용하면 로컬에 빌드된 패키지를 전역에 설치하여 다른 패키지에서 설치해 쓸 수 있다. 한마디로 npm publish를\n하지 않아도 된다. 라이브러리 빌드 결과물 폴더에서 npm link실행 후 사용을 원하는 곳에서 npm link <패키지명>으로 연결하여 사용할\n수 있다. 마찬가지로 라이브러리 개수가 늘면 불편하고. 개발이 끝난 후 전역에 설치된 패키지를 일일이 제거해주어야 하는 번거로움이 있다.\n\n\nSUBMODULE, SUBTREE 사용\n\n리포지토리 내 특정 폴더에 다른 리포지토리를 사용하는 방식으로 부모 리포지토리에서는 폴더 자체를 특정 커밋으로 다루며 커밋은 자식과 별도로\n독립적으로 관리한다. 부모 리포지토리에서 diff를 보면 서브모듈 폴더는 단순히 서브모듈의 여러 커밋 중 한 군데를 가르키는 해시값만 보인다.\n\n따라서 서브모듈에 컨플릭이 일어났을 때 부모 리포지토리에서 바로 머지할 수 없고. 서브모듈 안에서 컨플릭을 해결하고 그 커밋을 가르키도록 부모\n리포지토리를 수정해야 하는데 이게 번거롭기도 하고 git에 익숙하지 않은 개발자라면 실수할 여지가 있다.\n\n서브트리는 부모 리포지토리에서 파일을 관리할 수 있다고 한다 사용해보지는 않았지만 Git subtree를 활용한 코드 공유\n[https://blog.rhostem.com/posts/2020-01-03-code-sharing-with-git-subtree]를 참고했을\n때 서브모듈보다 편리할 것으로 보인다.\n\n글에서 언급하는 컨플릭트는 코드리뷰를 하는 팀이라면 머지 전에 사이드 이펙이 예상되는지 담당자들이 잘 검토하는 것도 방법이고. 특정 패키지가\n수정되었을 때 영향이 있는 의존 패키지들을 알려주는 도구들도 있으니 응용한다면 효율적으로 관리할 수 있을 것으로 보인다.\n\n> typescript 프로젝트의 경우 각 모듈이 tsconfig.json에 정의된 root 경로 하위에 존재해야 참조할 수 있으므로.\n> 서브모듈, 서브트리 모두 참조가 가능하도록 폴더 구조를 잡아야 한다.\n\n\n모노레포 (YARN WORKSPACE + LERNA)\n\n하나의 리포지토리에서 사내의 모든 코드를 관리하는 방식이다. nodejs 프로젝트의 경우 일반적으로 yarn workspace를 이용하여 패키지\n의존성을 관리하고. lerna를 사용해 로컬 패키지 간 의존성 추가, 다수의 패키지에 특정 task를 수행, 사이드 이펙이 발생하는 앱들을\n추려내거나 하는 형태로 운영한다. 먼저 장단점을 정리해 보면 다음과 같다.\n\n\n코드 공유가 간편해진다\n\n코드들이 모두 하나의 리포지토리 내에 있으므로 패키지들의 추가 및 제거가 간단하다. 중복 코드는 그냥 패키지용 폴더를 만들어 옮기고 lerna\nadd로 의존성 추가 후 yarn install 하면 되고. 제거하는 경우 그냥 삭제하면 된다.\n\n로컬 패키지들 간의 의존성은 심볼릭 링크로 처리된다. 윈도우로 치면 폴더 바로 가기의 형태로 연결하기 때문에 npm install로 사용하는\n것과 똑같이 쓸 수 있지만, 공유를 위한 절차가 간단해져 편리해지는 것이다.\n\n두 개의 블로그가 공유하는 모듈을 node_modules내에 symlink로 추가해준 모습\n[/_astro/symlink.DIBjrqQF_1Bajs3.webp]두 개의 블로그가 공유하는 모듈을 node_modules내에 symlink로\n추가해준 모습\n\n또 단순히 심볼릭 링크로 연결되어 있고. 라이브러리 소스도 같은 리포지토리에 있어서 디버깅이 매우 간편하다. 앱을 띄워 두고 라이브러리 코드를\n직접 수정하면서 디버깅할 수 있다.\n\n\n버전관리가 편해진다\n\nlerna에는 각 패키지의 버전 관리를 쉽게 할 수 있는 기능도 있다. 커밋 메시지를 Conventional Commits\n[https://www.conventionalcommits.org/ko/v1.0.0/]에 맞게 작성하는 경우. 배포 시점에 추가적인 옵션을 주면\n쌓인 메시지를 읽어 각 패키지의 새로운 버전을 계산해 준다.\n\n예를 들어 feat(core): 렌더링 로직 성능 향상이라는 메시지라면 feat이므로 minor 업데이트이며. (core)로 여러 패키지 중\ncore 패키지가 업데이트 되는 것임을 인식한다. 따라서 다른 패키지의 버전은 올리지 않고. 오직 core 패키지의 버전만 올린다. 렌더링 로직\n향상이라는 메시지는 CHANGELOG.md 파일에 자동으로 정리해 준다. 그리고 변경사항이 있는 패키지를 추려 자동으로 npm에\npublish까지 해 준다.\n\n자동으로 생성된 CHANGELOG.md [/_astro/changelog.DUyFHgg2_Zcqc0c.webp]자동으로 생성된\nCHANGELOG.md\n\n모노레포 운영 시 변경사항을 관리하는 것이 더욱더 중요한데 이런 점에서 큰 도움이 될 것으로 보인다. 이 기능은 꼭 모노레포 운영을 하지\n않더라도. 하나의 리포지토리에서 여러 npm 패키지를 관리하는 경우 업무량을 크게 줄여줄 수 있을 것으로 보인다.\n\n\n리포지토리의 크기가 커짐\n\n일반적으로 많이 알려진 문제이다. angular의 소스를 clone 받을 때마다 느끼는 점으로 리포지토리 하나가 너무 커진다. 프로젝트 초기에는\n별문제가 없겠지만 오랫동안 쌓이면 git 액션들의 처리 속도가 조금씩 느려질 것이다. 하지만 angular 컨트리뷰팅 과정에서 크게 느려서\n못쓰겠다는 느낌을 받은 적은 없어서 괜찮을 것으로 생각된다.\n\n\n사이드 이펙트 문제\n\n도입 검토 중에 제일 논란이 되었던 문제다. 배포 시기가 다른 모바일 웹 서비스 A, B가 있다고 가정할 때. B의 기능 추가 커밋이 A에도\n영향을 줄 수 있다는 것이다. 만약 기능 추가 중에 라이브러리를 수정했다면 A는 사이드 이펙트가 있을 수 있는데. 담당자가 이것들을 모두 사전에\n검토할 수 있을지 모르겠다는 것이다.\n\n사이드 이펙트 문제 [/_astro/side-effect.wJ0eN_Vv_2oVHSw.webp]사이드 이펙트 문제\n\n현 회사는 모든 변경사항은 반드시 PR을 거치게 되어 있지만. 검토해야 하는 양이 많고 당장 해결해야 하는 업무가 많은 상황에서는 놓치는 경우가\n많을 것이다. 이 이슈는 모노레포의 문제 중 하나로 팀원 개개인에게 적정한 수준의 역량을 요구한다는 것이다. 결국 전면적 도입은 힘들다는 결론이\n났다.\n\n\n모노레포 적용이 어려운 이유\n\n한 회사에 서비스가 여럿이라면 보통 배포 시기가 다 다르기 마련이다. 앞서 언급한 대로 개개인의 역량이 따라준다고 하면 모르겠지만 서비스의\n안정적인 운영을 고려한다면 같은 리포지토리 내에서 사이드 이펙트가 있을지 없을지 모르는 커밋이 쌓이는 것 자체가 문제가 될 여지는 있다.\n\n또 현 회사는 테스트용으로 3개의 서버 (alpha, sandbox, beta), 실 배포용으로 1개의 서버(real)를 운영하는 4단계\nphase 운영을 하므로 모든 리포지토리가 최소 두 개 이상의 배포 브랜치를 가지고 있다.\n\n그러나 lerna publish 명령 자체가 만들어내는 버전 태그들은 유일해야 한다. 이런 단일 배포 브랜치 기반 프로세스를 보면 애초에 팀의\n배포 프로세스부터가 맞지 않는 것으로 보인다.\n\n\n결국은 전면적 도입은 하지 않기로\n\n결국 단일 배포 브랜치를 갖는 라이브러리만 적용하고 서비스는 기존대로 운영하게 되었다. lerna를 이용해 멀티 패키지 라이브러리 리포지토리\n관리를 하게 된 것만으로도 업무량이 크게 줄어서 도입을 검토했던 시간이 아깝지는 않았다. 환경이 맞지 않아 도입할 수 없었지만 적용에 따른\n이슈도 크리티컬한 부분이라 아쉽다는 생각이 들지는 않았다.\n\n이야기 중에 라이브러리에만 모노레포를 적용하자는 말을 많이 들었다. 그 자리에서 언급한 내용이기도 하지만 그렇게 되는 순간 이미 모노레포가\n아니게 된다. 본문에서 언급했던 모노레포의 장점이 희석되기 때문이다.","frontmatter":{"title":"앱과 라이브러리 관리에 Monorepo?","description":"실무에서 사용하며 느꼈던 monorepo의 장점과 결국 도입할 수 없었던 이유에 대해 설명합니다","categories":["Development"],"tags":["monorepo","submodule","subtree","lerna","yarn"],"publishedAt":"2020-06-18T20:00+09:00","readingTime":832800,"summary":"현 직장에서 사내의 모든 FE 프로젝트를 하나의 팀에서 담당하게 되다 보니 어느 순간 의존성 관리 문제에 봉착하게 되었다. 라이브러리와 앱이 N:N으로 늘어나 의존성 관리가 복잡해지고 있다.예를 들면. 쇼핑몰의 경우 보통 웹과 관리자 한 세트로 개발하여 운영한다. 만약"},"pathname":"/blog/앱과-라이브러리-관리에-monorepo/"},{"content":"AMPLIFY의 불편한 점\n\nAmplify는 웹 서비스를 개발하고 배포하기 위해 필요한 AWS의 기능들을 짜깁기한 서비스다. github을 연동하고 설정만 조금 만지면 웹\n앱 하나가 뚝딱 만들어진다. S3, CloudFront를 설정하고 연동해야 하는 부담이 없어 사용했다. 하지만 기능이 간소화 된 만큼 디테일한\n부분을 다루는 게 불가능했다. 나의 경우 배포 후 캐시를 삭제할 수 없어서 기다리면 되겠거니 했는데 거의 하루 반나절 동안 페이지를 볼 수\n없었다.\n\n또 리다이렉트 규칙을 세세히 적용할 수 없었다. gatsby의 정적 리소스들을 서빙하는 데 문제가 발생했다. 예를 들어 *.js, *.css로\n끝나는 요청의 경우 해당 파일을 서빙해야 하고. 그 외에는 index.html을 서빙해야 하는데. 해당 기능을 설정하면 이게 기능이 각 서버에\n배포 중인지 아닌지에 대한 정보도 표시되지 않고. 스크립트 오류는 계속 발생했다.\n\nAmplify Redirect 설정 [/_astro/aws-amplify-redirect.CitaV0rq_1E3eF6.webp]Amplify\nRedirect 설정\n\n위와 같이 정규식으로 Redirect를 설정하는데 배포 상태를 알 수 없으니 식을 맞게 작성한것인지 파악하기도 어려웠다. *.js 요청에\nhtml본문이 응답되는 현상이 해결되지 않아서 Amplify를 쓰지 않기로 결정했다.\n\n\nS3, CLOUDFRONT로 GATSBY 서빙하기\n\n그리하여 Gatsby를 사용하기 위한 AWS의 서비스들을 직접 설정하고 연동했다. 번거롭지만 확실하게 해당 문제들을 해결할 수 있게 되어\n만족스러웠다. 아래부터는 mnkim.com [https://mnkim.com]을 S3, CloudFront에 설정했던 내용에 대해 자세히\n다룬다.\n\n\nS3 버킷 설정하기\n\nS3는 AWS서비스 중 스토리지 관리와 모니터링을 제공하는 기능이다. 사실 나도 무슨 말인지 모르겠다. 다만 우리는 gatsby build\n명령을 통해 만들어진 public/* 파일들을 업로드하고 각 파일에 서비스 별 접근 권한을 설정하는 기능을 사용하게 될 것이다.\n\n먼저 버킷을 만든다. 버킷의 이름은 나중에 다른사람들이 블로그에 방문할 때 쓰는 주소로 한다. 예를 들어 www를 붙이지 않은\nmnkim.com을 원하는 경우 이름은 mnkim.com이 되어야 한다. 반대의 경우 www.mnkim.com로 이름을 짓는다.\n\n[/_astro/s3-1.BJ5cLgPQ_wwvex.webp]\n\n생성할 때는 위 화면처럼 퍼블릭 엑세스를 가능하게끔만 설정하면 된다. 생성이 완료되었다면 개요 탭에서 gatsby build를 통해 생성된\npublic/*의 하위 파일들을 모두 선택해 업로드한다. 다음 속성 탭에서 정적 웹 사이트 호스팅을 선택하고 아래와 같이 설정한다.\n\n[/_astro/perm.EApx57PJ_ZnA2AX.webp]\n\n다음으로는 권한 탭의 버킷 정책에서 ‘정책 편집기’ 타이틀 옆 ARN값을 복사해 두고 에디터 아래 정책 생성기를 클릭힌다. 그럼 다음곽 같은\n화면이 나오는데 위의 스샷처럼 입력한다. Actions는 GetObject를 선택하면 된다.\n\n이어서 Add Statement를 클릭하면 아래 정책목록이 추가되고. Generate Policy를 클릭하면 팝업에 json값이 출력되는데\n이것이 S3 버킷에 대한 권한 설정 내용이다. 복사하여 부모 창의 편집기에 넣고 저장을 누른다. 이 때 반드시 Resource뒤에 ‘/*’를\n붙여 버킷 내 파일 모두에 접근할 수 있도록 해 주어야 한다.\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"Policy1239293829383\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt15910279239283\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::<내 도메인>/*\"\n    }\n  ]\n}\n\n[/_astro/s3-2.DdCQGRky_1gqL5X.webp]\n\n이 버킷을 사용하여 웹 사이트를 호스팅합니다 체크, 인덱스 문서는 index.html, 오류 문서는 404.html로 적고 저장을 누른다. 그럼\n이제부터 상단에 있는 엔드포인트 주소로 사이트를 서빙할 수 있게 된다. 다만 이 주소는 기억하기 어려운 형태이고 http이기도 해서 이후\n과정에서 별도의 도메인을 연결한다.\n\n\nCERTIFICATE MANAGER로 인증서 만들기\n\n먼저 https로 서빙하기 위한 인증서를 만들어야 한다. Certificate Manager에서 인증서 요청 을 누르고 이어 공인 인증서 요청\n선택. 도메인 이름에 johnny.com. 아래 다른 이름 추가 를 누르고 *.johnny.com (소유한 도메인으로 입력한다)\n\n[/_astro/cm-1.WoaOUM3e_Z1T0P4X.webp]\n\n이후 과정에서 해당 도메인의 진짜 소유자인지를 검사하는 과정을 거친다. DNS인증을 선택하고 나오는 각 도메인 별 이름과 값을 도메인을 구입한\n사이트에서 제공하는 DNS구성 기능으로 CNAME으로 등록해야 한다. 드롭다운으로 A인지 CNAME인지 TXT인지 선택하고 이름과 값을 입력하는\nUI구성으로 아마 조금 살펴보면 알 수 있을 것이다.\n\n도메인을 AWS Route 53에서 구입했거나. 구입처가 다르지만 Route 53에서 관리하도록 설정한 경우에는 아래 버튼을 누르면 자동으로 그\n값들을 등록해준다.\n\n[/_astro/cm-2.C6leAfa9_1iqwp4.webp]\n\n\nCLOUDFRONT 설정하기\n\nCloudFront는 S3버킷에 담긴 파일을 전세계 서버에 캐싱해서 빠르게 서빙할 수 있도록 해 주는거 같다. 안해도 상관은 없는데 하면 사이트\n성능이 크게 개선되므로 해보는것도 나쁘지 않다.\n\nCreate Distribution클릭. Web 섹션의 Get Started클릭. 아래 필드에서 따로 언급하지 않은 것들은 기본값으로 내버려\n둔다.\n\n[/_astro/cf-1.DY_ffoiy_WOgG0.webp]\n\n아래 Distribution Settings에서 Alternative Domain Names에 사용할 도메인을 입력하고. 아래 Custom\nSSL Certificate 를 선택한다. 그 후 아래 텍스트박스에 포커스하면 만들어둔 인증서가 보일텐데 그것을 선택한다.\n\n[/_astro/cf-2.blwCps28_28MnWG.webp]\n\n마지막으로 Default Root Object를 반드시 index.html로 입력한 후 다음으로 넘어간다. 이렇게 되면 세팅이 끝나고 전세계의\n모든 서버에 해당 설정이 반영되는 시간 후 Distribution Status가 Deployed로 바뀌며 화면의 Domain Name 으로\n블로그에 접속할 수 있게 된다. 하지만 우리는 이 주소가 아닌 내 소유의 도메인 johnny.com을 연결해야 한다 이후에서 진행한다.\n\n[/_astro/cf-3.BG0kj8AB_ZjnOvn.webp]\n\n\nROUTE 53으로 커스텀 도메인 연결하기\n\n일단 나처럼 Godaddy의 도메인을 Route 53에서 관리하도록 설정하든, Route 53에서 직접 구입하든 도메인이 하나 있어야 한다.\n(위에서 인증서를 커스텀으로 설정했다면 있을 것이다) 호스팅 영역을 생성하고 레코드 세트를 하나 생성한다. 유형은 A. 값은 방금 설정했던\nCloudFront의 Domain Name을 입력하면 되는데.\n\n직접 입력해도 되지만 별칭 을 예로 선택하고 텍스트박스에 포커스 하면 목록이 출력되는데. 그 중 CloudFront배포 에 이전에 설정한 값이\n있으므로 선택해서 채워도 된다. Route 53 미사용자는 앞서 도메인을 인증할 때 썼던 페이지에서 주소를 직접 입력하면 된다.\n\n[/_astro/r-1.rwoWdY33_2pv8f9.webp]\n\n저장 후 10분 정도 기다리면 해당 도메인으로 블로그에 접속할 수 있다!\n\n\nWWW에서 NON-WWW로 보내기 (선택)\n\n안해도 되는데 나처럼 non-www로만 서빙을 원하는 경우 설정하면 된다. 아까 johnny.com으로 S3버킷을 만들었던 것 처럼 똑같이\nwww.johnny.com이름으로 버킷을 만든다. 이 때 퍼블릭 엑세스는 풀고. 파일은 업로드하지 않아도 된다.\n\n속성에서 정적 웹사이트 호스팅을 체크하고 이번엔 요청 리디렉션을 선택한다. 대상 버킷 또는 도메인에 johnny.com을 입력하고 프로토콜은\nhttps를 선택한다. 반대의 경우는 반대로 입력하면 된다. 물론 위의 세팅들이 모두 반대 (www)로 설정되었어야 한다.\n\n[/_astro/non-www.D56TUqGp_1nX8bL.webp]\n\n그 다음 Route 53에서 같은 호스팅 영역에 새 레코드 세트를 만든다. 이름은 www.johnny.com. 유형은 A. 값은 별칭으로\n동일하게 선택하되 **www.johnny.com**버킷을 선택한다.\n\n\n배포 설정\n\n이제 맨 처음 S3에 빌드 결과물을 직접 업로드 했던 것을 명령으로 할 수 있게 할 차례다. 그런데 별도의 인증 없이 내 버킷에 업로드가\n가능하다면 다른사람도 업로드할 수 있다는 말이 되니까 인증을 해야 한다. 먼저 AWS서비스 중 IAM에서 인증 토큰을 발급한다.\n\n대시보드에서 사용자를 누르고 상단 사용자 추가를 누른다. 사용자 이름을 입력하고 AWS액세스 유형은 프로그래밍 방식 액세스. 그룹 생성을 눌러\n그룹 이름은 원하는 대로 짓고 AdministratorAccess 권한을 부여한다.\n\n모든 과정을 완료하면 마지막에 액세스 키 ID와 시크릿 액세스 키가 보이고 따로 인증 csv를 다운로드 받을 수 있는 화면이 보인다. 이\n페이지를 벗어나면 더 이상 키를 확인할 수 없으므로 csv파일은 잘 보관해둔다.\n\n먼저 프로젝트에 dotenv패키지를 설치한다.\n\nnpm install --save-dev dotenv\nyarn add -D dotenv\n\n다음 .env파일을 생성하고 다음 내용을 입력한다. 해당 파일은 git에 커밋되지 않도록 해야 한다. 만약 커밋을 한다면 다른사람들이 이 내용을\n볼 수 없도록 repository자체를 private으로 만들어야 한다.\n\nAWS_ACCESS_KEY_ID=xxxx\nAWS_SECRET_ACCESS_KEY=xxxx\n\n이어서 gatsby-plugin-s3패키지를 설치하고 gatsby-config.js의 플러그인 항목에 아래 내용을 추가한다.\n\nrequire('dotenv').config() // .env 파일 읽어서 환경변수에 추가한다\n\nmodule.exports = {\n  plugins: [\n    {\n      resolve: 'gatsby-plugin-s3',\n      options: {\n        bucketName: 'johnny.com', // 업로드 대상 S3 버킷 이름\n        protocol: 'https', // 프로토콜\n        hostname: 'johnny.com', // 호스트명\n      },\n    },\n  ],\n}\n\n그 후 gatsby build && npx -n \"-r dotenv/config\" gatsby-plugin-s3 deploy 명령을 실행하면\n빌드 후 내용이 S3 버킷에 업로드 된다. 아래처럼 package.json에 설정해두고 쓰면 편하다.\n\n\"scripts\": {\n    \"deploy\": \"npx -n \\\"-r dotenv/config\\\" gatsby-plugin-s3 deploy\"\n}\n\n\n작업 후기\n\n * johnny.com으로는 접속이 잘 되는데 johnny.com/blog와 같이 서브도메인에서 새로고침했을 때 AccessDenied에러가\n   나는 경우는 S3버킷 정책이 올바르게 설정되지 않았기 때문일 수 있다. 최종적으로 설정했을 때 정책이 바뀌어 있다면 맨 위 S3버킷 설정과\n   동일하게 다시 설정한다.\n * 이제 캐시를 직접 제거할 수 있다. CloudFront에서 해당 Distribution을 선택하고 아래 처럼 Invalidation을\n   만들어 주면 된다.\n\n[/_astro/cache.Dbk-dXt3_1vz8ek.webp]\n * CloudFront의 내용 수정이 모든 Edge에 반영되기까지 시간이 꽤 걸린다. 그리고 삭제를 원하는 경우 Distribution이\n   비활성화가 되어야 한다.","frontmatter":{"title":"Amplify에서 S3 CloudFront로 전환하기","description":"블로그를 Amplify에서 S3와 CloudFront로 전환했던 경험을 설명합니다","categories":["Infra"],"tags":["aws","amplify","s3","cloudfront"],"publishedAt":"2020-06-02T20:00+09:00","readingTime":850800,"summary":"Amplify는 웹 서비스를 개발하고 배포하기 위해 필요한 AWS의 기능들을 짜깁기한 서비스다. github을 연동하고 설정만 조금 만지면 웹 앱 하나가 뚝딱 만들어진다. S3, CloudFront를 설정하고 연동해야 하는 부담이 없어 사용했다. 하지만 기능이 간소화 "},"pathname":"/blog/amplify에서-s3-cloudfront로-전환하기/"},{"content":"전환 배경\n\n두달 전 우연히 Gatsby라는 Static Site Generator(이하 SSG)를 알게 되었고. 프로토타이핑 후 기존 블로그를\nGatsby로 전환하는 작업을 하기 시작했다. Gatsby의 전반적인 시스템을 이해하는데 총 2주 정도 걸렸고. 나머지는 두 개의 블로그\n프로젝트 마이그레이션, 최적의 호스팅 서비스 찾기와 연동, 모노레포 구성, 빌드 및 배포설정에 6주 정도 걸렸다.\n\n\nFROM JEKYLL\n\n익숙한 그 디자인. 심지어 featured image도 없다면 완전 클론의 습격이다.\n[/_astro/minimal-mistakes.CxHOiQRj_gAAuF.webp]익숙한 그 디자인. 심지어 featured image도 없다면\n완전 클론의 습격이다.\n\n지금 보고 있는 기술 블로그 johnny-mh.github.io [https://johnny-mh.github.io]는 원래 jekyll로\n운영하고 있었다. jekyll은 ruby기반의 SSG이며 이제는 역사가 깊은 도구인 듯 싶다. github.io 에 호스팅을 전제로 개발하는 듯\n접근하기 쉽고 제공 기능들도 부족함 없고 디자인 템플릿도 많다.\n\n하지만 많이 쓰다 보니 비슷한 디자인의 사이트가 많아 재미가 없다는 점과. 그래서 바꿔보려니 익숙하지 않아 커스터마이징이 어렵다는 것이 계속\n걸렸다. 그러다 보니 글을 올리고 싶다는 생각도 잘 들지 않았다.\n\n\nFROM SQUARESPACE\n\n사진 블로그 mnkin.com [https://mnkim.com]은 squarespace.com\n[https://www.squarespace.com]를 이용하여 운영하고 있었다. 웹 기반으로 블로그 및 쇼핑몰 사이트를 만들고 관리할 수 있는\n서비스이다. 처음 발견했을 때 서비스의 웹 기반 에디터에 반해버려서 1년에 이용료와 호스팅 비용 18만원을 지불하며 3년을 사용했다.\n\n사진만 조금 넣고 글 조금 쓰면 이런 룩앤필이 뚝딱\n[/_astro/squarespace-fillmore.QZkblWAH_ZI0lOv.webp]사진만 조금 넣고 글 조금 쓰면 이런 룩앤필이 뚝딱\n\n개발자라 근본이 사대주의라 그런지 모르겠으나 디자인도 대부분 너무 이쁘고. 반응형 기본제공, SEO자동 최적화, 아이폰 안드로이드용 관리 툴,\n통합 google analytics등 서비스 운영에 필요한 모든 것이 기본으로 제공된다.\n\n지금봐도 서비스 내 위지윅 에디터는 국내 IT대기업들보다 훨씬 직관적이고 사용하기 좋게 만들었다고 생각한다. 만약 외국에서 사이트를 서비스한다면\n그냥 여기를 사용하면 될 정도다.\n\n하지만 1년에 많아봐야 10개 이내의 글을 쓰게 되면서 비용이 부담스럽다는 생각이 들었고 서비스를 외국에서만 하다 보니 국내에서는 심각하게 느린\n것도 불편했다. 이미지를 주로 서빙하는 블로그인 특성 상 이 부분이 치명적이었다.\n\n\nGATSBYJS의 특징 및 장점\n\n\nREACT, GRAPHQL\n\n현재는 업무 상 angular만을 사용하고 있지만 커리어 문제로 React와 GraphQL을 항상 공부하고 싶었던 차에 Gatsby는 신기하게도\n시기적절하게 발견한 SSG 프레임웍이었다. 내부적으로 리소스 파일들을 GraphQL로 쿼리하고. NodeJS API를 통해 가공하여 React로\nhtml페이지를 만들어내는 형태라 너무 가볍지도 무겁지도 않게 기술을 접해볼 수 있어서 즐거웠다.\n\nReact Hook은 말로만 좋다고 들어왔는데 실제로 써 보니 신세계라는 말이 아깝지 않았다. 이전 회사에서 React기반으로 개발할 때\n가려웠던 부분을 기초에 충실하면서도 강력하게 개선했다고 생각한다. 그냥 함수였을 뿐인데. 그것만 가지고 이런 시스템을 만들어내었다는것이 놀랍다.\n\n\n강력한 플러그인\n\nGatsby 플러그인 중 gatsby-plugin-sharp을 이용하면. 직접 구현하기는 조금 까다로운 이미지 서빙 최적화를 간편하게 적용 할\n수 있다. medium.com의 글을 보다보면 로딩 중에는 이미지가 뿌옇다가 완료 후 부드럽게 선명해지는 그 효과 말이다.\n\n사진 전문 블로그인 mnkim.com [https://mnkim.com]는 컨텐츠의 대부분이 이미지라 큰 도움이 되었다. 아래처럼 플러그인 몇\n개만 설정하면 마크다운의 이미지들에 자동 점진적 로딩 최적화가 적용된다. prerender 시점에 저 품질의 base64이미지를 생성하여\nhtml을 먼저 서빙하고. 로딩 후에 페이드 인으로 교체해 준다.\n\n// gatsby-config.js\n// 사이트의 html페이지들을 생성할 때 적용되는 설정 및 플러그인을 추가하는 인터페이스\nmodule.exports = {\n  plugins: [\n    'gatsby-plugin-sharp',\n    {\n      resolve: 'gatsby-transformer-remark', // .md파일을 html 컨텐츠로 변환하는 플러그인\n      options: {\n        plugins: [\n          {\n            resolve: 'gatsby-remark-images', // .md파일을 변환할 때 이미지들에 최적화를 적용한다\n            options: { maxWidth: 1300, showCaptions: ['alt'] },\n          },\n        ],\n      },\n    },\n  ],\n}\n\nGatsby를 한 마디로 표현하자면 FE개발자에게 치명적인 만능 장난감인 듯 하다. 마이그레이션을 하면서 React, GraphQL은 물론이고\n성능 최적화, SEO, lerna, syntax-tree [https://github.com/syntax-tree] 등 많은 도구에 대한 공부를\n하게 되어서 너무 즐거웠다. 블로그를 만들 계획이 있다면 적극 추천한다.","frontmatter":{"title":"블로그들 Gatsby로 전환 후기","description":"squarespace에서 gatsby로 블로그를 전환했던 경험을 공유합니다","categories":["Framework"],"tags":["gatsby","react"],"publishedAt":"2020-05-24T20:00+09:00","readingTime":405000,"summary":"두달 전 우연히 Gatsby라는 Static Site Generator(이하 SSG)를 알게 되었고. 프로토타이핑 후 기존 블로그를 Gatsby로 전환하는 작업을 하기 시작했다. Gatsby의 전반적인 시스템을 이해하는데 총 2주 정도 걸렸고. 나머지는 두 개의 블로그"},"pathname":"/blog/블로그들-gatsby로-전환-후기/"},{"content":"CentOS 7.7 기준으로 작성하였으며, Master, Slave 노드 공통으로 해야하는 일과 각각 해야하는 일들로 나누어 정리했다\n\n\nMASTER, SLAVE 공통\n\nJDK설치, git 2.x 설치\n\n> lerna처럼 근래에 나온 도구들은 git 2.x이상을 요구하는 경우가 있으므로 업데이트 한다\n\nsudo yum update\nsudo rpm -Uvh http://opensource.wandisco.com/centos/7/git/x86_64/wandisco-git-release-7-2.noarch.rpm\nsudo yum install -y java-1.8.0-openjdk-devel git\n\n사내망일 경우 프록시를 설정해야 할 수 있다\n\nvim .bash_profile\n\n# 맨밑에 아래 내용 추가\nexport http_proxy={{프록시 서버 주소}}\nexport HTTP_PROXY=$http_prox\nexport https_proxy=$http_proxy\nexport HTTPS_PROXY=$http_proxy\nexport no_proxy=\"localhost,127.0.0.1\"\nexport NO_PROXY=$no_proxy\n\n\nMASTER 설정\n\n\nJENKINS 설치 및 포트 설정\n\nhttps://linuxize.com/post/how-to-install-jenkins-on-centos-7\n[https://linuxize.com/post/how-to-install-jenkins-on-centos-7]를 참고하여 설치한다\n\n만약 서버에 80포트가 관리자 권한으로 막혀 있다면 아래 명령으로 우회 사용할 수 있도록 한다\n\nsudo -i\niptables -A PREROUTING -t nat -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8080\niptables-save\n\njenkins계정의 패스워드 삭제 및 로그인 가능하게 하기\n\nsudo -i\npasswd -d jenkins\nsudo vim /etc/passwd\n\n# jenkins 라인 마지막 /bin/false를 /bin/bash로 변경\n\n\nGITHUB 액세스용 인증 키 생성\n\nsu - jenkins\nmkdir .ssh\ncd .ssh\nssh-keygen\n\n# id_rsa, id_rsa.pub생성됨\n\ngithub.com 로그인 후 Settings > Developer settings > Personal access tokens 에서 새 토큰을\n등록하고 해당 값을 복사한다\n\nJenkins 설정 중 Crediential 에서 Secret text 로 해당 값을 등록해둔다. 등록할 때 description을 잘 적어서\n다른 키들과 혼동하지 않게 한다 나는 **‘Personal Access Token’**으로 적었다\n\n\n플러그인 설치 및 설정\n\n아래 플러그인들 설치\n\nNodeJS, AnsiColor, GitHub Pull Request Builder\n\nJenkins 관리 > Global Tool Configuration > NodeJS 항목 추가한다. 버전은 어떤것이든 상관없으나 가능하면\nLTS (Long Term Support)버전을 선택한다\n\nName: NodeJS 10.15.3\nVersion: NodeJS 10.15.3\nGlobal npm packages to install: typescript@3.5.3 ts-node@8.8.2 @angular/cli@8.2.0 @sentry/cli@1.52.1 yarn lerna\n\n위에는 Angular프로젝트 빌드 및 배포용으로 적었다. 각 전역 패키지들의 버전을 명시적으로 적어두면 좋다 빌드 시점에 매번 설치하기 때문\n\nprivate npm을 사용중이라면 주소를 Jenkins 관리 > Managed Files > Npm config file 을 추가하여 설정한다\n\nregistry=<private npm url>\n\n\n환경변수 설정\n\nJenkins 관리 > 시스템 설정 > Global properties > Environment variables 에 아래 값들을 추가한다\n\n위의 두 값은 빌드 콘솔이 출력될 때 유니코드 문자열들을 제대로 보기 위함이고. 아래 프록시 설정은 사내망 등 해당할때만 추가한다\n\n이름: JAVA_TOOLS_OPTIONS\n값: -Dfile.encoding=UTF-8\n\n이름: LANG\n값: ko_KR.UTF-8\n\n이름: HTTP_PROXY\n값: 프록시 주소\n\n이름: HTTPS_PROXY\n값: 프록시 주소\n\n이름: NO_PROXY\n값: 프록시 타면 안되는 도메인들\n\n\nSLAVE 설정\n\njenkins 계성 생성 및 패스워드 삭제\n\nsudo i\nuseradd -d /var/lib/jenkins jenkins\npasswd -d jenkins\nexit\n\njenkins 계정에 ssh 키 추가후 Master 노드가 접근할 수 있도록 등록\n\nsu - jenkins\nmkdir .ssh\ncd .ssh\nvim authorized_keys\n\n# Master 노드의 인증서 설정에서 만든 id_rsa.pub의 내용을 복사하여 붙여넣고 저장한다.\n# 만약 기존에 파일이 있다면 맨 아랫줄에 추가하면 된다","frontmatter":{"title":"FrontEnd 개발을 위한 Jenkins CI서버 세팅하기","categories":["Infra"],"tags":["jenkins","frontend","ci"],"publishedAt":"2020-04-17T20:00+09:00","updatedAt":"2020-06-03T20:00+09:00","readingTime":232500,"summary":"CentOS 7.7 기준으로 작성하였으며, Master, Slave 노드 공통으로 해야하는 일과 각각 해야하는 일들로 나누어 정리했다JDK설치, git 2.x 설치lerna처럼 근래에 나온 도구들은 git 2.x이상을 요구하는 경우가 있으므로 업데이트 한다sudo yu"},"pathname":"/blog/frontend-개발을-위한-jenkins-ci서버-세팅하기/"},{"content":"CAPSLOCK과 CONTROL키 바꾸기\n\n처음에는 게임과 영화를 보기 위한 용도로 데스크탑을 조립했는데 이제는 개발도구로 사용하고 있다. Window는 터미널을 사용할 때의 제약이 많아\n개발할때는 꺼렸는데 wsl의 등장으로 지금은 현업에서도 큰 불편 없이 사용할 수 있는 정도가 되었다.\n\n특히 vscode의 wsl 플러그인을 사용하면 wsl의 리눅스 파일시스템에 있는 프로젝트들을 마치 호스트의 파일시스템에 있는 것 처럼 사용할 수\n있어서 집에서는 이제 맥북으로 개발하지 않을 것 같다. 다만 몇 가지 불편한 부분이 있었다.\n\n첫번째는 Capslock키와 Control키의 위치였다. 해피해킹 키보드에 익숙해져 Control대신 Capslock을 마구 눌러댔다.\n윈도우에서 이를 해결할 수 있는 방법은 두가지가 있다. 첫번째로 레지스트리를 수정하는 방법인데. 대부분의 프로그램에서는 잘 동작하지만 특정\n게임들 (몬스터 헌터, 토탈워 삼국)에서는 Control키를 아예 누를 수 없는 상태가 되어 버린다.\n\n두 번째는 AutoHotKey [https://www.autohotkey.com/]를 사용하는 것이다. 이 방법으로 지금까지 만족스럽게 사용하고\n있다. 사이트에서 프로그램을 설치하고 바탕화면에 우클릭 후 ‘새로 만들기’ > **‘AutoHotkey Script’**를 선택한 후 생성된\n파일에 아래 내용을 붙여 넣고 저장한다.\n\n; CapsLock, Control 전환\nCapsLock::Ctrl\nCtrl::CapsLock\n\n다음 해당 파일을 우클릭하여 **‘Run Script’**로 실행한다. 시스템 트레이 아이콘에 ‘H’아이콘이 나타나면 된 것이다. 그럼 이제\nCapslock과 Control이 바뀌었을 것이다. 이 동작은 언제까지나 스크립트가 실행되어 있는 상태만 유효하다.\n\n\n수정모드를 빠져나갈 때 영문으로 전환하기\n\n이전에 Spacemacs를 사용할 땐 에디터 내장 언어 입력기가 존재하여 수정모드를 빠져나갈 때 자동으로 영문으로 바꿔 주었는데. 이 기능이\n정말 편리했다. VSCODE를 사용한 뒤로는 그 기능을 쓸 수 없어 Esc로 수정모드를 빠져나온 후 항상 언어 전환 키를 눌러줘야만 했다.\n\n이 문제를 해결하기 위한 설정 [https://github.com/daipeihust/im-select]이 있긴 하지만 이게 IME입력기를 쓰는\n환경에서는 잘 동작하지 않는다. 이 문제도 AutoHotKey를 이용해 해결할 수 있었다. 위에서 했던 방법과 마찬가지로 아래 스크립트를 쓰면\n된다.\n\n; vscode에서 vim insert 모드 종료시 한글이면 영문으로 전환\n#IfWinActive, ahk_exe Code.exe\nEscape::\n if (ImeCheck(\"A\") = 1)\n  Send {vk15sc138}\n Send {Escape}\nReturn\n#IfWinActive\n\n; 키보드 언어 상태 확인 1이면 한글 0이면 영문\nImeCheck(WinTitle) {\n WinGet,hWnd,ID,%WinTitle%\n Return SendImeControl(ImmGetDefaultIMEWnd(hWnd),0x005,\"\")\n}\nSendImeControl(DefaultIMEWnd, wParam, lParam) {\n DetectSave := A_DetectHiddenWindows\n DetectHiddenWindows,ON\n SendMessage 0x283, wParam,lParam,,ahk_id %DefaultIMEWnd%\n if (DetectSave <> A_DetectHiddenWindows)\n  DetectHiddenWindows,%DetectSave%\n return ErrorLevel\n}\nImmGetDefaultIMEWnd(hWnd) {\n return DllCall(\"imm32\\ImmGetDefaultIMEWnd\", Uint,hWnd, Uint)\n}\n\n그럼 이제 매 부팅시마다 위의 스크립트들이 자동실행만 되면 된다. 방법은 스크립트 파일 우클릭 후 **‘Compile Script’**를\n선택한다. 그럼 같은 경로에 exe파일이 생겼을 것이다. 이제 Window의 시작 버튼에 우클릭 후 ‘실행’ 을 열고 거기에\nshell:startup을 입력하고 ‘열기’ 를 누른다.\n\n그럼 폴더가 하나 뜨는데 여기에 exe파일들을 넣으면 된다. 참고로 위의 두 스크립트를 하나의 파일에 넣어도 된다.","frontmatter":{"title":"Windows에서 VS Code vim플러그인 자동 한영전환","categories":["Other"],"tags":["vscode","autohotkey"],"publishedAt":"2020-03-13T20:00+09:00","readingTime":280800,"summary":"처음에는 게임과 영화를 보기 위한 용도로 데스크탑을 조립했는데 이제는 개발도구로 사용하고 있다. Window는 터미널을 사용할 때의 제약이 많아 개발할때는 꺼렸는데 wsl의 등장으로 지금은 현업에서도 큰 불편 없이 사용할 수 있는 정도가 되었다.특히 vscode의 ws"},"pathname":"/blog/windows에서-vs-code-vim플러그인-자동-한영전환/"},{"content":"🚀 ROUTEREUSESTRATEGY\n\nAngular는 라우팅 시점마다 RoutingModule에 제공된 Routes 중 이전 페이지와 다음 페이지에 해당하는 Route 객체를 찾아\n서로 비교하여 변경이 있을 때만 컴포넌트를 교체한다.\n\n동일한 Route 간 이동 시. 같은 컴포넌트가 렌더링 되며 이뤄지는 API 호출은 중복으로 판단하는 것으로 보인다. 가이드 문서에는 없지만,\n이 동작은 개발자가 커스터마이징 할 수 있다.\n\n예를 들면 상세에서 목록으로 뒤로 가기로 이동했을 때는 원래 Route 설정이 달라 컴포넌트를 새로 만들어야 하지만 목록에서 상세로 진입 시점에\n목록 컴포넌트 상태를 캐시 했다가 뒤로 가기 시점에 복원하여 API 호출을 줄일 수 있다.\n\n이 RouterReuseStrategy API 상세 설명\n[https://itnext.io/cache-components-with-angular-routereusestrategy-3e4c8b174d5f]을\n간략히 설명하면 아래와 같다.\n\nexport abstract class RouteReuseStrategy {\n  /**\n   * 현재 이동에 컴포넌트 재사용 '여부'를 확인한다\n   * false반환 시 재사용없이 컴포넌트를 새로 만들고\n   * true를 반환하면 아래 4개의 메서드를 상황별로 호출하여 캐시 및 복원한다\n   * (캐시, 복원 로직은 직접 구현해야 한다)\n   */\n  abstract shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ): boolean\n\n  /**\n   * 페이지를 빠져나갈 때 현재 컴포넌트 캐시 '여부'를 반환한다\n   * false 반환 시 캐시 안해도 되는것으로 판단\n   * true 반환 시 아래 store메서드를 호출한다\n   */\n  abstract shouldDetach(route: ActivatedRouteSnapshot): boolean\n\n  /**\n   * 페이지 빠져나가기 전 상태를 캐시한다\n   * 캐시를 위해서 두 번째 인자인 DetachedRouteHandle을 어딘가에 저장하면 된다\n   */\n  abstract store(\n    route: ActivatedRouteSnapshot,\n    handle: DetachedRouteHandle | null\n  ): void\n\n  /**\n   * 페이지 진입 시점에 복원 '여부'를 반환한다\n   * false 반환 시 복원 안해도 되는것으로 판단\n   * true 반환 시 아래 retrieve메서드를 호출한다\n   */\n  abstract shouldAttach(route: ActivatedRouteSnapshot): boolean\n\n  /**\n   * 페이지 진입 시 캐시된 데이터를 복원한다\n   * 위에서 구현한 store 메서드 호출 시점에 어딘가에 저장했던 캐시를 반환하면 된다\n   */\n  abstract retrieve(route: ActivatedRouteSnapshot): DetachedRouteHandle | null\n}\n\n위의 인터페이스를 상속받아 최상위 NgModule에 Providing 해 주면 된다.\n\nAngular의 DefaultRouteReuseStrategy\n[https://github.com/angular/angular/blob/5bc39f8c8d5238a9be9bd968cf18ea4b738bd6be/packages/router/src/route_reuse_strategy.ts#L65]는\nRouteConfig [https://angular.io/api/router/Route]가 같을 때 shouldReuseRoute의 실행 결과를\ntrue로 반환하고 있지만. 각 메서드에서 캐시 여부(shouldDetach), 복원 여부(shouldAttach) 모두 false를 반환하고\n있어 아무 일도 일어나지 않는다.\n\n\nROUTERLINK가 동작하지 않아요\n\nAngular는 앞서 설명한 대로 라우팅 전, 후의 Route 객체를 비교하여 다를 때만 컴포넌트를 교체한다. 따라서 같은 Route 객체 간\n이동이라면 컴포넌트가 교체되지 않는다.\n\nconst routes = [\n  {\n    path: 'detail/:id',\n    component: DetailComponent,\n  },\n]\n\n위의 라우팅 설정에서 아래 컴포넌트의 링크 클릭해서 '/detail/3'에서 '/detail/12'로 이동했다면 같은 Route객체(정확히는\n같은 ActivatedRouteSnapshot)를 비교한다.\n\n따라서 컴포넌트가 교체되지 않아 ngOnInit 을 비롯한 라이프사이클 메서드들이 실행되지 않는다. 만약 ngOnInit에서 id를 받아\nAPI를 호출해 상세 데이터를 보여주도록 개발했다면 문제가 될 수 있다.\n\n@Component({\n  selector: 'app-detail',\n  template: `\n    <h1>detail component</h1>\n    <a routerLink=\"/detail/12\">go to '/detail/12'</a>\n    <div>{{ content }}</div>\n  `,\n})\nexport class DetailComponent {\n  content = ''\n\n  // '/detail/12'로 이동했을 때는 호출되지 않아 3번 데이터를 계속 보여준다\n  ngOnInit() {\n    this.http\n      .get(`/detail/${this.activatedRoute.snapshot.params.id}`)\n      .subscribe((o) => (this.content = o))\n  }\n}\n\n아래 데모를 통해 문제를 확인해 볼 수 있다.\n\n\n\n이런 경우 ngOnInit이 재실행되지 않아도 갱신되도록 스트림을 이용하여 수정하는 것이 일반적이지만. RouteReuseStrategy를\n이용하여 Route 객체 비교를 커스터마이징 할 수 있고. 강제로 컴포넌트를 교체하도록 할 수 있다.\n\nexport class CustomRouteReuseStrategy extends RouteReuseStrategy {\n  shouldDetach(route: ActivatedRouteSnapshot) {\n    return false\n  }\n\n  store(route: ActivatedRouteSnapshot, detachedTree: DetachedRouteHandle) {}\n\n  shouldAttach(route: ActivatedRouteSnapshot) {\n    return false\n  }\n\n  retrieve(route: ActivatedRouteSnapshot) {\n    return null\n  }\n\n  shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ) {\n    const [futureUrl, currUrl] = [future, curr].map((o) =>\n      o.url.map((seg) => seg.path).join('/')\n    )\n\n    /**\n     * Route비교 시 둘 다 'detail'을 포함한 path라면 컴포넌트를\n     * 재사용하지 않도록 false를 반환한다.\n     */\n    if (futureUrl.includes('detail') && currUrl.includes('detail')) {\n      return false\n    }\n\n    return future.routeConfig === curr.routeConfig\n  }\n}\n\n아래는 위 CustomRouteReuseStrategy를 이용한 강제 컴포넌트 교체의 예제이다.\n\n\n\n\nSHOULDREUSEROUTE는 여러 번 호출된다\n\n> 👀 Angular v8 버전 기준으로 작성했으나 v9 에서도 비슷하게 동작한다\n\nAngular의 RouteConfig [https://angular.io/api/router/Route]는 재귀적으로 선언할 수 있게 되어있다.\n그래서 shouldReuseRoute의 future, curr파라미터는 path에 대한 정보를 트리 구조로 담고 있다. 자세한 내용은 예제를\n통해 파악해 보자.\n\n먼저 앱의 라우팅 설정이 아래처럼 선언되어 있다고 가정한다.\n\n// app-routing.module.ts\nconst routes = [\n  { path: 'list', component: ListComponent },\n  { path: 'detail/:id', component: DetailComponent },\n  {\n    path: 'delivery',\n    loadChildren: () =>\n      import(\"'./delivery/delivery.module'\").then((mod) => mod.DeliveryModule),\n  },\n]\n\n// delivery-routing.module.ts\nconst routes = [{ path: 'detail/:id', component: DeliveryDetailComponent }]\n\n아래 RouteReuseStrategy는 각 호출 단계에서 url과 해당 Route와 연결된 컴포넌트 이름을 출력한다. 이 strategy를\n사용하여 위의 Route 설정에서 발생할 수 있는 이동들에 대한 호출 로그를 분석해 보자.\n\nexport class CustomRouteReuseStrategy implements RouteReuseStrategy {\n  shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ) {\n    // 분석을 위해 파라미터를 로깅함\n    console.log(\n      `[future]\\n${getInfo(future)}\\n\\n[curr]:\\n${getInfo(curr)}\\n\\n----------`\n    )\n\n    return future.routeConfig === curr.routeConfig\n  }\n}\n\n1. 앱 진입\n\n[future]\n  → '' / null\n[curr]:\n  → '' / null\n----------\n\n앱 진입 시점에 한번 호출된다. 큰 의미는 없다\n\n2. ” 에서 ‘list’로 이동하는 경우\n\n[future]\n  → '' / AppComponent\n  → 'list' / ListComponent\n[curr]:\n  → '' / null\n----------\n\nfuture를 보면 ''는 AppComponent, 'list'는 ListComponent에 제공되는 것을 알 수 있다.\n\n3. ‘list’에서 ‘detail/2’로 이동하는 경우\n\n[future]\n  → '' / AppComponent\n  → 'detail/2' / DetailComponent\n[curr]:\n  → '' / AppComponent\n  → 'list' / ListComponent\n----------\n[future]\n  → 'list' / ListComponent\n[curr]:\n  → 'detail/2' / DetailComponent\n----------\n\n * shoudReuseRoute가 두 번 호출되고 있다.\n * 이상한 점이 있는데 두 번째 호출에서는 future, curr값이 뒤바뀌었다.\n * future를 보면 AppComponent에는 :id에 해당하는 문자열이 없다. 라우팅 설정 자체도 그러한데. AppComponent가\n   DI 받는 ActivatedRouteSnapshot에서는 id를 가져올 수 없는 이유이기도 하다.\n\n4. ‘detail/2’에서 ‘delivery/detail/4’로 이동하는 경우\n\n[future]\n  → '' / AppComponent\n  → 'delivery' / null\n  → 'detail/4' / DeliveryDetailComponent\n[curr]:\n  → '' / AppComponent\n  → 'detail/2' / DetailComponent\n----------\n[future]\n  → 'detail/2' / DetailComponent\n[curr]:\n  → 'delivery' / null\n  → 'detail/4' / DeliveryDetailComponent\n----------\n\n * 3번처럼 두 번째 호출의 future, curr값이 뒤바뀌어 있다. 관련 PR\n   [https://github.com/angular/angular/issues/16192]이 있는데 아직 별다른 업데이트가 없다.\n * 첫 shouldReuseRoute의 호출에서 future파라미터를 보면loadChildren을 사용한 Route에는 컴포넌트가 없다.\n\n두 번씩 호출되는 이유는 어디에도 나와 있지 않지만. 관련 코드\n[https://github.com/angular/angular/blob/3e51a1998304ab6a15e5bea6bc66e7a8c636a8ad/packages/router/src/create_router_state.ts]를\n볼 때 두 번씩 호출하더라도 컴포넌트에 올바른 라우팅 상태를 줄 수 있기 때문에 따로 정리하지 않은 것으로 보인다. 따라서 구현 할 때 주의가\n필요하다.\n\n\n상품상세, 목록 간 컴포넌트 캐싱 예제\n\n상세, 목록 페이지의 경우 상세에서 뒤로가기 시 이전에 보고 있던 목록과 스크롤을 유지하면 페이지 탐색 사용성을 크게 개선할 수 있다. 특히\n전자상거래 서비스의 경우 매출과 직결되는 부분이기도 하다.\n\n캐싱을 위해 일반적으로 bfcache에 의존하거나. 상세 진입 전의 앱 상태를 persist로 저장했다가 복원하는 방법을 사용하는데. 두 방법은\n코드베이스 외적인 부분에 의존하기 때문에 관리가 어렵고 사이드이펙트가 있을 수 있다.\n\nRouteReuseStrategy를 이용한 방법은 캐싱이 필요한 구간에 부분적으로 적용해야 하지만 구현이 코드베이스 안에 있으므로 앞서 언급한\n문제에서 자유롭다. 가능한 이 방법을 도입하는 것이 좋아 보인다.\n\n\n\n위 데모는 상품목록 상세 예제이다. 원래라면 상세에서 뒤로 가기로 목록으로 돌아왔을 때 컴포넌트가 교체어 버리므로 화면이 깜빡이며 1번째\n페이지부터 새로 그리고. 스크롤 위치도 잃어버린다.\n\nexport class CustomRouteReuseStrategy extends RouteReuseStrategy {\n  private cache = new Map<string, DetachedRouteHandle>()\n\n  shouldDetach(route: ActivatedRouteSnapshot) {\n    // 목록에서 빠져나갈 때 true반환하여 store를 호출한다\n    if (getPath(route).startsWith('list')) {\n      return true\n    }\n\n    return false\n  }\n\n  store(route: ActivatedRouteSnapshot, detachedTree: DetachedRouteHandle) {\n    // 컴포넌트 상태 캐시\n    this.cache.set(getPath(route), detachedTree)\n  }\n\n  shouldAttach(route: ActivatedRouteSnapshot) {\n    const path = getPath(route)\n\n    // 목록 재진입 시 캐시가 있다면 true반환하여 retrieve를 호출한다\n    if (path.startsWith('list') && this.cache.has(path)) {\n      return true\n    }\n\n    return false\n  }\n\n  retrieve(route: ActivatedRouteSnapshot) {\n    // 컴포넌트 상태 복원\n    return this.cache.get(getPath(route))\n  }\n\n  shouldReuseRoute(\n    future: ActivatedRouteSnapshot,\n    curr: ActivatedRouteSnapshot\n  ) {\n    return future.routeConfig === curr.routeConfig\n  }\n}\n\n하지만 본문에서 설명한 RouteReuseStrategy를 상속한 커스텀 클래스를 구현하면 캐시된 컨텍스트를 복원하기 때문에 상품목록 컴포넌트가\n깜빡이지 않고 곧바로 렌더링 되는 것을 볼 수 있다.","frontmatter":{"title":"Angular의 RouteReuseStrategy","description":"Angular의 라우팅 동작을 커스터마이징 할 수 있는 RouteReuseStrategy를 설명합니다","categories":["Framework"],"tags":["angular","routereusestrategy"],"publishedAt":"2020-02-29T20:00+09:00","updatedAt":"2020-05-14T20:00+09:00","readingTime":703200,"summary":"Angular는 라우팅 시점마다 RoutingModule에 제공된 Routes 중 이전 페이지와 다음 페이지에 해당하는 Route 객체를 찾아 서로 비교하여 변경이 있을 때만 컴포넌트를 교체한다.동일한 Route 간 이동 시. 같은 컴포넌트가 렌더링 되며 이뤄지는 AP"},"pathname":"/blog/angular의-routereusestrategy/"},{"content":"Louqe Ghost S1과 Synology DS216j [/_astro/R0000467.XB_GIjxw_Z19zpUl.webp]Louqe\nGhost S1과 Synology DS216j\n\n어렸을 때 PC게임을 좋아했다. 100만원 남짓이었던 첫 월급으로 게임용 PC를 구입하고. 상상도 못했던 크라이시스\n[https://www.ea.com/games/crysis/crysis?isLocalized=true]를 돌렸을 때의 감격이 아직도 기억난다.\n\n얼마 전 정말 오랜만에 최신 PC게임을 무리없이 할 수 있는 컴퓨터를 조립했다. 사실은 웹 서핑중에 Ghost S1\n[http://www.louqe.com/]이라는 작고 매우 이쁜 케이스를 발견하고는 한달 정도 고민하다가 부품까지 같이 주문했다.\n\n조립하고 잘 사용하다가. 얼마 후 Google Chrome을 실행하는 과정에서 블루스크린이 뜨더니 재부팅되며 윈도우 진입이 불가한 상태가\n되었다. 😢\n\n매 부팅시마다 BIOS에서 M.2 SSD가 인식이 되었다 안되었다 했고. 인식이 되더라도 설치 파티션 선택 과정에서 0.0MB 로 디스크가\n인식되었다가. 선택해서 설치하려 하면 설치할 수 없다는 경고 메시지가 출력되고 진행이 불가했다.\n\n결국 SSD(삼성 970 Evo Plus 500GB), 메인보드(GIGABYTE Z390 I AORUS PRO WIFI) 를 모두 A/S\n보냈다. 그런데 기간이 너무 길어져 그냥 SSD, 메인보드를 구입하고 교환한 제품을 중고로 팔았다.\n\n그런데 새로 구입한 조합에서도 윈도우 설치 1%일 때 0xC0000005 오류가 발생하며 설치를 할 수 없었다. 슬슬 스트레스를 받기 시작했다.\n🤬\n\n찾다 보니 RAM 오류라는 말이 있어. A/S센터에 연락해 보니. 그럴 확률이 높다는 대답을 들었고. RAM도 A/S를 보냈다. 그리고 또\n귀찮아서 중간에 RAM을 새로 구입했다 (삼성 16GB RAM 두장) 그런데 동일한 현상이 발생했다.\n\n수리를 시작한 지 2주가 지나 알게 된 사실인데. 설치를 위해 만든 USB자체에 문제가 있었나보다. USB를 새로 구입해 설치를 진행하니 별\n문제 없이 설치가 되었다.","frontmatter":{"title":"Ghost S1 조립컴퓨터 수리","categories":["Other"],"tags":["ghost s1"],"publishedAt":"2020-02-26T20:00+09:00","readingTime":162900,"summary":"어렸을 때 PC게임을 좋아했다. 100만원 남짓이었던 첫 월급으로 게임용 PC를 구입하고. 상상도 못했던 크라이시스를 돌렸을 때의 감격이 아직도 기억난다.얼마 전 정말 오랜만에 최신 PC게임을 무리없이 할 수 있는 컴퓨터를 조립했다. 사실은 웹 서핑중에 Ghost S1"},"pathname":"/blog/ghost-s1-조립컴퓨터-수리/"}]}